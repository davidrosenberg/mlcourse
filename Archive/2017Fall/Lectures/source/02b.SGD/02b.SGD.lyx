#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}


% Set Color ==============================
\definecolor{BBGblue}{RGB}{13,157,219}
\definecolor{BBGgreen}{RGB}{77,170,80}


\setbeamercolor{title}{fg=BBGblue}
%\setbeamercolor{frametitle}{fg=BBGblue}
\setbeamercolor{frametitle}{fg=BBGblue}

\setbeamercolor{background canvas}{fg=BBGblue, bg=white}
\setbeamercolor{background}{fg=black, bg=BBGblue}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=black, bg=BBGblue}

\setbeamertemplate{headline}{}
\end_preamble
\options aspectratio=169,handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\boxbgcolor #ff31d8
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\dom}{\textrm{\textbf{dom} }}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\rank}{\text{\textbf{rank }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\conv}{\textrm{\textbf{conv} }}
\end_inset


\begin_inset FormulaMacro
\newcommand{\relint}{\text{\textbf{relint }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\aff}{\text{\textbf{aff }}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Title
Gradient and Stochastic Gradient Descent
\begin_inset Argument 1
status open

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David S.
 Rosenberg 
\end_layout

\begin_layout Date
September 29, 2017
\end_layout

\begin_layout Institute
Bloomberg ML EDU
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Just in article version
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Might be useful to pull in some content from sgd-gd-revisited!!!
\end_layout

\end_inset

 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Give example of why gradient descent is bad when we have very different
 scales of different coordinates.
 Figure 7.4 of Bishop's Neural Networks for Pattern Recognition book.
 
\end_layout

\begin_layout Plain Layout
What about simple scale [in]variance? e.g.
 g(x)=f(10x).
 Does gradient descent follow the same path, rescaled? Seems like not.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Gradient Descent
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Unconstrained Optimization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Setting
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Objective function 
\begin_inset Formula $f:\reals^{d}\to\reals$
\end_inset

 is 
\emph on
differentiable.
\end_layout

\end_deeper
\begin_layout Block
Want to find 
\begin_inset Formula 
\[
x^{*}=\arg\min_{x\in\reals^{d}}f(x)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Gradient
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $f:\reals^{d}\to\reals$
\end_inset

 be differentiable at 
\begin_inset Formula $x_{0}\in\reals^{d}$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The 
\series bold
gradient
\series default
 of 
\begin_inset Formula $f$
\end_inset

  at the point 
\begin_inset Formula $x_{0}$
\end_inset

, denoted 
\begin_inset Formula $\del_{x}f(x_{0})$
\end_inset

, is the direction to move in for the 
\series bold
fastest increase
\series default
 in 
\begin_inset Formula $f(x)$
\end_inset

, when starting from 
\begin_inset Formula $x_{0}$
\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename two-dim-gradient.png
	height 40theight%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Figure A.111 from Newtonian Dynamics, by Richard Fitzpatrick.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Gradient Descent
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Initialize 
\begin_inset Formula $x=0$
\end_inset


\end_layout

\begin_layout Itemize
repeat 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x\gets x-\underbrace{\eta}_{\mbox{step size}}\del f(x)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
until stopping criterion satisfied
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent Path
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename vlad-GD-fixedAndBacktracking.png
	lyxscale 40
	height 80theight%

\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent Path
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Gradient Descent Path for the Rosenbrock Function (not convex)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Banana-SteepDesc.gif
	lyxscale 50
	height 60theight%

\end_inset


\end_layout

\end_deeper
\begin_layout Plain Layout
(Figure by P.A.
 Simionescu from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{http://en.wikipedia.org/wiki/Image:Banana-SteepDesc.gif}{Wikipedia page
 on gradient descent}
\end_layout

\end_inset

 )
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent: Step Size
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
A fixed step size will work, eventually, as long as it's small enough (roughly
 - details to come)
\end_layout

\begin_deeper
\begin_layout Itemize
Too fast, may diverge
\end_layout

\begin_layout Itemize
In practice, try several fixed step sizes
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Intuition on when to take big steps and when to take small steps?
\end_layout

\begin_deeper
\begin_layout Itemize
Demo.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Convergence Theorem for Fixed Step Size
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
Suppose 
\begin_inset Formula $f:\reals^{d}\to\reals$
\end_inset

 is convex and differentiable, and 
\begin_inset Formula $\del f$
\end_inset

 is 
\series bold
Lipschitz continuous
\series default
 with constant 
\begin_inset Formula $L>0$
\end_inset

, i.e.
\begin_inset Formula 
\[
\|\del f(x)-\del f(y)\|\le L\|x-y\|
\]

\end_inset

for any 
\begin_inset Formula $x,y\in\reals^{d}$
\end_inset

.
 Then gradient descent with fixed step size 
\begin_inset Formula $t\le1/L$
\end_inset

 
\series bold
converges
\series default
.
 In particular,
\begin_inset Formula 
\[
f(x^{(k)})-f(x^{*})\le\frac{\|x^{(0)}-x^{*}\|^{2}}{2tk}.
\]

\end_inset


\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Proof sketch: Fit a quadratic at 
\begin_inset Formula $x$
\end_inset

 that's tangent to 
\begin_inset Formula $f(x)$
\end_inset

 and has Lipschitz constant 
\begin_inset Formula $L$
\end_inset

.
 Taylor remainder theorem to show that 
\begin_inset Formula $f(x)$
\end_inset

 is below quadratic.
 Jump to minimizer of quadratic.
\end_layout

\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Step Size: Practical Note
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Although a 
\begin_inset Formula $1/L$
\end_inset

 step-size guarantees convergence, 
\end_layout

\begin_deeper
\begin_layout Itemize
it may be 
\series bold
much slower
\series default
 than necessary.
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
May be worth trying larger step sizes as well.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
But math tells us, no need for anything smaller.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent: Questions to Ponder
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Empirically 
\begin_inset Formula $\eta=0.1$
\end_inset

 often works well
\begin_inset Quotes erd
\end_inset

 (says an ML textbook)
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
How can one rate work well for most functions?
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Suppose 
\begin_inset Formula $\eta=0.1$
\end_inset

 works well for 
\begin_inset Formula $f(x)$
\end_inset

, what about 
\begin_inset Formula $g(x)=f(10x)$
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Do we want bigger steps or smaller steps?
\end_layout

\begin_layout Itemize
How does the magnitude of the gradient compare between 
\begin_inset Formula $g(x)$
\end_inset

 and 
\begin_inset Formula $f(x)$
\end_inset

?
\end_layout

\begin_layout Itemize
How does the Lipschitz constant compare between 
\begin_inset Formula $g(x)$
\end_inset

 and 
\begin_inset Formula $f(x)$
\end_inset

?
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Backtracking Line Search
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
If we step in negative gradient direction, 
\begin_inset Formula $\|\del f(x)\|$
\end_inset

 gives us rate of decrease.
\end_layout

\begin_deeper
\begin_layout Itemize
at least for infinitesimally small step size.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Find step size that gives at least some fixed fraction of instantaneous
 rate of decrease.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We'll discuss 
\series bold
backtracking line search, 
\series default
based on this idea, in the Lab
\series bold
.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

Steepest descent
\begin_inset Quotes erd
\end_inset

 means going all the way to the local minimum in the direction 
\begin_inset Formula $-\del f(x)$
\end_inset

.
 Consecutive step directions are orthogonal.
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent: When to Stop?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Wait until 
\begin_inset Formula $\|\del f(x)\|_{2}\leq\eps$
\end_inset

, for some 
\begin_inset Formula $\eps$
\end_inset

 of your choosing.
\end_layout

\begin_deeper
\begin_layout Itemize
(Recall 
\begin_inset Formula $\del f(x)=0$
\end_inset

 at minimum.)
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
For learning setting,
\end_layout

\begin_deeper
\begin_layout Itemize
evalute performance on validation data as you go
\end_layout

\begin_layout Itemize
stop when not improving, or getting worse
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Gradient Descent for Empirical Risk (And Other Averages)
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\emph on
Linear
\emph default
 Least Squares Regression
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Setup
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Output space 
\begin_inset Formula $\cy=\reals$
\end_inset


\end_layout

\begin_layout Itemize
Action space 
\begin_inset Formula $\cy=\reals$
\end_inset


\end_layout

\begin_layout Itemize
Loss: 
\begin_inset Formula $\loss(\hat{y},y)=\frac{1}{2}\left(y-\hat{y}\right)^{2}$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Hypothesis space:
\series default
 
\begin_inset Formula $\cf=\left\{ f:\reals^{d}\to\reals\mid f(x)=w^{T}x\,,\,w\in\reals^{d}\right\} $
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Given data set 
\begin_inset Formula $\cd_{n}=\left\{ (x_{1},y_{1}),\ldots,(x_{n},y_{n})\right\} $
\end_inset

,
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let's find the ERM 
\begin_inset Formula $\hat{f}\in\cf$
\end_inset

.
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\emph on
Linear
\emph default
 Least Squares Regression
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Objective Function: Empirical Risk
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The function we want to minimize is the empirical risk:
\begin_inset Formula 
\[
\hat{R}_{n}(w)=\frac{1}{n}\sum_{i=1}^{n}\left(w^{T}x_{i}-y_{i}\right)^{2},
\]

\end_inset

where 
\begin_inset Formula $w\in\reals^{d}$
\end_inset

 parameterizes the hypothesis space 
\begin_inset Formula $\cf$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Now let's think more generally...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent for Empirical Risk and Averages
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have a hypothesis space of functions 
\begin_inset Formula $\cf=\left\{ f_{w}:\cx\to\ca\mid w\in\reals^{d}\right\} $
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Parameterized by 
\begin_inset Formula $w\in\reals^{d}$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
ERM is to find 
\begin_inset Formula $w$
\end_inset

 minimizing
\begin_inset Formula 
\[
\hat{R}_{n}(w)=\frac{1}{n}\sum_{i=1}^{n}\loss(f_{w}(x_{i}),y_{i})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Suppose 
\begin_inset Formula $\loss(f_{w}(x_{i}),y_{i})$
\end_inset

 is differentiable as a function of 
\begin_inset Formula $w$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Then we can do gradient descent on 
\begin_inset Formula $\hat{R}_{n}(w)$
\end_inset

...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent: How does it scale with 
\begin_inset Formula $n$
\end_inset

?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
At every iteration, we compute the gradient at current 
\begin_inset Formula $w$
\end_inset

:
\begin_inset Formula 
\[
\del\hat{R}_{n}(w)=\frac{1}{n}\sum_{i=1}^{n}\del_{w}\ell(f_{w}(x_{i}),y_{i})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We have to touch all 
\begin_inset Formula $n$
\end_inset

 training points to take a single step.
 [
\begin_inset Formula $O(n)$
\end_inset

]
\begin_inset Note Note
status collapsed

\begin_layout Pause

\end_layout

\begin_layout Itemize
A method that looks at all training points before each step is called a
 
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
batch optimization
\series default
 method.
 
\end_layout

\begin_layout Itemize
So far we've presented 
\series bold

\begin_inset Quotes eld
\end_inset

batch gradient descent
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Will this scale to 
\begin_inset Quotes eld
\end_inset

big data
\begin_inset Quotes erd
\end_inset

?
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Can we make progress without looking at all the data?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

Noisy
\begin_inset Quotes erd
\end_inset

 Gradient Descent 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
We know gradient descent works.
\end_layout

\begin_layout Itemize
But the gradient may be slow to compute.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
What if we just use an estimate of the gradient?
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Turns out that can work fine.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Intuition
\series default
: 
\end_layout

\begin_deeper
\begin_layout Itemize
Gradient descent is an interative procedure anyway.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
At every step, we have a chance to recover from previous missteps.
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
Turns out, even terrible estimates will work, so long as they are 
\series bold
unbiased
\series default
.
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Real goal is to minimize the risk (expected loss):
\begin_inset Formula 
\[
\argmin_{f\in\cf}\ex\left[\ell(f(X),Y)\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For linear regression, that's
\begin_inset Formula 
\[
\argmin_{w}\ex\left(w^{T}X-Y\right)^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Gradient descent on this?
\begin_inset Formula 
\begin{align*}
\del_{w}\ex\left(w^{T}X-Y\right)^{2}= & \ex\left[2\left(w^{T}X-Y\right)X\right]
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk [approximately]
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Want to find gradient of the risk: 
\begin_inset Formula 
\[
\del R(w)=\ex\left[2\left(w^{T}X-Y\right)X\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can estimate expectation with a sample:
\begin_inset Formula 
\[
\widehat{\del R(w)}=\frac{1}{n}\sum_{i=1}^{n}\left[2\left(\underbrace{w^{T}x_{i}-y_{i}}_{\mbox{i'th residual}}\right)x_{i}\right]
\]

\end_inset

 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Let's return to the general case...
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk: General Case
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Gradient of Risk:
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Say hypothesis space 
\begin_inset Formula $\cf$
\end_inset

 is parameterized by 
\begin_inset Formula $w\in\reals^{d}$
\end_inset

.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Switching
\begin_inset Note Note
status open

\begin_layout Plain Layout
http://planetmath.org/differentiationundertheintegralsign
\end_layout

\end_inset

 
\begin_inset Formula $\del_{w}$
\end_inset

 and 
\begin_inset Formula $\ex$
\end_inset

 we can write the gradient of risk as
\begin_inset Formula 
\begin{align*}
\mbox{Gradient(Risk)}=\del_{w}\ex\left[\ell(f(X),Y)\right]= & \ex\left[\del_{w}\ell(f(X),Y)\right]
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout

\series bold
\size large
\color blue
Unbiased 
\series default
\size default
\color inherit
estimator for Gradient(Risk):
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Plain Layout
\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}\left[\del_{w}\ell(f_{w}(x_{i}),y_{i})\right]\approx\underbrace{\ex\left[\del_{w}\ell(f(X),Y)\right]}_{\mbox{Gradient(Risk)}}
\]

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Real goal is to minimize the risk (expected loss):
\begin_inset Formula 
\[
\argmin_{f\in\cf}\ex\left[\ell(f(X),Y)\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For linear regression, that's
\begin_inset Formula 
\[
\argmin_{w}\ex\left(w^{T}X-Y\right)^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Gradient descent on this?
\begin_inset Formula 
\begin{align*}
\del_{w}\ex\left(w^{T}X-Y\right)^{2}= & \ex\left[2\left(w^{T}X-Y\right)X\right]
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk [approximately]
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Want to find gradient of the risk: 
\begin_inset Formula 
\[
\del R(w)=\ex\left[2\left(w^{T}X-Y\right)X\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can estimate expectation with a sample:
\begin_inset Formula 
\[
\widehat{\del R(w)}=\frac{1}{n}\sum_{i=1}^{n}\left[2\left(\underbrace{w^{T}x_{i}-y_{i}}_{\mbox{i'th residual}}\right)x_{i}\right]
\]

\end_inset

 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Let's return to the general case...
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk: General Case
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Gradient of Risk:
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Say hypothesis space 
\begin_inset Formula $\cf$
\end_inset

 is parameterized by 
\begin_inset Formula $w\in\reals^{d}$
\end_inset

.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Switching
\begin_inset Note Note
status open

\begin_layout Plain Layout
http://planetmath.org/differentiationundertheintegralsign
\end_layout

\end_inset

 
\begin_inset Formula $\del_{w}$
\end_inset

 and 
\begin_inset Formula $\ex$
\end_inset

 we can write the gradient of risk as
\begin_inset Formula 
\begin{align*}
\mbox{Gradient(Risk)}=\del_{w}\ex\left[\ell(f(X),Y)\right]= & \ex\left[\del_{w}\ell(f(X),Y)\right]
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout

\series bold
\size large
\color blue
Unbiased 
\series default
\size default
\color inherit
estimator for Gradient(Risk):
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Plain Layout
\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}\left[\del_{w}\ell(f_{w}(x_{i}),y_{i})\right]\approx\underbrace{\ex\left[\del_{w}\ell(f(X),Y)\right]}_{\mbox{Gradient(Risk)}}
\]

\end_inset


\end_layout

\end_deeper
\end_deeper
\end_inset

 
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Minibatch Gradient
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The 
\series bold
full gradient
\series default
 is
\begin_inset Formula 
\[
\del\hat{R}_{n}(w)=\frac{1}{n}\sum_{i=1}^{n}\del_{w}\ell(f_{w}(x_{i}),y_{i})
\]

\end_inset


\end_layout

\begin_layout Itemize
It's an average over the 
\series bold
full batch
\series default
 of data 
\begin_inset Formula $\cd_{n}=\left\{ (x_{1},y_{1}),\ldots,(x_{n},y_{n})\right\} $
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Let's take a subsample of size 
\begin_inset Formula $N$
\end_inset

:
\begin_inset Formula 
\[
(x_{m_{1}},y_{m_{1}}),\ldots,(x_{m_{N}},y_{m_{N}})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The 
\series bold
minibatch gradient is
\begin_inset Formula 
\[
\del\hat{R}_{N}(w)=\frac{1}{N}\sum_{i=1}^{N}\del_{w}\ell(f_{w}(x_{m_{i}}),y_{m_{i}})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
What can we say about the minibatch gradient?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Minibatch Gradient
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
What's the expected value of the 
\series bold
minibatch gradient
\series default
?
\begin_inset Formula 
\begin{eqnarray*}
\pause\ex\left[\del\hat{R}_{N}(w)\right] & = & \frac{1}{N}\sum_{i=1}^{N}\ex\left[\del_{w}\ell(f_{w}(x_{m_{i}}),y_{m_{i}})\right]\\
\pause & = & \ex\left[\del_{w}\ell(f_{w}(x_{m_{1}}),y_{m_{1}})\right]\\
\pause & = & \sum_{i=1}^{n}\pr\left(m_{1}=i\right)\del_{w}\ell(f_{w}(x_{i}),y_{i})\\
\pause & = & \frac{1}{n}\sum_{i=1}^{n}\del_{w}\ell(f_{w}(x_{m_{i}}),y_{m_{i}})\\
\pause & = & \del\hat{R}_{n}(w)
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Minibatch Gradient Properties
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Minibatch gradient is an 
\series bold
unbiased estimator
\series default
 for the [full] batch gradient: 
\begin_inset Formula 
\[
\ex\left[\del\hat{R}_{N}(w)\right]=\del\hat{R}_{n}(w)
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The bigger the minibatch, the better the estimate.
 
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
In fact, by Strong Law of Large Numbers, 
\begin_inset Formula $\lim_{N\to\infty}\del\hat{R}_{N}(w)=\del\hat{R}_{n}(w)$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
\pause\lim_{N\to\infty}\del\hat{R}_{N}(w) & = & \lim_{N\to\infty}\frac{1}{N}\sum_{i=1}^{N}\del_{w}\ell(f_{w}(x_{m_{i}}),y_{m_{i}})\\
\pause & = & \ex\left[\del_{w}\ell(f_{w}(x_{m_{1}}),y_{m_{1}})\right]\\
 & = & \del\hat{R}_{n}(w)
\end{eqnarray*}

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Minibatch Gradient – In Practice
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Tradeoffs of minibatch size:
\end_layout

\begin_deeper
\begin_layout Itemize
Bigger 
\begin_inset Formula $N$
\end_inset

 
\begin_inset Formula $\implies$
\end_inset

Better estimate of gradient, but slower (more data to touch)
\end_layout

\begin_layout Itemize
Smaller 
\begin_inset Formula $N$
\end_inset

 
\begin_inset Formula $\implies$
\end_inset

Worse estimate of gradient, but can be quite fast 
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Even 
\begin_inset Formula $N=1$
\end_inset

 works, it's called 
\series bold
stochastic gradient descent
\series default
 (SGD).
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Terminology Review
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Gradient descent
\series default
 or 
\series bold

\begin_inset Quotes eld
\end_inset

batch
\begin_inset Quotes erd
\end_inset

 gradient descent
\end_layout

\begin_deeper
\begin_layout Itemize
Use full data set of size 
\begin_inset Formula $n$
\end_inset

 to determine step direction
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Minibatch gradient descent
\end_layout

\begin_deeper
\begin_layout Itemize
Use a random subset of size 
\begin_inset Formula $N$
\end_inset

 to determine step direction
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Yoshua Bengio says
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Yoshua Bengio's 
\begin_inset Quotes eld
\end_inset

Practical recommendations for gradient-based training of deep architectures
\begin_inset Quotes erd
\end_inset

 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://arxiv.org/abs/1206.5533
\end_layout

\end_inset

.
\end_layout

\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $N$
\end_inset

 is typically between 
\begin_inset Formula $1$
\end_inset

 and few hundred
\end_layout

\begin_layout Itemize
\begin_inset Formula $N=32$
\end_inset

 is a good default value
\end_layout

\begin_layout Itemize
With 
\begin_inset Formula $N\ge10$
\end_inset

 we get computational speedup (per datum touched)
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Stochastic gradient descent
\end_layout

\begin_deeper
\begin_layout Itemize
Minibatch with 
\begin_inset Formula $m=1$
\end_inset

.
\end_layout

\begin_layout Itemize
Use a single randomly chosen point to determine step direction.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Minibatch Gradient Descent
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Minibatch Gradient Descent (minibatch size 
\begin_inset Formula $N$
\end_inset

)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
initialize 
\begin_inset Formula $w=0$
\end_inset


\end_layout

\begin_layout Itemize
repeat
\end_layout

\begin_deeper
\begin_layout Itemize
randomly choose 
\begin_inset Formula $N$
\end_inset

 points 
\begin_inset Formula $\left\{ (x_{i},y_{i})\right\} _{i=1}^{N}\subset\cd_{n}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $w\gets w-\eta\left[\frac{1}{N}\sum_{i=1}^{N}\del_{w}\ell(f_{w}(x_{i}),y_{i})\right]$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
until stopping criteria met
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Stochastic Gradient Descent (SGD)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Stochastic Gradient Descent
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
initialize 
\begin_inset Formula $w=0$
\end_inset


\end_layout

\begin_layout Itemize
repeat
\end_layout

\begin_deeper
\begin_layout Itemize
randomly choose training point 
\begin_inset Formula $(x_{i},y_{i})\in\cd_{n}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $w\gets w-\eta\underbrace{\del_{w}\ell(f_{w}(x_{i}),y_{i})}_{\mbox{Grad(Loss on i'th example)}}$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
until stopping criteria met
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Step Size 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For SGD, fixed step size can work well in practice, but no theorem.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For convergence guarantee, use decreasing step sizes (dampens noise in step
 direction).
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\eta_{t}$
\end_inset

 be the step size at the 
\begin_inset Formula $t$
\end_inset

'th step.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Robbins-Monro Conditions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Many classical convergence results depend on the following two conditions:
\begin_inset Formula 
\[
\sum_{t=1}^{\infty}\eta_{t}^{2}<\infty\qquad\sum_{t=1}^{\infty}\eta_{t}=\infty
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
As fast as 
\begin_inset Formula $\eta_{t}=O\left(\frac{1}{t}\right)$
\end_inset

 would satisfy this...
 but should be faster than 
\begin_inset Formula $O\left(\frac{1}{\sqrt{t}}\right)$
\end_inset

.
 
\end_layout

\begin_layout Itemize
A useful reference for practical techniques: Leon Bottou's 
\begin_inset Quotes eld
\end_inset

Tricks
\begin_inset Quotes erd
\end_inset

: 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://research.microsoft.com/pubs/192769/tricks-2012.pdf
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Many interesting examples involve a loss function Q(z, w) which is not different
iable on a subset of points with probability zero.
 Intuition suggests that this is a minor problems because the iterations
 of the online gradient descent have zero probability to reach one of these
 points.
 Even if we reach one of these points, we can just draw another example
 z.
\end_layout

\begin_layout Plain Layout
The analysis presented in this section addresses the convergence of the
 general online gradient algorithm (section 2.3) applied to the optimization
 of a differentiable cost function C(w) with the following properties: •
 The cost function C(w) has a single minimum w ∗ .
 • The cost function C(w) satisfies the following condition: ∀ε > 0, inf
 (w−w∗) 2>ε (w − w ∗ ) ∇wC(w) > 0 (4.1) Condition (4.1) simply states that
 the opposite of the gradient −∇wC(w) always points towards the minimum
 w ∗ .
 This particular formulation also rejects cost functions which have plateaus
 on which the gradient vanishes without making us closer to the minimum.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
[http://jmlr.csail.mit.edu/proceedings/papers/v5/sunehag09a/sunehag09a.pdf]
 References: Robbins and Monro (1951) proved a theorem that implies convergence
 for one-dimensional stochastic gradient descent; Blum (1954) generalized
 it to the multivariate case.
 Robbins and Siegmund (1971) achieved a stronger result of wider applicability
 in supermartingale theory.
 Here we extend the known convergence results (Bottou and LeCun, 2005) in
 two ways: a) We prove that updates that include scaling matrices with eigenvalu
es bounded by positive constants from above and below will converge almost
 surely; b) under slightly stronger assumptions we obtain a O
\end_layout

\end_inset


\end_layout

\end_body
\end_document
