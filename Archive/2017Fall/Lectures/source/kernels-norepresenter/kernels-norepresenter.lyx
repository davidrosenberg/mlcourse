#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin /Users/drosen/Dropbox/repos/mlcourse/Lectures/
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty
\usepackage{tikz}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\end_preamble
\options handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Title
Kernel Methods
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003 
\begin_inset Note Note
status open

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David Rosenberg 
\end_layout

\begin_layout Institute
New York University
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status collapsed

\begin_layout Plain Layout
From percy notes 
\begin_inset Quotes eld
\end_inset

Aside: sometimes, we can compute dot products eﬃciently in high (even inﬁnite)
 dimensions without using the kernel trick.
 If φ(x) is sparse (as is often the case in natural language processing),
 then hφ(x), φ(x 0 )i can be computed in O(s) time rather than O(d) time,
 where s is the number of nonzero entries in φ(x).
 
\begin_inset Quotes eld
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Plots courtesy of Ningshan Zhang.}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Kernels: High-Level View
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Input Space 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Our general learning theory setup: no assumptions about 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
But 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset

 for the specific methods we've developed: 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Ridge regression
\end_layout

\begin_layout Itemize
Lasso regression
\end_layout

\begin_layout Itemize
Linear SVM
\end_layout

\end_deeper
\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Feature Extraction
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
Mapping an input from 
\begin_inset Formula $\cx$
\end_inset

 to a vector in 
\begin_inset Formula $\reals^{d}$
\end_inset

 is called 
\series bold
feature extraction
\series default
 or 
\series bold
featurization
\series default
.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename feature-extraction.png
	lyxscale 60
	width 90text%

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename feature-extraction.png
	width 80text%

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
e.g.
 Quadratic feature map: 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\begin_inset Formula 
\[
\ensuremath{\phi(x)=(x_{1},\ldots,x_{d},x_{1}^{2},\ldots,x_{d}^{2},\sqrt{2}x_{1}x_{2},\ldots,\sqrt{2}x_{i}x_{j},\ldots\sqrt{2}x_{d-1}x_{d})^{T}}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
High-Dimensional Features Good but Expensive
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
To get 
\series bold
expressive
\series default
 hypothesis spaces using linear models, 
\end_layout

\begin_deeper
\begin_layout Itemize
need high-dimensional feature spaces
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
But more costly in terms of computation and memory.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Some Methods Can Be 
\begin_inset Quotes eld
\end_inset

Kernelized
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A method is 
\series bold
kernelized 
\series default
if inputs only appear inside inner products: 
\begin_inset Formula $\left\langle \phi(x),\phi(y)\right\rangle $
\end_inset

 for 
\begin_inset Formula $x,y\in\cx$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The function 
\begin_inset Formula 
\[
k(x,y)=\left\langle \phi(x),\phi(y)\right\rangle 
\]

\end_inset

is called the 
\series bold
kernel 
\series default
function.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernel Evaluation Can Be Fast
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Example
Quadratic feature map
\begin_inset Formula 
\[
\ensuremath{\phi(x)=(x_{1},\ldots,x_{d},x_{1}^{2},\ldots,x_{d}^{2},\sqrt{2}x_{1}x_{2},\ldots,\sqrt{2}x_{i}x_{j},\ldots\sqrt{2}x_{d-1}x_{d})^{T}}
\]

\end_inset

has dimension 
\begin_inset Formula $O(d^{2})$
\end_inset

, but
\begin_inset Formula 
\[
k(w,x)=\left\langle \phi(w),\phi(x)\right\rangle =\left\langle w,x\right\rangle +\left\langle w,x\right\rangle ^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Naively explicit computation of 
\begin_inset Formula $k(w,x)$
\end_inset

: 
\begin_inset Formula $O(d^{2})$
\end_inset


\end_layout

\begin_layout Itemize
Implicit computation of 
\begin_inset Formula $k(w,x)$
\end_inset

: 
\begin_inset Formula $O(d)$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Recap
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Given a kernelized ML algorithm.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Can swap out the inner product for a new kernel function.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
New kernel may correspond to a high dimensional feature space.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Once kernel matrix is computed, computational cost depends on number of
 data points, rather than the dimension of feature space.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Example: Kernelized Ridge Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Kernelized Ridge Regression:
\begin_inset Formula 
\[
\min_{\alpha\in\reals^{n}}||XX^{T}\alpha-y||^{2}+\lambda XX^{T}\lambda
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Plain Layout
\begin_inset Separator parbreak
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Feature Extraction
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Focus on effectively representing 
\begin_inset Formula $x\in\cx$
\end_inset

 as a vector 
\begin_inset Formula $\phi(x)\in\reals^{d}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
e.g.
 Bag of words:
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ventureBeat.png
	lyxscale 50
	width 35col%

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernel Methods
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Primary focus is on comparing two inputs 
\begin_inset Formula $w,x\in\cx$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Definition
A 
\series bold
kernel 
\series default
is a function that takes a pair of inputs 
\begin_inset Formula $w,x\in\cx$
\end_inset

 and returns a real value.
 That is, 
\begin_inset Formula $k:\cx\times\cx\to\reals$
\end_inset

.
\end_layout

\begin_layout Itemize
Can interpret 
\begin_inset Formula $k(w,x)$
\end_inset

 as a 
\series bold
similarity score
\series default
, but this is not precise.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We will deal with symmetric kernels: 
\begin_inset Formula $k(w,x)=k(x,w)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Kernel Examples
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comparing Documents
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename comparingDocs1.png
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comparing Documents: Bag of Words
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename comparingDocs2.png
	lyxscale 60
	width 80col%
	groupId fullPage

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comparing Documents: Bag of Words
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename comparingDocs3.png
	lyxscale 60
	width 80col%
	groupId fullPage

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comparing Documents: Cosine Similarity
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename cosineSimilarity1.png
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Enumerate
Normalize each feature vector to have 
\begin_inset Formula $\|x\|_{2}=1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comparing Documents
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename cosineSimilarity2.png
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Enumerate
Normalize each feature vector to have 
\begin_inset Formula $\|x\|_{2}=1$
\end_inset

.
\end_layout

\begin_layout Enumerate
Take inner product
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comparing Documents: Cosine Similarity
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename cosineSimilarity3.png
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Enumerate
Normalize each feature vector to have 
\begin_inset Formula $\|x\|_{2}=1$
\end_inset

.
\end_layout

\begin_layout Enumerate
Take inner product
\end_layout

\begin_layout Enumerate
Then define
\begin_inset Formula 
\[
k(\text{VentureBeat},\text{Twitter Tweet})=0.85
\]

\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Cosine Similarity Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Why the name? Recall
\begin_inset Formula 
\[
\left\langle w,x\right\rangle =\|w\|\|x\|\cos\theta,
\]

\end_inset

where 
\begin_inset Formula $\theta$
\end_inset

 is the angle between 
\begin_inset Formula $w,x\in\reals^{d}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
So
\begin_inset Formula 
\[
k(w,x)=\cos\theta=\left\langle \frac{w}{\|w\|},\frac{x}{\|x\|}\right\rangle 
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\begin_inset Formula 
\[
k(w,x)=w^{T}x
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
When we 
\begin_inset Quotes eld
\end_inset

kernelize
\begin_inset Quotes erd
\end_inset

 an algorithm, we write it in terms of the linear kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then we can swap it out a replace it with a more sophisticated kernel
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Quadratic Kernel in 
\begin_inset Formula $\reals^{2}$
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Feature map:
\begin_inset Formula 
\[
\phi:(x_{1},x_{2})\mapsto\left(x_{1},x_{2},x_{1}^{2},x_{2}^{2},\sqrt{2}x_{1}x_{2}\right)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Gives us ability to represent conic section boundaries.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Define kernel as inner product in feature space:
\begin_inset Formula 
\begin{eqnarray*}
k(w,x) & = & \langle\phi(w),\phi(x)\rangle\\
\pause & = & w_{1}x_{1}+w_{2}x_{2}+w_{1}^{2}x_{1}^{2}+w_{2}^{2}x_{2}^{2}+2w_{1}w_{2}x_{1}x_{2}\\
\pause & = & w_{1}x_{1}+w_{2}x_{2}+(w_{1}x_{1})^{2}+(w_{2}x_{2})^{2}+2(w_{1}x_{1})(w_{2}x_{2})\\
\pause & = & \langle w,x\rangle+\langle w,x\rangle^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Based on Guillaume Obozinski's Statistical Machine Learning course
 at Louvain, Feb 2014.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Quadratic Kernel in 
\begin_inset Formula $\reals^{d}$
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Feature map:
\begin_inset Formula 
\[
\ensuremath{\phi(x)=(x_{1},\ldots,x_{d},x_{1}^{2},\ldots,x_{d}^{2},\sqrt{2}x_{1}x_{2},\ldots,\sqrt{2}x_{i}x_{j},\ldots\sqrt{2}x_{d-1}x_{d})^{T}}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Number of terms = 
\begin_inset Formula $d+d(d+1)/2\approx d^{2}/2.$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Still have
\begin_inset Formula 
\begin{eqnarray*}
k(w,x) & = & \left\langle \phi(w),\phi(x)\right\rangle \\
 & = & \left\langle x,y\right\rangle +\left\langle x,y\right\rangle ^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Computation for inner product with explicit mapping: 
\begin_inset Formula $O(d^{2})$
\end_inset


\end_layout

\begin_layout Itemize
Computation for implicit kernel calculation: 
\begin_inset Formula $O(d)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Based on Guillaume Obozinski's Statistical Machine Learning course
 at Louvain, Feb 2014.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Polynomial Kernel in 
\begin_inset Formula $\reals^{d}$
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Kernel function:
\begin_inset Formula 
\[
k(w,x)=\left(1+\left\langle w,x\right\rangle \right)^{M}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Corresponds to a feature map with all terms up to degree 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For any 
\begin_inset Formula $M$
\end_inset

, computing the kernel has same computational cost
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Cost of explicit inner product computation grows rapidly in 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Radial Basis Function (RBF) Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\begin_inset Formula 
\[
k(w,x)=\exp\left(-\frac{\|w-x\|^{2}}{2\sigma^{2}}\right),
\]

\end_inset

where 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is known as the bandwidth parameter.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Does it act like a similarity score?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Why 
\begin_inset Quotes eld
\end_inset

radial
\begin_inset Quotes erd
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Have we departed from our 
\begin_inset Quotes eld
\end_inset

inner product of feature vector
\begin_inset Quotes erd
\end_inset

 recipe?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Yes and no: corresponds to an infinte dimensional feature vector
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Probably the most common nonlinear kernel.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Kernelizing the SVM Dual
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear SVM
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The SVM prediction function is the solution to
\begin_inset Formula 
\[
\min_{w\in\reals^{d},b\in\reals}\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\left(1-y_{i}\left[w^{T}x_{i}+b\right]\right)_{+}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Found it's equivalent to solve the dual problem to get 
\begin_inset Formula $\alpha^{*}$
\end_inset

:i
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\sup_{\alpha} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i}\\
\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
 & \quad & \alpha_{i}\in\left[0,\frac{c}{n}\right]\;i=1,\ldots,n.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Notice: 
\begin_inset Formula $x$
\end_inset

's only show up as inner products with other 
\begin_inset Formula $x$
\end_inset

's.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
We say a machine learning method is 
\series bold
kernelized 
\series default
if all references to inputs 
\begin_inset Formula $x\in\cx$
\end_inset

 are through an inner product between pairs of points 
\begin_inset Formula $\left\langle x,y\right\rangle $
\end_inset

 for 
\begin_inset Formula $x,y\in\reals^{d}$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout AlertBlock
\begin_inset Argument 2
status open

\begin_layout Plain Layout
So far, we've only partially kernelized SVM
\end_layout

\end_inset


\end_layout

\begin_layout AlertBlock
We've shown that the training portion is kernelized.
 Later we'll show the prediction portion is also kernelized.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
SVM Dual Problem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x$
\end_inset

's only show up in pairs of inner products: 
\begin_inset Formula $x_{j}^{T}x_{i}=\left\langle x_{j},x_{i}\right\rangle $
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\sup_{\alpha} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}\left\langle x_{j},x_{i}\right\rangle \\
\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
 & \quad & \alpha_{i}\in\left[0,\frac{c}{n}\right]\;i=1,\ldots,n.
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then primal optimal solution is given as:
\begin_inset Formula 
\[
w^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}
\]

\end_inset

and for any 
\begin_inset Formula $\alpha_{i}\in\left(0,\frac{c}{n}\right)$
\end_inset

, 
\begin_inset Formula 
\[
b^{*}=y_{i}-x_{i}^{T}w^{*}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
SVM: Kernelizing 
\begin_inset Formula $b$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
We found that for any 
\begin_inset Formula $j$
\end_inset

 with 
\begin_inset Formula $\alpha_{j}\in\left(0,\frac{c}{n}\right)$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
b^{*} & = & y_{j}-x_{j}^{T}w^{*}\\
\pause & = & y_{j}-x_{j}^{T}\left(\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}\right).\\
\pause & = & y_{j}-\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}\left\langle x_{j},x_{i}\right\rangle .\pause
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
What about kernelizing 
\begin_inset Formula $w^{*}$
\end_inset

?
\begin_inset Formula 
\[
w^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}\pause
\]

\end_inset


\end_layout

\begin_layout Itemize
Not obvious...
\end_layout

\begin_layout Itemize
But we really only care about kernelizing the predictions 
\begin_inset Formula $f^{*}(x)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
SVM: Kernelizing Predictions 
\begin_inset Formula $f^{*}(x)$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For any 
\begin_inset Formula $j$
\end_inset

 with 
\begin_inset Formula $\alpha_{j}\in\left(0,\frac{c}{n}\right)$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
\pause f^{*}(x) & = & x^{T}w^{*}+b^{*}\\
\pause & = & x^{T}\left(\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}\right)+b^{*}\\
\pause & = & \sum_{i=1}^{n}\alpha_{i}^{*}y_{i}\left\langle x_{i},x\right\rangle +\left(y_{j}-\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}\left\langle x_{j},x_{i}\right\rangle \right)\pause
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
We now have a fully kernelized version of SVM.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can we kernelize the primal version of the SVM?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Kernelizing the SVM Primal Problem 
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelizing the SVM Primal Problem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Primal SVM 
\begin_inset Formula 
\[
\min_{w\in\reals^{d},b\in\reals}\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\left(1-y_{i}\left[w^{T}x_{i}+b\right]\right)_{+}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
From our study of the dual, found that
\begin_inset Formula 
\[
w^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
So 
\begin_inset Formula $w^{*}$
\end_inset

 is a linear combination of the input vectors.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Restrict to optimization to 
\begin_inset Formula $w$
\end_inset

 of the form
\begin_inset Formula 
\[
w=\sum_{i=1}^{n}\beta_{i}x_{i}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Some Vectorization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Design matrix
\series default
 
\begin_inset Formula $X\in\reals^{n\times d}$
\end_inset

 has input vectors as rows: 
\begin_inset Formula 
\[
X=\begin{pmatrix}-x_{1}-\\
\vdots\\
-x_{n}-
\end{pmatrix}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The contraint on 
\begin_inset Formula $w$
\end_inset

 looks like
\begin_inset Formula 
\[
w=\begin{pmatrix}w_{1}\\
\vdots\\
w_{d}
\end{pmatrix}=\begin{pmatrix}| & \cdots & |\\
x_{1} & \cdots & x_{n}\\
| & \cdots & |
\end{pmatrix}\begin{pmatrix}\beta_{1}\\
\vdots\\
\beta_{n}
\end{pmatrix}=X^{T}\beta.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
So replace all 
\begin_inset Formula $w$
\end_inset

 with 
\begin_inset Formula $X^{T}\beta$
\end_inset

, with 
\begin_inset Formula $\beta\in\reals^{n}$
\end_inset

 unrestricted.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Matrix (or the Gram Matrix)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
For a set of 
\begin_inset Formula $\left\{ x_{1},\ldots,x_{n}\right\} $
\end_inset

 and an inner product 
\begin_inset Formula $\left\langle \cdot,\cdot\right\rangle $
\end_inset

on the set, the 
\series bold
kernel matrix
\series default
 or the 
\series bold
Gram matrix
\series default
 is defined as
\begin_inset Formula 
\[
K=\begin{pmatrix}\left\langle x_{i},x_{j}\right\rangle \end{pmatrix}_{i,j}=\begin{pmatrix}\left\langle x_{1},x_{1}\right\rangle  & \cdots & \left\langle x_{1},x_{n}\right\rangle \\
\vdots & \ddots & \cdots\\
\left\langle x_{n},x_{1}\right\rangle  & \cdots & \left\langle x_{n},x_{n}\right\rangle 
\end{pmatrix}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
Then for the standard Euclidean inner product 
\begin_inset Formula $\left\langle x_{i},x_{j}\right\rangle =x_{i}^{T}x_{j}$
\end_inset

, we have
\begin_inset Formula 
\[
K=XX^{T}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Some Vectorization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Regularization Term:
\begin_inset Formula 
\[
\|w\|^{2}=w^{T}w=\beta^{T}XX^{T}\beta=\beta^{T}K\beta
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Prediction on training point 
\begin_inset Formula $x_{i}$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
f(x_{i}) & = & b+x_{i}^{T}w\\
\pause & = & b+x_{i}^{T}\left(\sum_{j=1}^{n}\beta_{j}x_{j}\right)\\
\pause & = & b+\sum_{j=1}^{n}\beta_{j}K_{ij}
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized Primal SVM
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Putting it together, kernelized primal SVM is 
\begin_inset Formula 
\[
\min_{\beta\in\reals^{n},b\in\reals}\frac{1}{2}\beta^{T}K\beta+\frac{c}{n}\sum_{i=1}^{n}\left(1-y_{i}\left[b+\sum_{j=1}^{n}\beta_{j}K_{ij}\right]\right)_{+}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We can write this as a differentiable, constrained optimization problem:
\begin_inset Formula 
\begin{eqnarray*}
\textrm{minimize} &  & \frac{1}{2}\beta^{T}K\beta+\frac{c}{n}\mathbf{1}^{T}\xi\\
\textrm{subject to} &  & \xi\succeq0\\
 &  & \xi\succeq\left(\mathbf{1}-Y\left[b+K\beta\right]\right),
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $Y=\diag(y_{1},\ldots,y_{n})$
\end_inset

, 
\begin_inset Formula $\mathbf{1}$
\end_inset

 is a column vector of 
\begin_inset Formula $1$
\end_inset

's, and 
\begin_inset Formula $\succeq$
\end_inset

 represent element-wise vector inequality.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized Primal SVM: Kernel Trick
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Kernelized primal SVM is 
\begin_inset Formula 
\[
\min_{\beta\in\reals^{n},b\in\reals}\frac{1}{2}\beta^{T}K\beta+\frac{c}{n}\sum_{i=1}^{n}\left(1-y_{i}\left[b+\sum_{j=1}^{n}\beta_{j}K_{ij}\right]\right)_{+}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We derived this with 
\begin_inset Formula $K=XX^{T}$
\end_inset

, which corresponds to the linear kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Suppose we have another kernel defined in terms of a map 
\begin_inset Formula $\phi$
\end_inset

, i.e.
\begin_inset Formula 
\[
k(w,x)=\left\langle \phi(w),\phi(x)\right\rangle ,
\]

\end_inset

then we can just plug in the corresponding kernel matrix 
\begin_inset Formula $K_{\phi}$
\end_inset

 to the optimization problem above.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What kernels can be written as an inner product of feature vectors?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Kernelizing Ridge Regression
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Ridge Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Recall the ridge regression objective:
\begin_inset Formula 
\[
J(w)=||Xw-y||^{2}+\lambda||w||^{2}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Differentiating and setting equal to zero ,we get
\begin_inset Formula 
\begin{eqnarray*}
\left(X^{T}X+\lambda I\right)w & = & X^{T}y
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
On board to review?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelizing Ridge Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
So we have, for 
\begin_inset Formula $\lambda>0$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
(X^{T}X+\lambda I)w & = & X^{T}y\\
\pause\lambda w & = & X^{T}y-X^{T}Xw\\
\pause w & = & \frac{1}{\lambda}X^{T}(y-Xw)\\
\pause w & = & X^{T}\alpha
\end{eqnarray*}

\end_inset

 for 
\begin_inset Formula $\alpha=\lambda^{-1}(y-Xw)\in\reals^{n}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
So 
\begin_inset Formula $w$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset


\series bold
in the span of the data
\series default

\begin_inset Quotes erd
\end_inset

:
\begin_inset Formula 
\begin{align*}
w= & \begin{pmatrix}| & \cdots & |\\
x_{1} & \cdots & x_{n}\\
| & \cdots & |
\end{pmatrix}\begin{pmatrix}\alpha_{1}\\
\vdots\\
\alpha_{n}
\end{pmatrix}=\alpha_{1}x_{1}+\cdots\alpha_{n}x_{n}
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelizing Ridge Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
So plugging in 
\begin_inset Formula $w=X^{T}\alpha$
\end_inset

 to
\begin_inset Formula 
\begin{eqnarray*}
\alpha & = & \lambda^{-1}(y-Xw)\\
\pause\lambda\alpha & = & y-XX^{T}\alpha\\
\pause XX^{T}\alpha+\lambda\alpha & = & y\\
\pause\left(XX^{T}+\lambda I\right)\alpha & = & y\\
\pause\alpha & = & (\lambda I+XX^{T})^{-1}y
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
So we have 
\begin_inset Formula $\alpha$
\end_inset

.
 How to do prediction?
\begin_inset Formula 
\begin{eqnarray*}
\pause Xw & = & \pause X\left(X^{T}\alpha\right)\\
\pause & = & \left(XX^{T}\right)(\lambda I+XX^{T})^{-1}y
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
To predict on new data, need the 
\begin_inset Quotes eld
\end_inset

cross-kernel
\begin_inset Quotes erd
\end_inset

 matrix, between new and old data.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Suppose you have two sets of points in input space, say A={x1,…,xm} and
 B={w1,…,wn}.
 Then the cross kernel matrix for the sets A and B would be the matrix of
 size m×n for which the (i,j)'th entry has value k(xi,wj).
 In typical usage, you may have a cross-kernel matrix where A consists of
 your training points, and B consists of our test points where you want
 to make predictions.
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Mercer's Theorem
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Positive Semidefinite Matrices
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A real, symmetric matrix 
\begin_inset Formula $M\in\reals^{n\times n}$
\end_inset

 is 
\series bold
positive semidefinite (psd)
\series default
 if for any 
\begin_inset Formula $x\in\reals^{n}$
\end_inset

, 
\begin_inset Formula 
\[
x^{T}Mx\ge0.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Theorem
The following conditions are each necessary and sufficient for 
\begin_inset Formula $M$
\end_inset

 to be positive semidefinite:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $M$
\end_inset

 has a 
\begin_inset Quotes eld
\end_inset

square root
\begin_inset Quotes erd
\end_inset

, i.e.
 there exists 
\begin_inset Formula $R$
\end_inset

 s.t.
 
\begin_inset Formula $M=R^{T}R$
\end_inset

.
\end_layout

\begin_layout Itemize
All eigenvalues of 
\begin_inset Formula $M$
\end_inset

 are greater than or equal to 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Positive Semidefinite Function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A symmetric kernel function 
\begin_inset Formula $k:\cx\times\cx\to\reals$
\end_inset

 is 
\series bold
positive semidefinite (psd)
\series default
 if for any finite set 
\begin_inset Formula $\left\{ x_{1},\ldots,x_{n}\right\} \in\cx$
\end_inset

, the kernel matrix on this set 
\begin_inset Formula 
\[
K=\begin{pmatrix}k(x_{i},x_{j})\end{pmatrix}_{i,j}=\begin{pmatrix}k(x_{1},x_{1}) & \cdots & k(x_{1},x_{n})\\
\vdots & \ddots & \cdots\\
k(x_{n},x_{1}) & \cdots & k(x_{n},x_{n})
\end{pmatrix}
\]

\end_inset

is a positive semidefinite matrix.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Mercer's Theorem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
A symmetric function 
\begin_inset Formula $k(w,x)$
\end_inset

 can be expressed as an inner product
\begin_inset Formula 
\[
k(w,x)=\left\langle \phi(w),\phi(x)\right\rangle 
\]

\end_inset

for some 
\begin_inset Formula $\phi$
\end_inset

 if and only if 
\begin_inset Formula $k(w,x)$
\end_inset

 is 
\series bold
positive semidefinite.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If we start with a psd kernel, can we generate more?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Additive Closure
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k_{1}$
\end_inset

 and 
\begin_inset Formula $k_{2}$
\end_inset

 are psd kernels with feature maps 
\begin_inset Formula $\phi_{1}$
\end_inset

 and 
\begin_inset Formula $\phi_{2}$
\end_inset

, respectively.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula 
\[
k_{1}(w,x)+k_{2}(w,x)
\]

\end_inset

is a psd kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Concatenate the feature vectors to get 
\begin_inset Formula 
\[
\phi(x)=\left(\phi_{1}(x),\phi_{2}(x)\right).
\]

\end_inset

Then 
\begin_inset Formula $\phi$
\end_inset

 is a feature map for 
\begin_inset Formula $k_{1}+k_{2}$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closure under Positive Scaling
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k$
\end_inset

 is a psd kernel with feature maps 
\begin_inset Formula $\phi$
\end_inset

.
\end_layout

\begin_layout Itemize
Then for any 
\begin_inset Formula $\alpha>0$
\end_inset

, 
\begin_inset Formula 
\[
\alpha k
\]

\end_inset

is a psd kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Note that 
\begin_inset Formula 
\[
\phi(x)=\sqrt{\alpha}\phi(x)
\]

\end_inset

 is a feature map for 
\begin_inset Formula $\alpha k$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Scalar Function Gives a Kernel
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For any function 
\begin_inset Formula $f(x)$
\end_inset

, 
\begin_inset Formula 
\[
k(w,x)=f(w)f(x)
\]

\end_inset

is a kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Let 
\begin_inset Formula $f(x)$
\end_inset

 be the feature mapping.
 (It maps into a 1-dimensional feature space.)
\begin_inset Formula 
\[
\left\langle f(x),f(w)\right\rangle =f(x)f(w)=k(w,x).
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closure under Hadamard Products
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k_{1}$
\end_inset

 and 
\begin_inset Formula $k_{2}$
\end_inset

 are psd kernels with feature maps 
\begin_inset Formula $\phi_{1}$
\end_inset

 and 
\begin_inset Formula $\phi_{2}$
\end_inset

, respectively.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula 
\[
k_{1}(w,x)k_{2}(w,x)
\]

\end_inset

is a psd kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Take the outer product of the feature vectors: 
\begin_inset Formula 
\[
\phi(x)=\phi_{1}(x)\left[\phi_{2}(x)\right]^{T}.
\]

\end_inset

Note that 
\begin_inset Formula $\phi(x)$
\end_inset

 is a matrix.
\end_layout

\begin_layout Itemize
Continued...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closure under Hadamard Products
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Then
\begin_inset Formula 
\begin{eqnarray*}
\left\langle \phi(x),\phi(w)\right\rangle  & = & \sum_{i,j}\phi(x)\phi(w)\\
 & = & \sum_{i,j}\left[\phi_{1}(x)\left[\phi_{2}(x)\right]^{T}\right]_{ij}\left[\phi_{1}(w)\left[\phi_{2}(w)\right]^{T}\right]_{ij}\\
 & = & \sum_{i,j}\left[\phi_{1}(x)\right]_{i}\left[\phi_{2}(x)\right]_{j}\left[\phi_{1}(w)\right]_{i}\left[\phi_{2}(w)\right]_{j}\\
 & = & \left(\sum_{i}\left[\phi_{1}(x)\right]_{i}\left[\phi_{1}(w)\right]_{i}\right)\left(\sum_{j}\left[\phi_{2}(x)\right]_{j}\left[\phi_{2}(w)\right]_{j}\right)\\
 & = & k_{1}(w,x)k_{2}(w,x)
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Are RBFs Kernels?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let's consider the RBF kernel on 
\begin_inset Formula $\reals$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
k(w,x) & = & \exp\left(-\|w-x\|^{2}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Note that
\begin_inset Formula 
\begin{eqnarray*}
\|w-x\|^{2} & = & \left(w-x\right)^{T}\left(w-x\right)\\
\pause & = & w^{T}w+x^{T}x-2w^{T}x\\
\pause & = & \|w\|^{2}+\|x\|^{2}-2w^{T}x
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
So
\begin_inset Formula 
\begin{eqnarray*}
\exp\left(-\|w-x\|^{2}\right) & = & e^{-\|w\|^{2}}e^{-\|x\|^{2}}e^{2w^{T}x}
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Plain Layout
\begin_inset Separator parbreak
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Kernel Machines
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Feature Vectors from a Kernel
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
So what can we do with a kernel?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We can generate feature vectors:
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Idea: Characterize input 
\begin_inset Formula $x$
\end_inset

 by its similarity to 
\begin_inset Formula $r$
\end_inset

 fixed prototypes in 
\begin_inset Formula $\cx$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Definition
A 
\series bold
kernelized feature vector 
\series default
for an input 
\begin_inset Formula $x\in\cx$
\end_inset

 with respect to a kernel 
\begin_inset Formula $k$
\end_inset

 and prototype points 
\begin_inset Formula $\mu_{1},\ldots,\mu_{r}\in\cx$
\end_inset

 is given by
\begin_inset Formula 
\[
\Phi(x)=\left[k(x,\mu_{1}),\ldots,k(x,\mu_{r})\right]\in\reals^{r}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernel Machines
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A 
\series bold
kernel machine 
\series default
is a linear model with kernelized feature vectors.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
This corresponds to a prediction functions of the form
\begin_inset Formula 
\begin{eqnarray*}
f(x) & = & \alpha^{T}\Phi(x)\\
\pause & = & \sum_{i=1}^{r}\alpha_{i}k(x,\mu_{i}),
\end{eqnarray*}

\end_inset

for 
\begin_inset Formula $\alpha\in\reals^{r}$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
An Interpretation
\end_layout

\end_inset


\end_layout

\begin_layout Block
For each 
\begin_inset Formula $\mu_{i}$
\end_inset

, we get a function on 
\begin_inset Formula $\cx$
\end_inset

: 
\begin_inset Formula 
\[
x\mapsto k(x,\mu_{i})
\]

\end_inset


\begin_inset Formula $f(x)$
\end_inset

 is a linear combination of these functions.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ventureBeatArticle.png
	lyxscale 40
	width 40col%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ventreBeatBagOfWords.png
	lyxscale 40
	width 40col%

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernel Machine Basis Functions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
RBF kernel 
\begin_inset Formula $k(w,x)=\exp\left(-\left(w-x\right)^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Prototypes at 
\begin_inset Formula $\left\{ -6,-4,-3,0,2,4\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Corresponding basis functions:
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename kernel-fns-1.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernel Machine Prediction Functions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Basis functions
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename kernel-fns-1.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Predictions of the form
\begin_inset Formula 
\[
f(x)=\sum_{i=1}^{r}\alpha_{i}k(x,\mu_{i})
\]

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename kernel-fns-2.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
RBF Network
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
An 
\series bold
RBF network 
\series default
is a linear model with an RBF kernel.
\end_layout

\begin_deeper
\begin_layout Itemize
First described in 1988 by Broomhead and Lowe (neural network literature)
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename kernel-fns-2.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Characteristics:
\end_layout

\begin_deeper
\begin_layout Itemize
Nonlinear
\end_layout

\begin_layout Itemize
Smoothness depends on RBF kernel bandwidth
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
How to Choose Prototypes
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Uniform grid on space?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
only feasible in low dimensions
\end_layout

\begin_layout Itemize
where to focus the grid?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Cluster centers of training data?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Possible, but clustering is difficult in high dimensions
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize

\series bold
Use all (or a subset of) the training points
\end_layout

\begin_deeper
\begin_layout Itemize
Most common approach for kernel methods
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
All Training Points as Prototypes
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider training inputs 
\begin_inset Formula $x_{1},\ldots,x_{n}\in\cx$
\end_inset


\end_layout

\begin_layout Itemize
Then
\begin_inset Formula 
\[
f(x)=\sum_{i=1}^{n}\alpha_{i}k(x,x_{i}).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Requires all training examples for prediction?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Not quite: Only need 
\begin_inset Formula $x_{i}$
\end_inset

 for 
\begin_inset Formula $\alpha_{i}\neq0$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Want 
\begin_inset Formula $\alpha_{i}$
\end_inset

's to be sparse.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Train with 
\begin_inset Formula $\ell_{1}$
\end_inset

 regularization: 
\series bold

\begin_inset Formula $\ell_{1}$
\end_inset

-regularized vector machine
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
[Will show SVM also gives sparse functions of this form.]
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
\begin_inset Formula $\ell_{1}$
\end_inset

-Regularized Vector Machine
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
RBF Kernel with bandwidth 
\begin_inset Formula $\sigma=0.3$
\end_inset

.
\end_layout

\begin_layout Itemize
Linear hypothesis space: 
\begin_inset Formula $\cf=\left\{ f(x)=\sum_{i=1}^{n}\alpha_{i}k(x,x_{i})\mid\alpha\in\reals^{n}\right\} $
\end_inset

.
\end_layout

\begin_layout Itemize
Logistic loss function: 
\begin_inset Formula $\ell(y,\hat{y})=\log\left(1+e^{-y\hat{y}}\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\ell_{1}$
\end_inset

-regularization, 
\begin_inset Formula $n=200$
\end_inset

 training points
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig14.4b.pdf
	width 45col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{KPM Figure 14.4b}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
\begin_inset Formula $\ell_{2}$
\end_inset

-Regularized Vector Machine
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
RBF Kernel with bandwidth 
\begin_inset Formula $\sigma=0.3$
\end_inset

.
\end_layout

\begin_layout Itemize
Linear hypothesis space: 
\begin_inset Formula $\cf=\left\{ f(x)=\sum_{i=1}^{n}\alpha_{i}k(x,x_{i})\mid\alpha\in\reals^{n}\right\} $
\end_inset

.
\end_layout

\begin_layout Itemize
Logistic loss function: 
\begin_inset Formula $\ell(y,\hat{y})=\log\left(1+e^{-y\hat{y}}\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\ell_{2}$
\end_inset

-regularization, 
\begin_inset Formula $n=200$
\end_inset

 training points
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig14.4a.pdf
	width 45col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{KPM Figure 14.4a}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Example: Vector Machine for Ridge Regression
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
\begin_inset Formula $\ell_{2}$
\end_inset

-Regularized Vector Machine for Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Kernel function 
\begin_inset Formula $k:\cx\times\cx\to\reals$
\end_inset

 is symmetric (but nothing else).
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Hypothesis space (linear functions on kernelized feature vector)
\begin_inset Formula 
\[
\cf=\left\{ f_{\alpha}(x)=\sum_{i=1}^{n}\alpha_{i}k(x,x_{i})\mid\alpha\in\reals^{n}\right\} .
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Objective function (square loss with 
\begin_inset Formula $\ell_{2}$
\end_inset

 regularization): 
\begin_inset Formula 
\[
J(\alpha)=\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-f_{\alpha}(x_{i})\right)^{2}+\lambda\alpha^{T}\alpha,
\]

\end_inset

where 
\begin_inset Formula 
\[
f_{\alpha}(x_{i})=\sum_{j=1}^{n}\alpha_{j}k(x_{i},x_{j}).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Note: All dependence on 
\begin_inset Formula $x$
\end_inset

's is via the kernel function
\series default
.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Matrix
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Note that
\begin_inset Formula 
\[
f(x_{i})=\sum_{j=1}^{n}\alpha_{j}k(x_{i},x_{j})
\]

\end_inset

only depends on the kernel function on all pairs of 
\begin_inset Formula $n$
\end_inset

 training points.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Definition
The 
\series bold
kernel matrix
\series default
 for a kernel 
\begin_inset Formula $k$
\end_inset

 on a set 
\begin_inset Formula $\left\{ x_{1},\ldots,x_{n}\right\} $
\end_inset

 as
\begin_inset Formula 
\[
K=\begin{pmatrix}k(x_{i},x_{j})\end{pmatrix}_{i,j}=\begin{pmatrix}k(x_{1},x_{1}) & \cdots & k(x_{1},x_{n})\\
\vdots & \ddots & \cdots\\
k(x_{n},x_{1}) & \cdots & k(x_{n},x_{n})
\end{pmatrix}\in\reals^{n\times n}.
\]

\end_inset


\end_layout

\begin_layout Definition

\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Vectorizing the Vector Machine
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Claim: 
\begin_inset Formula $K\alpha$
\end_inset

 gives the prediction vector 
\begin_inset Formula $\left(f_{\alpha}(x_{1}),\ldots,f_{\alpha}(x_{n})\right)^{T}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\pause K\alpha & = & \begin{pmatrix}k(x_{1},x_{1}) & \cdots & k(x_{1},x_{n})\\
\vdots & \ddots & \cdots\\
k(x_{n},x_{1}) & \cdots & k(x_{n},x_{n})
\end{pmatrix}\begin{pmatrix}\alpha_{1}\\
\vdots\\
\alpha_{n}
\end{pmatrix}\\
\pause & = & \begin{pmatrix}\alpha_{1}k(x_{1},x_{1})+\cdots+\alpha_{n}k(x_{1},x_{n})\\
\vdots\\
\alpha_{1}k(x_{n},x_{1})+\cdots+\alpha_{n}k(x_{1,}x_{n})
\end{pmatrix}\\
\pause & = & \begin{pmatrix}f_{\alpha}(x_{1})\\
\vdots\\
f_{\alpha}(x_{n})
\end{pmatrix}.
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Vectorizing the Vector Machine
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The 
\begin_inset Formula $i$
\end_inset

th residual is 
\begin_inset Formula $y_{i}-f_{\alpha}(x_{i})$
\end_inset

.
 We can vectorize as:
\begin_inset Formula 
\begin{eqnarray*}
y-K\alpha & = & \begin{pmatrix}y_{1}-f_{\alpha}(x_{1})\\
\vdots\\
y_{n}-f_{\alpha}(x_{n})
\end{pmatrix}
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Sum of square residuals is
\begin_inset Formula 
\[
\left(y-K\alpha\right)^{T}\left(y-K\alpha\right)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Objective function:
\begin_inset Formula 
\[
J(\alpha)=\frac{1}{n}\|y-K\alpha\|^{2}+\lambda\alpha^{T}\alpha
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Vectorizing the Vector Machine
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset

 and 
\begin_inset Formula $k(w,x)=w^{T}x$
\end_inset

 (linear kernel)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $X\in\reals^{n\times d}$
\end_inset

 be the 
\series bold
design matrix
\series default
, which has each input vector as a row: 
\begin_inset Formula 
\[
X=\begin{pmatrix}-x_{1}-\\
\vdots\\
-x_{n}-
\end{pmatrix}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then the kernel matrix is
\begin_inset Formula 
\[
K=XX^{T}=\begin{pmatrix}-x_{1}-\\
\vdots\\
-x_{n}-
\end{pmatrix}\begin{pmatrix}| & \cdots & |\\
x_{1} & \cdots & x_{n}\\
| & \cdots & |
\end{pmatrix}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
And the objective function is
\begin_inset Formula 
\[
J(\alpha)=\frac{1}{n}\|y-XX^{T}\alpha\|^{2}+\lambda\alpha^{T}\alpha
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Features vs Kernels
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Features vs Kernels
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
Suppose a kernel can be written as an inner product: 
\begin_inset Formula 
\[
k(w,x)=\left\langle \phi(w),\phi(x)\right\rangle .
\]

\end_inset

Then the kernel machine is
\series bold
 
\series default
a
\series bold
 linear classifier 
\series default
with feature map 
\begin_inset Formula $\phi(x)$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Mercer's Theorem characterizes kernels with these properties.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Features vs Kernels
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Proof
For prototype points 
\begin_inset Formula $x_{1},\ldots,x_{r}$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
f(x) & = & \sum_{i=1}^{r}\alpha_{i}k(x,x_{i})\\
\pause & = & \sum_{i=1}^{r}\alpha_{i}\left\langle \phi(x),\phi(x_{i})\right\rangle \\
\pause & = & \left\langle \sum_{i=1}^{r}\alpha_{i}\phi(x_{i}),\phi(x)\right\rangle \\
\pause & = & w^{T}\phi(x)
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $w=\sum_{i=1}^{r}\alpha_{i}\phi(x_{i})$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\end_body
\end_document
