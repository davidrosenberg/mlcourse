\section{Pre-Lecture 2: Optimization and linear algebra}
\textbf{Instructions}: Prior to lecture 2, please review the following problems
\subsection{Optimization Prerequisites for Lasso}
\begin{enumerate}
\item Given $a\in\RR$ we define $a^+,a^-$ as follows:
  $$a^+ = \left\{\begin{array}{ll}a & \text{if
  $a\geq 0$,}\\0&\text{otherwise,}\end{array}\right.
  \quad\text{and}\quad
  a^- = \left\{\begin{array}{ll}-a & \text{if
  $a<0$,}\\0&\text{otherwise.}\end{array}\right.$$
  We call $a^+$ the \textit{positive part} of $a$ and $a^-$ the \textit{negative part}
  of $a$.  Note that $a^+,a^-\geq0$.
  \begin{enumerate}
  \item Give an expression for $a$ in terms of $a^+,a^-$.
  \item Give an expression for $|a|$ in terms of $a^+,a^-$.
  \item[] For $x\in\RR^d$ define $x^+=(x_1^+,\ldots,x_d^+)$ and
    $x^-=(x_1^-,\ldots,x_d^-)$.
  \item Give an expression for $x$ in terms of $x^+,x^-$.
  \item Give an expression for $\|x\|_1$ without using any summations
    or absolute values. [Hint: Use $x^+,x^-$ and the
      vector $\mathbf{1}=(1,1,\ldots,1)\in\RR^d$.]
  \end{enumerate}
\begin{solution}
\item[]\Sol
  \begin{enumerate}
  \item $a=a^+-a^-$
  \item $|a|=a^++a^-$
  \item $x=x^+-x^-$
  \item $\|x\|_1=\mathbf{1}^T(x^++x^-)$
  \end{enumerate}
\end{solution}
\item Let $f:\RR\to\RR$ and $S\subseteq\RR$.  Consider the two
  optimization problems
  $$\begin{array}{ll}
    \text{minimize}_{x\in\RR}  & |x|\\
    \text{subject to} & f(x)\in S\\
    \vphantom{a,b}
  \end{array}
  \qquad\text{and}\qquad
  \begin{array}{ll}
    \text{minimize}_{a,b\in\RR}  & a+b\\
    \text{subject to} & f(a-b)\in S\\
    & a,b\geq 0.
  \end{array}$$
  Solve the following questions.
  \begin{enumerate}
  \item If $x$ in the first problem satisfies $f(x)\in S$ show how to quickly
    compute $(a,b)$ for the second problem with $a+b=|x|$ and
    $f(a-b)\in S$.
  \item If $a,b$ in the second problem
    satisfy $f(a-b)\in S$, show how to quickly compute an
    $x$ for the first problem with $|x|\leq a+b$ and $f(x)\in S$.
  \item Assume $x$ is a minimizer for the first problem with minimum
    value $p_1^*$ and $(a,b)$ is a minimizer for the second problem
    with minimum $p_2^*$.  Using the previous two parts, conclude
    that $p_1^*=p_2^*$.
  \end{enumerate}
\begin{solution}
\item[]\Sol
  \begin{enumerate}
  \item Let $a=x^+$ and $b=x^-$.  Then $a+b=|x|$ and $a-b=x$.
  \item Let $x=a-b$ and note that $|x|=|a-b|\leq |a|+|b|=a+b$.
  \item Part a) shows $p_2^*\leq p_1^*$ by letting $\hat{a}=x^+$ and
    $\hat{b}=x^-$. Part b) shows $p_1^*\leq p_2^*$ by letting $\hat{x}=a-b$.
  \end{enumerate}
\end{solution}
\item Let $f:\RR^d\to\RR$, $S\subseteq\RR$ and consider the following optimization problem:
  $$\begin{array}{ll}
  \text{minimize}_{x\in\RR^d} & \|x\|_1 \\
  \text{subject to} & f(x)\in S,
  \end{array}$$
  where $\|x\|_1 = \sum_{i=1}^d |x_i|$.
  Give a new optimization problem with a linear objective function and the same minimum
  value.  Show how to convert a solution to your new problem into a
  solution to the given problem. [Hint: Use the previous two problems.]
\begin{solution}
\item[]\Sol Consider the minimization problem
  $$\begin{array}{ll}
  \text{minimize}_{a,b\in\RR^d} & \mathbf{1}^T(a+b)\\
  \text{subject to} & f(a-b)\in S,\\
  & a_i,b_i \geq 0\quad\text{for $i=1,\ldots,d$.}
  \end{array}$$
  Let $p_1^*$ be the minimum for the original problem, and $p_2^*$ the
  minimum for our new problem.  We first show $p_1^*=p_2^*$.
  Suppose $x$ is a minimizer for the original problem and let $a=x^+$
  and $b=x^-$.  Then by the first question $\mathbf{1}^T(a+b)=\|x\|_1$
  and $a-b=x$.  This shows $p_2^*\leq p_1^*$.  Next suppose $(a,b)$ is
  a minimizer for our new problem, and let $x=a-b$.  Then
  $$\|x\|_1 = \|a-b\|_1 = \sum_{i=1}^d |a_i-b_i| \leq \sum_{i=1}^d |a_i|+|b_i|
  =\sum_{i=1}^d a_i+b_i = \mathbf{1}^T(a+b).$$
  This proves $p_1^*\leq p_2^*$.

  Finally, given a minimizer $(a,b)$ for the new problem we recover a
  minimizer $x$ for the original problem by letting $x=a-b$.
\end{solution}
\end{enumerate}

%
\subsection{Ellipsoids}
\begin{enumerate}
\item $(\star)$ Describe the following set geometrically:
  $$\left\{v\in\RR^2\;\middle|\;
  v^T\pMatt{2&2}{0&2}v = 4\right\}.$$
\begin{solution}
\item[]\Sol The set is an ellipse with semi-axis lengths $2/\sqrt{3}$ and $2$
  rotated counter-clockwise by $\pi/4$.
Letting $v=(x,y)^T$ and multiplying all terms we get
$$2x^2+2xy+2y^2=4.$$
From precalculus we can see this is a conic section, and must be an
ellipse or a hyperbola, but more work is needed to determine which
one.  Instead of proceeding along these lines,
let's use linear algebra to give a cleaner treatment that extends to
higher dimensions.

Let $A=\pMatt{2&2}{0&2}$.  Since $v^TAv$ is a number, we must have
$(v^TAv)^T=v^TA^Tv$.  This gives
$$v^TA^Tv=v^TAv = \frac{1}{2}v^T(A^T+A)v = v^T\pMatt{2&1}{1&2}v.$$
Our new matrix is symmetric, and thus allows us to apply the spectral
theorem to diagonalize it with an orthonormal basis of eigenvectors.
In other words, by rotating our axes we can get a diagonal matrix.
Either doing this by hand, or using a computer (Matlab, Mathematica,
Numpy) we obtain
$$\pMatt{2&1}{1&2} = Q\pMatt{3&0}{0&1}Q^T \quad\text{where}\quad
Q=\frac{1}{\sqrt{2}}\pMatt{1&-1}{1&1}=\pMatt{\cos(\pi/4)&-\sin(\pi/4)}{\sin(\pi/4)&\cos(\pi/4)}.$$
The set
$$\left\{w\in\RR^2\;\middle|\;
  w^T\pMatt{3&0}{0&1}w = 4\right\}$$
is an ellipse with semi-axis lengths $2/\sqrt{3}$ and $2$
since it corresponds to the equation $3w_1^2+w_2^2=4$.
Since $Q$ performs a counter-clockwise rotation by $\pi/4$ we obtain
the answer.  More concretely,
$$w^T\pMatt{3&0}{0&1}w=4\iff (Qw)^TQ\pMatt{3&0}{0&1}Q^T(Qw)=4\iff
(Qw)^T\pMatt{2&1}{1&2}(Qw)=4$$
so
$$\{v\mid v^TAv=4\} = \left\{Qw\;\middle|\; w^T\pMatt{3&0}{0&1}w=4\right\}.$$
\begin{figure}[H]
\centering
\begin{asy}
  include "2-PreLec-Check/Ellipse.asy";
\end{asy}
\qquad
\begin{asy}
  include "2-PreLec-Check/RotEllipse.asy";
\end{asy}
\caption{Rotated Ellipse}
\end{figure}

More generally, the solution to $v^TAv=c$ for $v\in\RR^n$,
$A\in\RR^{n\times n}$ and $c>0$ will be an ellipsoid if
$A$ is positive definite.  The $i$th semi-axis will have length
$\sqrt{c/\lambda_i}$ where $\lambda_i$ is the $i$th eigenvalue of~$A$.
\end{solution}
\end{enumerate}

\subsection{($\star$) Linear Algebra Prerequisites for Linear Regressions}

\begin{enumerate}
\item When performing linear regression we obtain the \textit{normal
  equations} $A^TAx=A^Ty$ where $A\in\RR^{m\times n}$, $x\in \RR^n$,
  and $y\in\RR^m$.
  \begin{enumerate}
  \item If $\rank(A)=n$ then solve the normal equations for $x$.
  \item $(\star)$ What if $\rank(A)\neq n$?
  \end{enumerate}
\begin{solution}
\item[]\Sol
  \begin{enumerate}
  \item We first show that $\rank(A^TA)=n$ to show that we can invert
    $A^TA$.  By the rank-nullity theorem, we can do this by showing
    $A^TA$ has trivial nullspace.  Note that for any $x\in\RR^n$ we have
    $$A^TAx=0\implies x^TA^TAx=0 \implies \|Ax\|_2^2=0 \implies Ax=0
    \implies x=0.$$
    This last implication follows since $\rank(A)=n$ so $A$ has
    trivial nullspace (again by rank-nullity).  This proves
    $A^TA$ has a trivial nullspace, and thus $A^TA$ is
    invertible. Applying the inverse we obtain
    $$x = (A^TA)^{-1}A^Ty.$$
    Since $A^TA$ is invertible, our answer for $x$ is unique.
  \item We will show that the equation always has infinitely many
    solutions~$x$. First note that $\rank(A)\neq n$ implies
    $\rank(A)<n$ since you cannot have larger rank than the number of
    columns. Next, recall $\rank(A) = \rank(A^TA)$. Hence, by rank-nullity, $A^TA$ has a non-trivial nullspace,
    which in turn implies that if there is a solution, there must be
    infinitely many solutions.\\

    Next note $A^T$ and $A^TA$ have the same column space. To see this, first note that every vector of the form
    $A^TAx$ must be a linear combination of the columns of $A^T$,
    and thus lies in the column space of $A^T$.  Since $\rank(A^TA) = \rank(A) = \rank(A^T)$, this implies
    $A^T$ and $A^TA$ have the same column spaces.

    A specific solution can be computed as $x=(A^TA)^+ A^Ty$, where
    $(A^TA)^+$ is the \textit{pseudoinverse} of $A^TA$.  Of the infinitely
    many possible solutions $x$, this gives the one that minimizes
    $\|x\|_2$.  More precisely, $x=(A^TA)^+ A^Ty$ solves the
    optimization problem
    $$\begin{array}{ll}
      \text{minimize} & \|x\|_2\\
      \text{subject to} & A^TAx=A^Ty.
    \end{array}$$
  \end{enumerate}
\end{solution}
\item Prove that $A^TA+\lambda \Id_{n\times n}$ is invertible if
  $\lambda>0$ and $A\in\RR^{n\times n}$.
\begin{solution}
\item[]\Sol If $(A^TA+\lambda\Id_{n\times n})x=0$ then
  $$0=x^T(A^TA+\lambda\Id_{n\times n})x = \|Ax\|_2^2 + \lambda\|x\|_2^2
  \implies x=0.$$
  Thus $A^TA+\lambda\Id_{n\times n}$ has trivial nullspace.
  Alternatively, we could notice that $A^TA$ is positive semidefinite,
  so adding $\lambda\Id_{n\times n}$ will give a matrix whose
  eigenvalues are all at least $\lambda>0$.  A square matrix is
  invertible iff its eigenvalues are all non-zero.
\end{solution}
\end{enumerate}
