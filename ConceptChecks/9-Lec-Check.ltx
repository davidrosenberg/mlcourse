\section{Week 9 Lecture: Concept Check Exercises}
\subsection{Trees}
\begin{enumerate}
\item 
  \begin{enumerate}
  \item How many regions (leaves) will a tree with $k$ node splits have?
  \item What is the maximum number of regions a tree of height $k$ can
    have?  Recall that the height of a tree is the number of edges in
    the longest path from the root to any leaf.
  \item Give an upper bound on the depth needed to exactly classify
    $n$ distinct points in $\RR^d$.  [Hint: In the worst case each 
    leaf will have a single training point.]
  \end{enumerate}
\begin{solution}
\item[]\Sol
  \begin{enumerate}
  \item Given a  fixed tree, if we split a leaf node we add a single
    leaf to the tree.  Thus $k$ splits corresponds to $k+1$ leaves.
  \item A tree of height $k$ can have at most $2^k$ regions (leaves).
  \item A tree of height $\lceil\log_2(n)\rceil$ is sufficient to
    distinguish all possible values for the first feature.  At each
    leaf we can then put another tree of this height that
    distinguishes the second feature, and so forth.
    These give an upper bound of $d\lceil \log_2(n)\rceil$.
  \end{enumerate}
\end{solution}
\item This question involves fitting a regression tree using the
  square loss. Assume the $n$ data points for the current node are sorted by
  the first feature.  Give pseudocode with $O(n)$ runtime
  for optimally splitting the current node with respect to the first
  feature.
\begin{solution}
\item[]\Sol
\lstinputlisting[language=Python,tabsize=2,basicstyle=\small,columns=fullflexible]{9-Lec-Check/buildnode.py}
\end{solution}
\item Suppose we are looking at a fixed node of a classification tree,
  and the class labels are, sorted by the first feature values,
  $$4,1,0,0,1,0,2,3,3.$$
  We are currently testing splitting the node into a left node
  containing $4,1,0,0,1,0$ and a right node containing
  $2,3,3$.  For each of the following impurity measures, give the
  value for the left and right parts, along with the total score for
  the split.
  \begin{enumerate}
  \item Misclassification error.
  \item Gini index.
  \item Entropy.
  \end{enumerate}
\begin{solution}
\item[]\Sol 
  \begin{enumerate}
  \item Left: $3/6$, Right: $1/3$, Total: $6(3/6)+3(1/3)=4$
  \item Left: $3/6(3/6)+2/6(4/6)+1/6(5/6)=22/36$, Right:
    $1/3(2/3)+2/3(1/3)=4/9$, Total: $6(22/36)+3(4/9)=30/6=5$
  \item Left: $-3/6\log(3/6)-2/6\log(2/6)-1/6\log(1/6)$, Right:
    $-1/3\log(1/3)-2/3\log(2/3)$, Total:
    $$6[-3/6\log(3/6)-2/6\log(2/6)-1/6\log(1/6)] + 3[-1/3\log(1/3)-2/3\log(2/3)].$$
  \end{enumerate}
\end{solution}
\end{enumerate}
\subsection{Bagging}
\begin{enumerate}
\item Let $X_1,\ldots,X_n$ be an i.i.d.~sample from a distribution
  with mean $\mu$ and variance $\sigma^2$.  How large must $n$ be so
  that the sample mean has standard error smaller than $.01$?
\begin{solution}
\item[]\Sol Recall that the sample mean has variance
  $$\Var\left(\frac{1}{n}\sum_{i=1}^n X_i\right) =
  \frac{\Var(X_1)}{n} = \frac{\sigma^2}{n},$$
  with standard error $\sigma/\sqrt{n}$.  Thus we have
  $$\sigma/\sqrt{n} < .01 \iff n > 10000\sigma^2.$$
\end{solution}
\item Let $X_1,\ldots,X_{2n+1}$ be an i.i.d.~sample from a distribution.
  To estimate the median of the distribution, you can compute the sample
  median of the data.  
  \begin{enumerate}
  \item Give pseudocode that computes an estimate of the variance of
    the sample median.
  \item Give pseudocode that computes an estimate of a 95\% confidence
    interval for the sample median.
  \end{enumerate}
\begin{solution}
\item[]\Sol
  \begin{enumerate}
  \item 
    \begin{enumerate}
    \item Draw $B$ bootstrap samples $D^{1},\ldots,D^B$ each of size
      $2n+1$.  The samples are formed by drawing uniformly with replacement from
      the original data set $X_1,\ldots,X_{2n+1}$.
      We will make a total of $B(2n+1)$ draws.
    \item For each $D^{i}$ compute the corresponding median $\hat{m}_i$.
    \item Compute the sample variance of the $B$ medians $m_1,\ldots,m_B$.
    \end{enumerate}
  \item 
    \begin{enumerate}
    \item Draw $B$ bootstrap samples $D^{1},\ldots,D^B$ each of size
      $2n+1$.  The samples are formed by drawing uniformly with replacement from
      the original data set $X_1,\ldots,X_{2n+1}$.
      We will make a total of $B(2n+1)$ draws.
    \item For each $D^{i}$ compute the corresponding median $\hat{m}_i$.
    \item Compute the $2.5\%$ and $97.5\%$ sample quantiles of the
      list $\hat{m}_1,\ldots,\hat{m}_B$.
      Use these as the estimates of the left and right endpoints of 
      the confidence interval, respectively.
    \end{enumerate}
  \end{enumerate}
\end{solution}
\end{enumerate}
\subsection{Boosting}
\begin{enumerate}
\item $(\star)$ Show the exponential margin loss is a convex upper bound for the
  $0-1$ loss.
\begin{solution}
\item[]\Sol Recall that the exponential margin loss is given by
  $\ell(y,a)=e^{-ya}$ where $y\in\{-1,1\}$ and $a\in\RR$, and the
  $0-1$ loss is $\Ind(y\neq\sgn(a))$. If
  $\sgn(y)\neq a$ then $ya\leq 0$ and
  $$e^{-ya}\geq 1-ya \geq 1 = \Ind(y\neq\sgn(a)).$$
  In general $e^{-ya}\geq0$ so the we obtain the upper bound.  To
  prove convexity, we compute the second derivative and note that it
  is positive:
  $$\frac{\partial^2}{\partial a^2}e^{-ya} = y^2e^{-ya} > 0.$$
\end{solution}
\item Show how to perform gradient boosting with the hinge loss.
\begin{solution}
\item[]\Sol Recall that the hinge loss is given by $\ell(y,a) =
  \max(0,1-ya)$.  Define $g$ by
  $$g(y,a) = \left\{\begin{array}{ll} -y & \text{if $1-ya>0$,}\\0&\text{else}.\end{array}\right.$$
  Then $g(y,a)$ is a subgradient of $\ell(y,a)$ with respect to $a$.
  At stage $m$ of gradient boosting, we alredy have
  formed
  $$f_{m-1} = \sum_{i=1}^{m-1} \nu_i h_i.$$
  We then compute the pseudoresiduals $r_m$ given by
  $$r_m =
  -\left(g(y_1,f_{m-1}(x_1)),\ldots,g(y_n,f_{m-1}(x_n))\right).$$
  After building the mock dataset
  $D^{m}=\{(x_1,(r_m)_1),\ldots,(x_n,(r_m)_n)\}$ we perform a least
  squares fit to obtain $h_m\in\HH$.  Then we can determine $\nu_m$
  (usually a small fixed value).  Finally we let $f_m = f_{m-1}+\nu_mh_m$.
\end{solution}
\item Suppose we are using gradient boosting.  On
  each step we can do a better job of fitting the pseudoresiduals if
  we allow for deeper trees.  Why might deep trees be discouraged
  while gradient boosting?
\begin{solution}
\item[]\Sol Deep trees can lead to overfitting the data.
\end{solution}
\end{enumerate}
