{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "In this homework we will explore different ways of handling categorical variables in decision trees and weigh pros\n",
    "and cons of each approach. In particular, we will see how hot encoding categorical variables yields poor performance if the\n",
    "number of possible values that a categorical variable can assume is high. We will also see how categorical variables can be\n",
    "handled directly without hot-encoding them in a natural way using decision trees. \n",
    "## 2. Dataset:\n",
    "In this homework we will work with a synthetically created dataset with known interactions between variables$^{1}$\n",
    "It contains one real valued variable $z$ and a categorical variable $c$ with 200 different possible level that strongly correlate\n",
    "with the observed variable $y$. Formally they follow the following relation:\n",
    "\n",
    "$$ y = \n",
    "\\begin{cases}\n",
    "    0, &\\text{if  } c \\in C^+ \\text{ or } z > 10 \\\\\n",
    "    1, &\\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "where $C+$ is the set of categories whose name begin with letter **A** and $C-$ the set that begin with letter **B**.\n",
    "Also the dataset contains 100 other real valued variables $x_i, i \\in (0, 99)$ with varying (but non-significant)\n",
    "correlations with the observed variable that serve the purpose of noise. \n",
    "The dataset contains 10000 examples in total.  \n",
    "\n",
    "A small subset of the data looks like the following:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>c</th>\n",
       "      <th>z</th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>...</th>\n",
       "      <th>x90</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x93</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Adk</td>\n",
       "      <td>3.249</td>\n",
       "      <td>-8.121</td>\n",
       "      <td>4.225</td>\n",
       "      <td>3.939</td>\n",
       "      <td>2.177</td>\n",
       "      <td>-4.563</td>\n",
       "      <td>-1.281</td>\n",
       "      <td>-18.063</td>\n",
       "      <td>...</td>\n",
       "      <td>4.282</td>\n",
       "      <td>-13.735</td>\n",
       "      <td>-4.926</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-4.438</td>\n",
       "      <td>-4.954</td>\n",
       "      <td>5.147</td>\n",
       "      <td>12.147</td>\n",
       "      <td>5.261</td>\n",
       "      <td>5.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adg</td>\n",
       "      <td>9.412</td>\n",
       "      <td>-22.232</td>\n",
       "      <td>6.170</td>\n",
       "      <td>-4.890</td>\n",
       "      <td>5.541</td>\n",
       "      <td>5.982</td>\n",
       "      <td>7.018</td>\n",
       "      <td>-4.786</td>\n",
       "      <td>...</td>\n",
       "      <td>7.122</td>\n",
       "      <td>-10.226</td>\n",
       "      <td>4.726</td>\n",
       "      <td>6.512</td>\n",
       "      <td>-10.393</td>\n",
       "      <td>6.112</td>\n",
       "      <td>9.457</td>\n",
       "      <td>-12.911</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>9.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Acs</td>\n",
       "      <td>9.740</td>\n",
       "      <td>-2.892</td>\n",
       "      <td>-1.393</td>\n",
       "      <td>6.831</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>-23.420</td>\n",
       "      <td>7.991</td>\n",
       "      <td>18.532</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.196</td>\n",
       "      <td>7.716</td>\n",
       "      <td>-15.627</td>\n",
       "      <td>-28.180</td>\n",
       "      <td>5.727</td>\n",
       "      <td>4.693</td>\n",
       "      <td>-12.137</td>\n",
       "      <td>12.972</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-7.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Acb</td>\n",
       "      <td>16.043</td>\n",
       "      <td>6.197</td>\n",
       "      <td>-4.527</td>\n",
       "      <td>-12.651</td>\n",
       "      <td>-15.924</td>\n",
       "      <td>12.013</td>\n",
       "      <td>5.821</td>\n",
       "      <td>13.547</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.758</td>\n",
       "      <td>-13.935</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>8.221</td>\n",
       "      <td>1.481</td>\n",
       "      <td>11.609</td>\n",
       "      <td>9.120</td>\n",
       "      <td>-1.903</td>\n",
       "      <td>-8.438</td>\n",
       "      <td>15.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Aas</td>\n",
       "      <td>13.717</td>\n",
       "      <td>6.480</td>\n",
       "      <td>7.840</td>\n",
       "      <td>-5.849</td>\n",
       "      <td>16.306</td>\n",
       "      <td>0.506</td>\n",
       "      <td>-14.651</td>\n",
       "      <td>4.347</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.990</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.416</td>\n",
       "      <td>-18.412</td>\n",
       "      <td>-8.506</td>\n",
       "      <td>4.638</td>\n",
       "      <td>-9.237</td>\n",
       "      <td>-1.395</td>\n",
       "      <td>19.512</td>\n",
       "      <td>-4.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Bca</td>\n",
       "      <td>5.956</td>\n",
       "      <td>7.167</td>\n",
       "      <td>15.060</td>\n",
       "      <td>0.331</td>\n",
       "      <td>-9.986</td>\n",
       "      <td>-9.756</td>\n",
       "      <td>-6.632</td>\n",
       "      <td>8.357</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.747</td>\n",
       "      <td>-12.302</td>\n",
       "      <td>-11.957</td>\n",
       "      <td>-6.538</td>\n",
       "      <td>11.635</td>\n",
       "      <td>7.446</td>\n",
       "      <td>7.618</td>\n",
       "      <td>3.729</td>\n",
       "      <td>-4.476</td>\n",
       "      <td>-3.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Bdk</td>\n",
       "      <td>2.923</td>\n",
       "      <td>24.327</td>\n",
       "      <td>-5.232</td>\n",
       "      <td>4.530</td>\n",
       "      <td>-7.362</td>\n",
       "      <td>7.063</td>\n",
       "      <td>1.951</td>\n",
       "      <td>7.538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>-7.271</td>\n",
       "      <td>0.213</td>\n",
       "      <td>1.155</td>\n",
       "      <td>2.093</td>\n",
       "      <td>-6.612</td>\n",
       "      <td>-1.755</td>\n",
       "      <td>12.442</td>\n",
       "      <td>-5.582</td>\n",
       "      <td>1.631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Bal</td>\n",
       "      <td>6.572</td>\n",
       "      <td>10.886</td>\n",
       "      <td>-4.413</td>\n",
       "      <td>8.222</td>\n",
       "      <td>7.348</td>\n",
       "      <td>-2.873</td>\n",
       "      <td>-1.676</td>\n",
       "      <td>-6.342</td>\n",
       "      <td>...</td>\n",
       "      <td>6.884</td>\n",
       "      <td>-7.649</td>\n",
       "      <td>6.742</td>\n",
       "      <td>16.675</td>\n",
       "      <td>-1.460</td>\n",
       "      <td>-6.662</td>\n",
       "      <td>-8.200</td>\n",
       "      <td>-1.357</td>\n",
       "      <td>15.118</td>\n",
       "      <td>-0.891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>Bcx</td>\n",
       "      <td>1.101</td>\n",
       "      <td>7.103</td>\n",
       "      <td>2.293</td>\n",
       "      <td>10.635</td>\n",
       "      <td>-10.746</td>\n",
       "      <td>1.309</td>\n",
       "      <td>-20.503</td>\n",
       "      <td>-5.371</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.795</td>\n",
       "      <td>-4.453</td>\n",
       "      <td>-10.653</td>\n",
       "      <td>-7.566</td>\n",
       "      <td>2.318</td>\n",
       "      <td>-18.155</td>\n",
       "      <td>-22.536</td>\n",
       "      <td>3.523</td>\n",
       "      <td>0.711</td>\n",
       "      <td>-0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>Bcn</td>\n",
       "      <td>6.202</td>\n",
       "      <td>-30.672</td>\n",
       "      <td>-7.411</td>\n",
       "      <td>4.389</td>\n",
       "      <td>-13.742</td>\n",
       "      <td>1.300</td>\n",
       "      <td>-10.432</td>\n",
       "      <td>10.468</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.976</td>\n",
       "      <td>6.892</td>\n",
       "      <td>-7.576</td>\n",
       "      <td>22.716</td>\n",
       "      <td>-3.475</td>\n",
       "      <td>-19.599</td>\n",
       "      <td>4.005</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-6.803</td>\n",
       "      <td>9.380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   y    c       z      x0      x1      x2      x3      x4      x5      x6  \\\n",
       "0  1  Adk   3.249  -8.121   4.225   3.939   2.177  -4.563  -1.281 -18.063   \n",
       "1  1  Adg   9.412 -22.232   6.170  -4.890   5.541   5.982   7.018  -4.786   \n",
       "2  1  Acs   9.740  -2.892  -1.393   6.831  -0.240 -23.420   7.991  18.532   \n",
       "3  1  Acb  16.043   6.197  -4.527 -12.651 -15.924  12.013   5.821  13.547   \n",
       "4  1  Aas  13.717   6.480   7.840  -5.849  16.306   0.506 -14.651   4.347   \n",
       "5  0  Bca   5.956   7.167  15.060   0.331  -9.986  -9.756  -6.632   8.357   \n",
       "6  0  Bdk   2.923  24.327  -5.232   4.530  -7.362   7.063   1.951   7.538   \n",
       "7  0  Bal   6.572  10.886  -4.413   8.222   7.348  -2.873  -1.676  -6.342   \n",
       "8  0  Bcx   1.101   7.103   2.293  10.635 -10.746   1.309 -20.503  -5.371   \n",
       "9  0  Bcn   6.202 -30.672  -7.411   4.389 -13.742   1.300 -10.432  10.468   \n",
       "\n",
       "    ...       x90     x91     x92     x93     x94     x95     x96     x97  \\\n",
       "0   ...     4.282 -13.735  -4.926  -0.930  -4.438  -4.954   5.147  12.147   \n",
       "1   ...     7.122 -10.226   4.726   6.512 -10.393   6.112   9.457 -12.911   \n",
       "2   ...    -2.196   7.716 -15.627 -28.180   5.727   4.693 -12.137  12.972   \n",
       "3   ...   -11.758 -13.935  -0.040   8.221   1.481  11.609   9.120  -1.903   \n",
       "4   ...   -11.990   0.875   0.416 -18.412  -8.506   4.638  -9.237  -1.395   \n",
       "5   ...    -1.747 -12.302 -11.957  -6.538  11.635   7.446   7.618   3.729   \n",
       "6   ...    -0.560  -7.271   0.213   1.155   2.093  -6.612  -1.755  12.442   \n",
       "7   ...     6.884  -7.649   6.742  16.675  -1.460  -6.662  -8.200  -1.357   \n",
       "8   ...    -1.795  -4.453 -10.653  -7.566   2.318 -18.155 -22.536   3.523   \n",
       "9   ...    -5.976   6.892  -7.576  22.716  -3.475 -19.599   4.005   0.160   \n",
       "\n",
       "      x98     x99  \n",
       "0   5.261   5.379  \n",
       "1  -0.384   9.922  \n",
       "2  -0.073  -7.936  \n",
       "3  -8.438  15.006  \n",
       "4  19.512  -4.426  \n",
       "5  -4.476  -3.037  \n",
       "6  -5.582   1.631  \n",
       "7  15.118  -0.891  \n",
       "8   0.711  -0.072  \n",
       "9  -6.803   9.380  \n",
       "\n",
       "[10 rows x 103 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle('categorical.pkl') # make sure you've downloaded the file and the path is correct\n",
    "df.head(10).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Handling categorical variables:\n",
    "There are a number of ways categorical variables can be handled. Most implementations either hot encode categorical variables\n",
    "or are able to handle them directly while building decision trees. \n",
    "\n",
    "Handling categorical variables by hot encoding them is quite straightforward. We transform categorical variable with $q$ levels\n",
    "into $q$ *independent* boolean variables. Implementations like *scikit* require categorical variables to be supplied as hot-\n",
    "encodings. We shall see in later sections that this is in fact a bad idea and results in unnecessary sparsity weakening the\n",
    "predictive power a categorical variable might have. \n",
    "\n",
    "The second approach, which decision trees quite naturally lend themselves to, is to handle categorical variables directly. Say\n",
    "a categorical variable has $q$ level and we are building a binary decision tree. We need to somehow be able to split the set of all $q$ different levels into 2 disjoint non-empty sets. How many ways can this be done? \n",
    "$$\\frac{2^q - 2}{2}$$\n",
    "\n",
    "$2^q$ since each level can belong to either of the two sets, $-2$ to account for non-empty sets and division by\n",
    "$2$ to account for symmetry.\n",
    "\n",
    "But does this mean while finding a split on a categorical variable we need to exhaustively search each possible split to find\n",
    "the one that best reduces the impurity? Well, depends! \n",
    "If the problem at hand is a binary classification there is an\n",
    "efficient way of finding the best split. Which is, **order levels by the fraction of positive (wlog) samples they contain and find the split as if the levels were ordered just like real valued variables**.\n",
    "Proof of this is beyond the scope of this exercise, however if you are interested the proof can be found in [Breiman et. al.](https://www.amazon.co.uk/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418)\n",
    "For multi-class classification however no such simplification exists, though various [heuristics](https://link.springer.com/article/10.1023/A%3A1009869804967?no-access=true) have been proposed.\n",
    "\n",
    "One of the questions that arises however is, do categorical variables with large number of levels cause overfitting? The answer is potentially! \n",
    "Consider categorical variables $C1$ and $C2$ with levels $q1$ and $q2$ respectively with $q1 > q2$. Which one of these variables do you think might be favored during tree induction? Since $C1$ provides more granularity in terms of how the samples can be split it is more likely to produce bigger reduction in sample impurity and hence is more likely be favored over $C2$. Since large number of levels allows for finer granularity while splitting samples this might potentially cause overfitting.\n",
    "\n",
    "One another possible way of handling categorical variables is through $q$ way splitting instead of 2 way splitting (but we won't have a binary tree anymore), where $q$ is the number of levels or different possible values a categorical variable can attain. What might be the problem with this approach?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tree induction with and without one-hot encoding\n",
    "In this section you will first create a baseline on the above dataset by building a tree on all features except the categorical feature $c$. You will then compare this baseline against:\n",
    "\n",
    "1. tree built by one-hot encoding categorical variable $c$ \n",
    "\n",
    "2. tree built without one-hot encoding categorical variable $c$ (i.e. handling $c$ directly by finding the best split using the approach detailed above). \n",
    "\n",
    "*Suggestion*: use 60% data to fully grow the tree, use 20% for pruning (using greedy pruning as mentioned in the lecture) and the rest 20% as a test set for performance reporting.\n",
    "\n",
    "Also make sure to print the list of features sorted by their feature importance after tree induction. Feature importance is simply *the amount of reduction in impurity induced by the feature while building tree*. \n",
    "\n",
    "Why do you notice such a huge difference in performance? Also how does categorical feature $c$ (or its hot-encoded splits) rank in terms of feature importance in both cases?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis: Why did one-hot encoding give such poor performance?\n",
    "One hot encoding transforms a categorical variable into a number of independent binary variables. This\n",
    "generates sparsity and weakens any predictive power a categorical variable may have by a large extent. \n",
    "\n",
    "Binary variables have very small degree of freedom compared to real valued variables to begin with since they can only be split in one way: all samples with value 0 for that variable in one bucket and the remaining samples in the other bucket. \n",
    "If categorical variables were to be handled directly however samples could be split in $\\frac{2^q - 2}{2}$ ways as we saw above. Real values variables on the other hand, owing to the total ordering of values, can be split anywhere.\n",
    "\n",
    "To see why transforming a categorical variable into a number of independent binary variables weakens its predictive power lets\n",
    "consider the following scenario:\n",
    "\n",
    "Say we have a categorical variable with 100 levels (and we hot-encode it to produce 100 independent boolean variables) and say the samples are distributed uniformly across different levels. This mean *at best* splitting samples on one of the induced binary variables can only reduce impurity by 1%. (Think why that is the case!) This is very insignificant and hence such induced variables struggle to be picked high up in the tree during tree induction. \n",
    "\n",
    "The take home message from this excercise is two-fold :\n",
    "1. hot encoding a categorical variable is bad. Trees can handle categorical variables directly in a natural way and should be \n",
    "favored instead.\n",
    "2. large number of levels for a categorical variable make trees suceptible to overfitting and might require strong regularization in place. \n",
    "\n",
    "\n",
    "#### References:\n",
    "[1] [Are categorical variables getting lost in your random forests?](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
