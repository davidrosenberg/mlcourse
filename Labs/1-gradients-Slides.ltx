\title{Recitation 1}
\subtitle{Gradients and Directional Derivatives}
\begin{document}
\begin{frame} 
  \titlepage 
\end{frame}
\section{Recitation 1}
\subsection{Initial Question}
\begin{frame}[fragile]
  \frametitle{Intro Question}
  \begin{block}{Question}
    We are given the data set $(x_1,y_1),\ldots,(x_n,y_n)$ where
    $x_i\in\RR^d$ and $y_i\in\RR$.  We want to fit a linear function to
    this data by performing empirical risk minimization.  More
    precisely, we are using the hypothesis space $\FF=\{f(x)=w^Tx\mid
    w\in\RR^d\}$ and the loss function $\ell(a,y)=(a-y)^2$.  Given an
    initial guess $\tilde{w}$ for the empirical risk minimizing
    parameter vector, how could we improve our guess?
  \end{block}
\begin{center}
\includegraphics[width=0.7\textwidth,height=0.4\textheight]{1-gradients/Data.pdf}
\end{center}
\end{frame}
\begin{frame}
  \frametitle{Intro Solution}
  \begin{block}{Solution}
    \begin{itemize}
    \item The empirical risk is given by
      $$\hat{R}_n(f) = \frac{1}{n}\sum_{i=1}^n \ell(f(x_i),y_i) =
      \frac{1}{n}\sum_{i=1}^n (w^Tx_i-y_i)^2
      = \frac{1}{n}\|Xw-y\|_2^2,$$
      where $X\in\RR^{n\times d}$ is the matrix whose $i$th row is
      given by $x_i$.
    \item Can improve a non-optimal guess $\tilde{w}$ by taking a
      small step in the direction of the negative gradient.
    \end{itemize}
  \end{block}  
\end{frame}
\subsection{Single Variable Calculus}
\begin{frame}
  \frametitle{Single Variable Differentiation}
  \begin{itemize}
  \item For $f:\RR\to\RR$ differentiable, the derivative is given by
    $$f'(x) = \lim_{h\to 0} \frac{f(x+h)-f(x)}{h}.$$
  \item Can also be written as
    $$f(x+h) = f(x) + hf'(x) + o(h)\quad\text{as $h\to 0$,}$$
    where $o(h)$ denotes a function $g(h)$ with $g(h)/h\to 0$ as
    $h\to0$.
  \item Points with $f'(x) = 0$ are called \textit{critical points}.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{1D Linear Approximation By Derivative}
\begin{center}
\includegraphics[width=\textwidth,height=0.8\textheight]{1-gradients/Taylor1d.pdf}
\end{center}
\end{frame}

\subsection{Multivariable Calculus}
\begin{frame}
  \frametitle{Multivariable Differentiation}
  \begin{itemize}
  \item Consider now a function $f:\RR^n\to\RR$ with inputs of the
    form $x=(x_1,\ldots,x_n)\in\RR^n$.
  \item Unlike the 1-dimensional case, we cannot assign a single
    number to the slope at a point since there are many directions we can move in.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Multiple Possible Directions for $f:\RR^2\to\RR$}
\begin{center}
\includegraphics[width=\textwidth,height=0.8\textheight]{1-gradients/Directions.pdf}
\end{center}
\end{frame}
\begin{frame}
  \frametitle{Directional Derivative}
  \begin{block}{Definition}
    Let $f:\RR^n\to\RR$.  The directional derivative $f'(x;u)$ of $f$ at $x\in\RR^n$ in
    the direction $u\in\RR^n$ is given by
    $$f'(x;u) = \lim_{h\to0}\frac{f(x+hu)-f(x)}{h}.$$
  \end{block}
  \begin{itemize}
  \item By fixing a direction $u$ we turned our multidimensional
    problem into a 1-dimensional problem.
  \item Similar to 1-d we have
    $$f(x+hu) = f(x) + hf'(x;u) + o(h).$$
  \item We say that $u$ is a \textit{descent direction} of $f$ at $x$ if $f'(x;u)<0$.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Directional Derivative as a Slope of a Slice}
\begin{center}
\includegraphics[width=0.7\textwidth,height=0.8\textheight]{1-gradients/Slice.pdf}
\end{center}
\end{frame}

\begin{frame}
  \frametitle{Partial Derivative}
  \begin{itemize}
  \item Let $e_i=(\overbrace{0,0,\ldots,0}^{i-1},1,0,\ldots,0)$ denote the $i$th
    standard basis vector.
  \item The $i$th \textit{partial derivative} is defined to be the
    directional derivative along $e_i$.
  \item It can be written many ways:
    $$f'(x;e_i) = \frac{\partial}{\partial x_i}f(x) =
    \partial_{x_i}f(x) = \partial_if(x).$$
  \item What is the intuitive meaning of $\partial_{x_i}f(x)$?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Differentiability}
  \begin{itemize}
  \item We say a function $f:\RR^n\to\RR$ is \textit{differentiable}
    at $x\in\RR^n$ if
    $$\lim_{v\to0}\frac{f(x+v)-f(x) -g^Tv}{\|v\|_2} = 0,$$
    for some $g\in\RR^n$.
  \item If it exists, this $g$ is unique and is called the \textit{gradient} of $f$ at
    $x$ with notation
    $$g = \nabla f(x).$$
  \item It can be shown that
    $$\nabla f(x) =
    \pMattt{\partial_{x_1}f(x)}{\vdots}{\partial_{x_n}f(x)}.$$
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Useful Convention}
  \begin{itemize}
  \item Consider $f:\RR^{p+q}\to\RR$.
  \item Split the input to $f$ into parts $w\in\RR^p$ and $z\in\RR^q$.
  \item Define the partial gradients
    $$\hspace{-.5cm}\nabla_w f(w,z) :=
    \pMattt{\partial_{w_1}f(w,z)}{\vdots}{\partial_{w_p}f(w,z)}
    \quad\text{and}\quad
    \nabla_z f(w,z) :=
    \pMattt{\partial_{z_1}f(w,z)}{\vdots}{\partial_{z_q}f(w,z)}.
    $$
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Tangent Plane}
  \begin{itemize}
  \item Analogous to the 1-d case we can express differentiability as
    $$f(x+v) = f(x) + \nabla f(x)^Tv + o(\|v\|_2).$$
  \item The approximation $f(x+v)\approx f(x)+\nabla f(x)^Tv$ gives a
    tangent plane at the point $x$.
  \item The tangent plane of $f$ at $x$ is given by
    $$P = \{(x+v,f(x)+\nabla f(x)^Tv)\mid v\in\RR^n\}\subseteq \RR^{n+1}.$$
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Tangent Plane for $f:\RR^2\to\RR$}
\begin{center}
\includegraphics[width=\textwidth,height=0.8\textheight]{1-gradients/Tangent.pdf}
\end{center}
\end{frame}
\begin{frame}
  \frametitle{Directional Derivatives from Gradients}
  \begin{itemize}
  \item If $f$ is differentiable we have
    $$f'(x;u) = \nabla f(x)^Tu.$$
  \item If $\nabla f(x)\neq 0$ this implies that
    $$\argmax_{\|u\|_2=1} f'(x;u) = \frac{\nabla f(x)}{\|\nabla f(x)\|_2}
    \quad\text{and}\quad
    \argmin_{\|u\|_2=1} f'(x;u) = -\frac{\nabla f(x)}{\|\nabla
      f(x)\|_2}.$$
  \item The gradient points in the direction of
    steepest ascent.
  \item The negative gradient points in the direction of
    steepest descent.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Critical Points}
  \begin{itemize}
  \item Analogous to 1-d, if $f:\RR^n\to\RR$ is differentiable and $x$
    is a local extremum then we must have $\nabla f(x)=0$.
  \item Points with $\nabla f(x)=0$ are called \textit{critical
    points}.
  \item Later we will see that for a convex differentiable function,
    $x$ is a critical point if and only if it is a global minimizer.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Critical Points of $f:\RR^2\to\RR$}
\begin{center}
\includegraphics[width=\textwidth,height=0.8\textheight]{1-gradients/Saddle.pdf}
\end{center}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Computing Gradients}
  \begin{block}{Question}
    For questions 1 and 2, compute the gradient of the given function.
    \begin{enumerate}
    \item $f:\RR^3\to\RR$ is given by
      $$f(x_1,x_2,x_3) = \log(1+e^{x_1+2x_2+3x_3}).$$
    \item $f:\RR^n\to\RR$ is given by
      $$f(x) = \|Ax-y\|_2^2 = (Ax-y)^T(Ax-y) = x^TA^TAx - 2y^TAx+y^Ty,$$
      for some $A\in\RR^{m\times n}$ and $y\in\RR^m$.
    \item Assume $A$ in the previous question has full column rank.
      What is the critical point of $f$?
    \end{enumerate}
  \end{block}
\end{frame}
\end{document}
