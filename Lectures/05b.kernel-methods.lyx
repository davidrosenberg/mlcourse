#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty


% Set Color ==============================
\definecolor{NYUPurple}{RGB}{87,6,140}
\definecolor{LightPurple}{RGB}{165,11,255}


\setbeamercolor{title}{fg=NYUPurple}
%\setbeamercolor{frametitle}{fg=NYUPurple}
\setbeamercolor{frametitle}{fg=NYUPurple}

\setbeamercolor{background canvas}{fg=NYUPurple, bg=white}
\setbeamercolor{background}{fg=black, bg=NYUPurple}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=gray!20!white, bg=NYUPurple}

\setbeamertemplate{headline}{}

\setbeamercolor{parttitle}{fg=NYUPurple}
\setbeamercolor{sectiontitle}{fg=NYUPurple}
\setbeamercolor{sectionname}{fg=NYUPurple}
\setbeamercolor{section page}{fg=NYUPurple}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
\setbeamercolor{section title}{fg=NYUPurple}
 \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\usebeamercolor[fg]{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\end_preamble
\options aspectratio=169,handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "allcolors=NYUPurple,urlcolor=LightPurple"
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\boxbgcolor #ff31d8
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\dom}{\textrm{\textbf{dom} }}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\rank}{\text{\textbf{rank }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\conv}{\textrm{\textbf{conv} }}
\end_inset


\begin_inset FormulaMacro
\newcommand{\relint}{\text{\textbf{relint }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\aff}{\text{\textbf{aff }}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Title
Kernel Methods
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David S.
 Rosenberg 
\end_layout

\begin_layout Date
February 26, 2019
\end_layout

\begin_layout Institute
New York University
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Just in article version
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Plots courtesy of Ningshan Zhang.}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Contents
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\end_deeper
\begin_layout Section
Bring in the feature map to kernelization
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
A kernelized objective
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Previously, we took the following optimization problem: 
\begin_inset Formula 
\[
w^{*}\in\argmin_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right)
\]

\end_inset

and kernelized it as 
\begin_inset Formula 
\[
\alpha^{*}\in\argmin_{\alpha\in\reals^{n}}R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right),
\]

\end_inset

where
\begin_inset Formula 
\[
K=\begin{pmatrix}\left\langle x_{1},x_{1}\right\rangle  & \cdots & \left\langle x_{1},x_{n}\right\rangle \\
\vdots & \ddots & \cdots\\
\left\langle x_{n},x_{1}\right\rangle  & \cdots & \left\langle x_{n},x_{n}\right\rangle 
\end{pmatrix}\qquad\text{and}\qquad k_{x}=\begin{pmatrix}\left\langle x_{1},x\right\rangle \\
\vdots\\
\left\langle x_{n},x\right\rangle 
\end{pmatrix}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
But computing 
\begin_inset Formula $K$
\end_inset

 and 
\begin_inset Formula $k_{x}$
\end_inset

 can be computationally hard for large feature spaces.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
To address this issue, we'll take a step back, and explicitly talk about
 feature maps.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Input Space 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Our general learning theory setup: no assumptions about 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
But 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset

 for the specific methods we've developed: 
\end_layout

\begin_deeper
\begin_layout Itemize
Ridge regression
\end_layout

\begin_layout Itemize
Lasso regression
\end_layout

\begin_layout Itemize
Support Vector Machines 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Our hypothesis space for these was all affine functions on 
\begin_inset Formula $\reals^{d}$
\end_inset

:
\begin_inset Formula 
\[
\cf=\left\{ x\mapsto w^{T}x+b\mid w\in\reals^{d},b\in\reals\right\} .
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What if we want to do prediction on inputs not natively in 
\begin_inset Formula $\reals^{d}$
\end_inset

? 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Models with Explicit Feature Map
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Input space
\series default
: 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Feature space
\series default
: 
\begin_inset Formula $\ch$
\end_inset

 (a Hilbert space, i.e.
 an inner product space with projections, e.g.
 
\begin_inset Formula $\reals^{d}$
\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
a pre-Hilbert space (i.e.
 not necessarily complete) can still have projections, in the sense of a
 vector that's closer than all the others with residual orthogonal to subspace...
 but need Hilbert space to guarantee existence
\end_layout

\end_inset

)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Feature map
\series default
: 
\begin_inset Formula $\psi:\cx\to\ch$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Hypothesis space of affine functions on feature space:
\begin_inset Formula 
\[
\cf=\left\{ x\mapsto\left\langle w,\psi(x)\right\rangle +b\mid w\in\ch,b\in\reals\right\} .
\]

\end_inset


\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
More generally, feature space could be a Hilbert space 
\begin_inset Formula $\ch$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized objective with feature map
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Optimization problem with explicit feature map: 
\begin_inset Formula 
\[
w^{*}\in\argmin_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,\psi\left(x_{1}\right)\right\rangle ,\ldots,\left\langle w,\psi\left(x_{n}\right)\right\rangle \right)
\]

\end_inset

and kernelized it as 
\begin_inset Formula 
\[
\alpha^{*}\in\argmin_{\alpha\in\reals^{n}}R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right),
\]

\end_inset

where
\begin_inset Formula 
\[
K=\begin{pmatrix}\left\langle \psi\left(x_{1}\right),\psi\left(x_{1}\right)\right\rangle  & \cdots & \left\langle \psi\left(x_{1}\right),\psi\left(x_{n}\right)\right\rangle \\
\vdots & \ddots & \cdots\\
\left\langle \psi\left(x_{n}\right),\psi\left(x_{1}\right)\right\rangle  & \cdots & \left\langle \psi\left(x_{n}\right),\psi\left(x_{n}\right)\right\rangle 
\end{pmatrix}\qquad\text{and}\qquad k_{x}=\begin{pmatrix}\left\langle \psi\left(x_{1}\right),\psi\left(x\right)\right\rangle \\
\vdots\\
\left\langle \psi\left(x_{n}\right),\psi\left(x\right)\right\rangle 
\end{pmatrix}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
The Kernel Function
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Input space
\series default
: 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Feature space
\series default
: 
\begin_inset Formula $\ch$
\end_inset

 (a Hilbert space, i.e.
 an inner product space with projections, e.g.
 
\begin_inset Formula $\reals^{d}$
\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
a pre-Hilbert space (i.e.
 not necessarily complete) can still have projections, in the sense of a
 vector that's closer than all the others with residual orthogonal to subspace...
 but need Hilbert space to guarantee existence
\end_layout

\end_inset

)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Feature map
\series default
: 
\begin_inset Formula $\psi:\cx\to\ch$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The 
\series bold
kernel function
\series default
 corresponding to 
\begin_inset Formula $\psi$
\end_inset

 in 
\begin_inset Formula $\ch$
\end_inset

 is 
\begin_inset Formula 
\[
k(x,x')=\left\langle \psi(x),\psi(x')\right\rangle ,
\]

\end_inset

where 
\begin_inset Formula $\left\langle \cdot,\cdot\right\rangle $
\end_inset

 is the inner product associated with 
\begin_inset Formula $\ch$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Function: Why do we need this?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Feature map
\series default
: 
\begin_inset Formula $\psi:\cx\to\ch$
\end_inset


\end_layout

\begin_layout Itemize
The 
\series bold
kernel function
\series default
 corresponding to 
\begin_inset Formula $\psi$
\end_inset

 is 
\begin_inset Formula 
\[
k(x,x')=\left\langle \psi(x),\psi(x')\right\rangle .
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Why introduce this new notation 
\begin_inset Formula $k(x,x')$
\end_inset

?
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We can often evaluate 
\begin_inset Formula $k(x,x')$
\end_inset

 without explicitly computing 
\begin_inset Formula $\psi(x)$
\end_inset

 and 
\begin_inset Formula $\psi(x')$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
For large feature spaces, can be much faster.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernel Evaluation Can Be Fast
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Example
Quadratic feature map for 
\begin_inset Formula $x=\left(x_{1},\ldots,x_{d}\right)\in\reals^{d}$
\end_inset

.
\begin_inset Formula 
\[
\ensuremath{\psi(x)=(x_{1},\ldots,x_{d},x_{1}^{2},\ldots,x_{d}^{2},\sqrt{2}x_{1}x_{2},\ldots,\sqrt{2}x_{i}x_{j},\ldots\sqrt{2}x_{d-1}x_{d})^{T}}
\]

\end_inset

has dimension 
\begin_inset Formula $O(d^{2})\pause$
\end_inset

, but for any 
\begin_inset Formula $x,x'\in\reals^{d}$
\end_inset

 and the standard Euclidean dot products, 
\begin_inset Formula 
\[
k(x,x')=\left\langle \psi(x),\psi(x')\right\rangle =\left\langle x,x'\right\rangle +\left\langle x,x'\right\rangle ^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Explicit computation of 
\begin_inset Formula $k(x,x')$
\end_inset

: 
\begin_inset Formula $O(d^{2})$
\end_inset


\end_layout

\begin_layout Itemize
Implicit computation of 
\begin_inset Formula $k(x,x')$
\end_inset

: 
\begin_inset Formula $O(d)$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernels as Similarity Scores
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Often useful to think of the kernel function as a 
\series bold
similarity score
\series default
.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
But this is not a mathematically precise statement.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
There are many ways to design a similarity score.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We will use kernel functions that correspond to inner products in some feature
 space.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
These are called 
\series bold
Mercer kernels.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
What are the Benefits of Kernelization?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Computational (when optimizing over 
\begin_inset Formula $\reals^{n}$
\end_inset

 is better than over 
\begin_inset Formula $\reals^{d}$
\end_inset

)
\begin_inset Note Note
status open

\begin_layout Plain Layout
(WAITâ€“ not sure we're prepared yet for this n vs d claim)
\end_layout

\end_inset

).
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Can sometimes avoid any 
\begin_inset Formula $O(d)$
\end_inset

 operations
\end_layout

\begin_deeper
\begin_layout Itemize
allows access to
\series bold
 infinite-dimensional feature spaces
\series default
.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Allows thinking in terms of 
\begin_inset Quotes eld
\end_inset

similarity
\begin_inset Quotes erd
\end_inset

 rather than features.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
(debatable)
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Matrix
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
The 
\series bold
kernel matrix
\series default
 for a kernel 
\begin_inset Formula $k$
\end_inset

 on 
\begin_inset Formula $x_{1},\ldots,x_{n}\in\cx$
\end_inset

 is
\begin_inset Formula 
\[
K=\begin{pmatrix}k(x_{i},x_{j})\end{pmatrix}_{i,j}=\begin{pmatrix}k(x_{1},x_{1}) & \cdots & k(x_{1},x_{n})\\
\vdots & \ddots & \cdots\\
k(x_{n},x_{1}) & \cdots & k(x_{n},x_{n})
\end{pmatrix}\in\reals^{n\times n}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
In ML this is also called a 
\series bold
Gram matrix
\series default
, but traditionally (in linear algebra),
\end_layout

\begin_deeper
\begin_layout Itemize
Gram matrices are defined without reference to a kernel or feature map.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Matrix
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The kernel matrix summarizes all the information we need about the training
 inputs 
\begin_inset Formula $x_{1},\ldots,x_{n}$
\end_inset

 to solve a kernelized optimization problem.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
e.g.
 in the kernelized SVM, we can replace 
\begin_inset Formula $\psi(x_{i})^{T}\psi(x_{j})$
\end_inset

 with 
\begin_inset Formula $K_{ij}$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\sup_{\alpha} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}K_{ij}\\
\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\qquad\text{and}\qquad\alpha_{i}\in\left[0,\frac{c}{n}\right]\;i=1,\ldots,n.
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The 
\begin_inset Quotes eld
\end_inset

Kernel Trick
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Given a kernelized ML algorithm (i.e.
 all 
\begin_inset Formula $\psi(x)$
\end_inset

's show up as 
\begin_inset Formula $\left\langle \psi(x),\psi(x')\right\rangle $
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Can swap out the inner product for a new kernel function.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
New kernel may correspond to a very high-dimensional feature space.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Once the kernel matrix is computed, the computational cost depends on number
 of data points, rather than the dimension of feature space.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
The 
\series bold
trick
\series default
 is that once you've implemented your method in terms of a kernel matrix,
 you can go from a kernel corresponding to a very small feature vector to
 a kernel corresponding to a very large (even infinite dimensional) feature
 vector, without changing your code, just by swapping one kernel matrix
 for another.
 Runtime is unaffected, after the kernel matrix is computed.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Trick(s)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Given a kernelized ML algorithm (i.e.
 all 
\begin_inset Formula $\psi(x)$
\end_inset

's show up as 
\begin_inset Formula $\left\langle \psi(x),\psi(x')\right\rangle $
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Can write training function to depend on 
\begin_inset Formula $x$
\end_inset

's only through kernel matrix:
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
kernelized_train(kernel_matrix K, training_labels y)
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
One 
\begin_inset Quotes eld
\end_inset

kernel trick
\begin_inset Quotes erd
\end_inset

 is that whether the kernel corresponds to a small feature vector, a very
 large feature vector, or even an infinite dimensional feature vector (to
 be discussed), the training code and training time remain the same (
\series bold
once the kernel matrix is computed
\series default
).
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Another 
\begin_inset Quotes eld
\end_inset

kernel trick
\begin_inset Quotes erd
\end_inset

 is that often we can compute the entries of the kernel matrix without the
 computational cost of explicilty working in the feature space.
 
\end_layout

\end_deeper
\begin_layout Section
Kernels
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Choose a kernel instead of choosing a feature map
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Once a method is kernelized, we can effectively change the feature map by
 changing the kernel function.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
This will allow us to run linear methods with kernels that correspond to
 infinite dimensional feature maps.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Below we discuss the most commonly used kernels.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space: 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Feature space: 
\begin_inset Formula $\ch=\reals^{d}$
\end_inset

, with standard inner product
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Feature map
\begin_inset Formula 
\[
\psi(x)=x
\]

\end_inset


\end_layout

\begin_layout Itemize
Kernel: 
\begin_inset Formula 
\[
k(x,x')=x^{T}x'
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Quadratic Kernel in 
\begin_inset Formula $\reals^{d}$
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Feature space: 
\begin_inset Formula $\ch=\reals^{D}$
\end_inset

, where 
\begin_inset Formula $D=d+{d \choose 2}\approx d^{2}/2$
\end_inset

.
\end_layout

\begin_layout Itemize
Feature map:
\begin_inset Formula 
\[
\ensuremath{\psi(x)=(x_{1},\ldots,x_{d},x_{1}^{2},\ldots,x_{d}^{2},\sqrt{2}x_{1}x_{2},\ldots,\sqrt{2}x_{i}x_{j},\ldots\sqrt{2}x_{d-1}x_{d})^{T}}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then for 
\begin_inset Formula $\forall x,x'\in\reals^{d}$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
k(x,x') & = & \left\langle \psi(x),\psi(x')\right\rangle \\
\pause & = & \left\langle x,x'\right\rangle +\left\langle x,x'\right\rangle ^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Computation for inner product with explicit mapping: 
\begin_inset Formula $O(d^{2})$
\end_inset


\end_layout

\begin_layout Itemize
Computation for implicit kernel calculation: 
\begin_inset Formula $O(d)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Based on Guillaume Obozinski's Statistical Machine Learning course
 at Louvain, Feb 2014.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Polynomial Kernel in 
\begin_inset Formula $\reals^{d}$
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Kernel function:
\begin_inset Formula 
\[
k(x,x')=\left(1+\left\langle x,x'\right\rangle \right)^{M}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Corresponds to a feature map with all monomials up to degree 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For any 
\begin_inset Formula $M$
\end_inset

, computing the kernel has same computational cost
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Cost of explicit inner product computation grows rapidly in 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Section
The RBF Kernel
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Radial Basis Function (RBF) / Gaussian Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\begin_inset Formula 
\[
k(x,x')=\exp\left(-\frac{\|x-x'\|^{2}}{2\sigma^{2}}\right),
\]

\end_inset

where 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is known as the bandwidth parameter.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Does it act like a similarity score?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Why 
\begin_inset Quotes eld
\end_inset

radial
\begin_inset Quotes erd
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Have we departed from our 
\begin_inset Quotes eld
\end_inset

inner product of feature vector
\begin_inset Quotes erd
\end_inset

 recipe?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Yes and no: corresponds to an infinite dimensional feature vector
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Probably the most common nonlinear kernel.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
RBF Basis
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals$
\end_inset


\end_layout

\begin_layout Itemize
Output space: 
\begin_inset Formula $\cy=\reals$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
RBF kernel 
\begin_inset Formula $k(w,x)=\exp\left(-\left(w-x\right)^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Suppose we have 6 training examples: 
\begin_inset Formula $x_{i}\in\left\{ -6,-4,-3,0,2,4\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If representer theorem applies, then
\begin_inset Formula 
\[
f(x)=\sum_{i=1}^{6}\alpha_{i}k(x_{i},x).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $f$
\end_inset

 is a linear combination of 6 basis functions of form 
\begin_inset Formula $k(x_{i},\cdot)$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Archive/2018/Lectures/source/05a.kernel-methods/kernel-fns-1.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset

 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
RBF Predictions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Basis functions
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Archive/2018/Lectures/source/05a.kernel-methods/kernel-fns-1.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Predictions of the form 
\begin_inset Formula $f(x)=\sum_{i=1}^{6}\alpha_{i}k(x_{i},x)$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Archive/2018/Lectures/source/05a.kernel-methods/kernel-fns-2.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
When kernelizing with RBF kernel, prediction functions always look this
 way.
\end_layout

\begin_layout Itemize
(Whether we get 
\begin_inset Formula $w$
\end_inset

 from SVM, ridge regression, etc...)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
RBF Feature Space: The Sequence Space 
\begin_inset Formula $\ell_{2}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
To work with infinite dimensional feature vectors, we need a space with
 certain properties.
\end_layout

\begin_deeper
\begin_layout Itemize
an inner product
\end_layout

\begin_layout Itemize
a norm related to the inner product 
\end_layout

\begin_layout Itemize
projection theorem: 
\begin_inset Formula $x=x_{\perp}+x_{\|}$
\end_inset

 where 
\begin_inset Formula $x_{\|}\in S=\linspan(w_{1},\ldots,w_{n})$
\end_inset

 and 
\begin_inset Formula $\left\langle x_{\perp},s\right\rangle =0\quad\forall s\in S$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Itemize
Basically, we need a Hilbert space.
\end_layout

\begin_layout Definition
\begin_inset Formula $\ell_{2}$
\end_inset

 is the space of all real-valued sequences: 
\begin_inset Formula $\left(x_{0},x_{1},x_{2},x_{3},\ldots\right)$
\end_inset

 with 
\begin_inset Formula $\sum_{i=0}^{\infty}x_{i}^{2}<\infty$
\end_inset

.
 
\end_layout

\begin_layout Theorem
With the the inner product 
\begin_inset Formula $\left\langle x,x'\right\rangle =\sum_{i=0}^{\infty}x_{i}x'_{i}$
\end_inset

, 
\begin_inset Formula $\ell_{2}$
\end_inset

 is a 
\series bold
Hilbert space
\series default
.
\end_layout

\begin_layout Theorem

\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
We know the inner product between 
\begin_inset Formula $x,x'\in\ell_{2}$
\end_inset

 is well-defined, as follows.
 First, note the following: For any numbers 
\begin_inset Formula $a,b\in\reals$
\end_inset

, we have 
\begin_inset Formula $\left|ab\right|\le\left|a\right|\left|b\right|\le\left(a^{2}+b^{2}\right)/2$
\end_inset

, since: 
\begin_inset Formula 
\begin{eqnarray*}
\left(\left|a\right|-\left|b\right|\right)^{2} & \ge & 0\\
\implies a^{2}+b^{2} & \ge & 2\left|a\right|\left|b\right|
\end{eqnarray*}

\end_inset

 Then let 
\begin_inset Formula $M=\sum_{i=0}^{\infty}x_{i}^{2}$
\end_inset

 and 
\begin_inset Formula $M'=\sum_{i=0}^{\infty}\left(x_{i}'\right)^{2}$
\end_inset

.
\begin_inset Formula 
\begin{eqnarray*}
\sum_{i=0}^{n}\left|x_{i}x_{i}'\right| & \le & \sum_{i=0}^{\infty}\left|x_{i}x_{i}'\right|\\
 & \le & \frac{1}{2}\left[\sum_{i=0}^{\infty}\left(x_{i}^{2}\right)+\sum_{i=0}^{\infty}\left(x_{i}'\right)^{2}\right]\le\left(M+M'\right)/2
\end{eqnarray*}

\end_inset

Thus 
\begin_inset Formula $\sum_{i=0}^{\infty}x_{i}x_{i}'$
\end_inset

 converges (since it is an absolutely convergent series).
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Infinite Dimensional Feature Vector for RBF
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider RBF kernel (1-dim): 
\begin_inset Formula $k(x,x')=\exp\left(-\left(x-x'\right)^{2}/2\right)$
\end_inset


\end_layout

\begin_layout Itemize
We claim that 
\begin_inset Formula $\psi:\reals\to\ell_{2}$
\end_inset

, defined by 
\begin_inset Formula 
\[
\left[\psi(x)\right]_{j}=\frac{1}{\sqrt{j!}}e^{-x^{2}/2}x^{j}
\]

\end_inset

 gives the 
\series bold

\begin_inset Quotes eld
\end_inset

infinite-dimensional feature vector
\begin_inset Quotes erd
\end_inset

 corresponding to RBF kernel
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Is this mapping even well-defined? Is 
\begin_inset Formula $\psi(x)$
\end_inset

 even an element of 
\begin_inset Formula $\ell_{2}$
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Yes: 
\begin_inset Formula 
\[
\sum_{j=0}^{\infty}\frac{1}{j!}e^{-x^{2}}x^{2j}=e^{-x^{2}}\sum_{j=0}^{\infty}\frac{\left(x^{2}\right)^{j}}{j!}=1<\infty
\]

\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Infinite Dimensional Feature Vector for RBF
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Does feature vector 
\begin_inset Formula $\left[\psi(x)\right]_{n}=\frac{1}{\sqrt{j!}}e^{-x^{2}/2}x^{j}$
\end_inset

 actually correspond to the RBF kernel?
\end_layout

\begin_layout Itemize
Yes! Proof:
\begin_inset Formula 
\begin{eqnarray*}
\left\langle \psi(x),\psi(x')\right\rangle  & = & \sum_{j=0}^{\infty}\frac{1}{j!}e^{-\left(x^{2}+\left(x'\right)^{2}\right)/2}x^{j}\left(x'\right)^{j}\\
 & = & e^{-\left(x^{2}+\left(x'\right)^{2}\right)/2}\sum_{j=0}^{\infty}\frac{\left(xx'\right)^{j}}{j!}\\
 & = & \exp\left(-\left[x^{2}+\left(x'\right)^{2}\right]/2\right)\exp\left(xx'\right)\\
 & = & \exp\left(-\left[(x-x')^{2}/2\right]\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
QED 
\end_layout

\end_deeper
\begin_layout Section
When is 
\begin_inset Formula $k(x,x')$
\end_inset

 a kernel function? (Mercer's Theorem)
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
How to Get Kernels?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Explicitly construct 
\begin_inset Formula $\psi(x):\cx\to\reals^{d}$
\end_inset

 and define 
\begin_inset Formula $k(x,x')=\psi(x)^{T}\psi(x')$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Directly define the kernel function 
\begin_inset Formula $k(x,x')$
\end_inset

, and verify it corresponds to 
\begin_inset Formula $\left\langle \psi(x),\psi(x')\right\rangle $
\end_inset

 for some 
\begin_inset Formula $\psi$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
There are many theorems to help us with the second approach
\end_layout

\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Positive Semidefinite Matrices
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A real, symmetric matrix 
\begin_inset Formula $M\in\reals^{n\times n}$
\end_inset

 is 
\series bold
positive semidefinite (psd)
\series default
 if for any 
\begin_inset Formula $x\in\reals^{n}$
\end_inset

, 
\begin_inset Formula 
\[
x^{T}Mx\ge0.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Theorem
The following conditions are each necessary and sufficient for a symmetric
 matrix 
\begin_inset Formula $M$
\end_inset

 to be positive semidefinite:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $M$
\end_inset

 has can be factorized as 
\begin_inset Formula $M=R^{T}R$
\end_inset

, for some matrix 
\begin_inset Formula $R$
\end_inset

.
\end_layout

\begin_layout Itemize
All eigenvalues of 
\begin_inset Formula $M$
\end_inset

 are greater than or equal to 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Positive Semidefinite Function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A symmetric kernel function 
\begin_inset Formula $k:\cx\times\cx\to\reals$
\end_inset

 is 
\series bold
positive semidefinite (psd)
\series default
 if for any finite set 
\begin_inset Formula $\left\{ x_{1},\ldots,x_{n}\right\} \in\cx$
\end_inset

, the kernel matrix on this set 
\begin_inset Formula 
\[
K=\begin{pmatrix}k(x_{i},x_{j})\end{pmatrix}_{i,j}=\begin{pmatrix}k(x_{1},x_{1}) & \cdots & k(x_{1},x_{n})\\
\vdots & \ddots & \cdots\\
k(x_{n},x_{1}) & \cdots & k(x_{n},x_{n})
\end{pmatrix}
\]

\end_inset

is a positive semidefinite matrix.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Mercer's Theorem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
A symmetric function 
\begin_inset Formula $k(x,x')$
\end_inset

 can be expressed as an inner product
\begin_inset Formula 
\[
k(x,x')=\left\langle \psi(x),\psi(x')\right\rangle 
\]

\end_inset

for some 
\begin_inset Formula $\psi$
\end_inset

 if and only if 
\begin_inset Formula $k(x,x')$
\end_inset

 is 
\series bold
positive semidefinite.
\begin_inset Note Note
status open

\begin_layout Theorem

\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Proof
[Sketch] Suppose 
\begin_inset Formula $k(w,x)$
\end_inset

 is psd.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
Let 
\end_layout

\end_deeper
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generating New Kernels from Old
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k,k_{1},k_{2}:\cx\times\cx\to\reals$
\end_inset

 are psd kernels.
 Then so are the following:
\begin_inset Formula 
\begin{eqnarray*}
k_{\mbox{new}}(x,x') & = & k_{1}(x,x')+k_{2}(x,x')\\
k_{\mbox{new}}(x,x') & = & \alpha k(x,x')\\
k_{\mbox{new}}(x,x') & = & f(x)f(x')\mbox{ for any function \ensuremath{f(\cdot)}}\\
k_{\mbox{new}}(x,x') & = & k_{1}(x,x')k_{2}(x,x')
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
See Appendix for details.
\end_layout

\begin_layout Itemize
Lots more theorems to help you construct new kernels from old...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
\start_of_appendix
Details on New Kernels from Old [Optional]
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Additive Closure
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k_{1}$
\end_inset

 and 
\begin_inset Formula $k_{2}$
\end_inset

 are psd kernels with feature maps 
\begin_inset Formula $\phi_{1}$
\end_inset

 and 
\begin_inset Formula $\phi_{2}$
\end_inset

, respectively.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula 
\[
k_{1}(x,x')+k_{2}(x,x')
\]

\end_inset

is a psd kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Concatenate the feature vectors to get 
\begin_inset Formula 
\[
\phi(x)=\left(\phi_{1}(x),\phi_{2}(x)\right).
\]

\end_inset

Then 
\begin_inset Formula $\phi$
\end_inset

 is a feature map for 
\begin_inset Formula $k_{1}+k_{2}$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closure under Positive Scaling
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k$
\end_inset

 is a psd kernel with feature maps 
\begin_inset Formula $\phi$
\end_inset

.
\end_layout

\begin_layout Itemize
Then for any 
\begin_inset Formula $\alpha>0$
\end_inset

, 
\begin_inset Formula 
\[
\alpha k
\]

\end_inset

is a psd kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Note that 
\begin_inset Formula 
\[
\phi(x)=\sqrt{\alpha}\phi(x)
\]

\end_inset

 is a feature map for 
\begin_inset Formula $\alpha k$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Scalar Function Gives a Kernel
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For any function 
\begin_inset Formula $f(x)$
\end_inset

, 
\begin_inset Formula 
\[
k(x,x')=f(x)f(x')
\]

\end_inset

is a kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Let 
\begin_inset Formula $f(x)$
\end_inset

 be the feature mapping.
 (It maps into a 1-dimensional feature space.)
\begin_inset Formula 
\[
\left\langle f(x),f(x')\right\rangle =f(x)f(x')=k(x,x').
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closure under Hadamard Products
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k_{1}$
\end_inset

 and 
\begin_inset Formula $k_{2}$
\end_inset

 are psd kernels with feature maps 
\begin_inset Formula $\phi_{1}$
\end_inset

 and 
\begin_inset Formula $\phi_{2}$
\end_inset

, respectively.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula 
\[
k_{1}(x,x')k_{2}(x,x')
\]

\end_inset

is a psd kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Take the outer product of the feature vectors: 
\begin_inset Formula 
\[
\phi(x)=\phi_{1}(x)\left[\phi_{2}(x)\right]^{T}.
\]

\end_inset

Note that 
\begin_inset Formula $\phi(x)$
\end_inset

 is a matrix.
\end_layout

\begin_layout Itemize
Continued...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closure under Hadamard Products
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Then
\begin_inset Formula 
\begin{eqnarray*}
\left\langle \phi(x),\phi(x')\right\rangle  & = & \sum_{i,j}\phi(x)\phi(x')\\
 & = & \sum_{i,j}\left[\phi_{1}(x)\left[\phi_{2}(x)\right]^{T}\right]_{ij}\left[\phi_{1}(x')\left[\phi_{2}(x')\right]^{T}\right]_{ij}\\
 & = & \sum_{i,j}\left[\phi_{1}(x)\right]_{i}\left[\phi_{2}(x)\right]_{j}\left[\phi_{1}(x')\right]_{i}\left[\phi_{2}(x')\right]_{j}\\
 & = & \left(\sum_{i}\left[\phi_{1}(x)\right]_{i}\left[\phi_{1}(x')\right]_{i}\right)\left(\sum_{j}\left[\phi_{2}(x)\right]_{j}\left[\phi_{2}(x')\right]_{j}\right)\\
 & = & k_{1}(x,x')k_{2}(x,x')
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Frame

\end_layout

\end_body
\end_document
