#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin /Users/drosen/Dropbox/repos/mlcourse/Lectures/
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty


% Set Color ==============================
\definecolor{BBGblue}{RGB}{13,157,219}
\definecolor{BBGgreen}{RGB}{77,170,80}


\setbeamercolor{title}{fg=BBGblue}
%\setbeamercolor{frametitle}{fg=BBGblue}
\setbeamercolor{frametitle}{fg=BBGblue}

\setbeamercolor{background canvas}{fg=BBGblue, bg=white}
\setbeamercolor{background}{fg=black, bg=BBGblue}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=black, bg=BBGblue}

\setbeamertemplate{headline}{}

\setbeamercolor{parttitle}{fg=BBGblue}
\setbeamercolor{sectiontitle}{fg=BBGblue}
\setbeamercolor{sectionname}{fg=BBGblue}
\setbeamercolor{section page}{fg=BBGblue}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
\setbeamercolor{section title}{fg=BBGblue}
 \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\usebeamercolor[fg]{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\end_preamble
\options aspectratio=169,handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "allcolors=BBGblue,urlcolor=BBGgreen"
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\boxbgcolor #ff31d8
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\dom}{\textrm{\textbf{dom} }}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\rank}{\text{\textbf{rank }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\conv}{\textrm{\textbf{conv} }}
\end_inset


\begin_inset FormulaMacro
\newcommand{\relint}{\text{\textbf{relint }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\aff}{\text{\textbf{aff }}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Title
Kernel Methods
\begin_inset Argument 1
status open

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David S.
 Rosenberg 
\end_layout

\begin_layout Date
October 26, 2017
\end_layout

\begin_layout Institute
Bloomberg ML EDU
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Just in article version
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO:
\end_layout

\begin_layout Plain Layout
URGENT: using 3 notations to specify to elements of input space: 
\begin_inset Formula $x,w$
\end_inset

, 
\begin_inset Formula $x^{(1)},x^{(2)}$
\end_inset

 and 
\begin_inset Formula $x,x'$
\end_inset

.
 Choose one and stick to it.
 Do NOT choose 
\begin_inset Formula $x,w$
\end_inset

 , because 
\begin_inset Formula $w$
\end_inset

 is our weight vector, which is very confusing.
 
\end_layout

\begin_layout Plain Layout
1) definition of 
\begin_inset Quotes eld
\end_inset

kernelized
\begin_inset Quotes erd
\end_inset

 – should we extend it? what about prediction? 
\end_layout

\begin_layout Plain Layout
2) work on transitions between SVM, linear kernel, kernel matrix – motivations
 etc.
\end_layout

\begin_layout Plain Layout
Maybe define the kernel matix in terms of kernel evaluations first and then
 plug in the linear inner product? Or introduce the Gram matrix of linear
 inner products very early on, and then later say we can replace it with
 a kernel matrix of inner products for a nonlinear inner product?
\end_layout

\begin_layout Plain Layout
3) I did some board work to show that the gram matri is XX^T.
 (XX^T)_{ij} = ? discussion of dimensionality nxn vs dxd to decide between
 kernel methods and primal form
\end_layout

\begin_layout Plain Layout
4) would be cool to demonstrate some kernel matrix substitutions and the
 effects on the output – maybe pull from the homework?
\end_layout

\begin_layout Plain Layout
5) the quadratic kernel shows up twice with different notations?
\end_layout

\begin_layout Plain Layout
6) discussion of coefficients of the monomials in the equivalent feature
 maps fro the polynomial kernels; these coefficients interact with the regulariz
ation;
\end_layout

\begin_layout Plain Layout
7) board work to explain what it would mean to have RBF kernel representable
 as an 
\begin_inset Quotes eld
\end_inset

inner product of feature vectors
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout
8) very interesrting treatment of the interaction between the kernel and
 the regularization described in Scholkopf and Smola's book 
\begin_inset Quotes eld
\end_inset

Learning with kernels
\begin_inset Quotes erd
\end_inset

 SECTION 4.3.
 
\end_layout

\begin_layout Plain Layout
9) need to rehearse the kernel trick summary
\end_layout

\begin_layout Plain Layout
10) are there situations where we can kernelize, but the representer theorem
 doesn't work? 
\end_layout

\begin_layout Plain Layout
11) should we break out the deck on representer theorem? YES [done]
\end_layout

\begin_layout Plain Layout
12) prove that l1 norm does not obey the parallelogram law [Homework?]
\end_layout

\begin_layout Plain Layout
13) illustrated projection on the board of 
\begin_inset Formula $x$
\end_inset

 onto subspace 
\begin_inset Formula $M$
\end_inset

.
 [Would be nice to have an image of that in the slides, just for the record]
\end_layout

\begin_layout Plain Layout
14) In looking at the 
\begin_inset Quotes eld
\end_inset

generalized objective
\begin_inset Quotes erd
\end_inset

 form, we say 
\begin_inset Quotes eld
\end_inset

what is 
\begin_inset Formula $\left\langle w,\psi(x_{i})\right\rangle $
\end_inset

 – it should be familiar.
 It's the score on 
\begin_inset Formula $x_{i}$
\end_inset

.
 So we have a general function L of the scores/predictions.
 Where are the labels 
\begin_inset Formula $y_{i}$
\end_inset

? theyr'e built into the function 
\begin_inset Formula $L$
\end_inset

.
\end_layout

\begin_layout Plain Layout
15) I need some rhyme or reason to when I use raw elements of input space
 
\begin_inset Formula $x_{i}$
\end_inset

 and 
\begin_inset Formula $x_{j}$
\end_inset

 and when I use 
\begin_inset Formula $\phi(x_{i})$
\end_inset

 and 
\begin_inset Formula $\phi(x_{j})$
\end_inset

.
 I think I can take a few slides to explain that generically 
\begin_inset Formula $\cx$
\end_inset

 is arbitrary, but all our methods use 
\begin_inset Formula $\reals^{d}$
\end_inset

.
 So for simplicity we'll usually just say 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset

 .
 But sometimes, e.g.
 in kernel context, it makes sense to talk abou kernels acting directly
 on 
\begin_inset Formula $\cx$
\end_inset

.
 So really only in kernel context is it worth taling about 
\begin_inset Formula $\phi$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Setup and Motivation
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Input Space 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Our general learning theory setup: no assumptions about 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
But 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset

 for the specific methods we've developed: 
\end_layout

\begin_deeper
\begin_layout Itemize
Ridge regression
\end_layout

\begin_layout Itemize
Lasso regression
\end_layout

\begin_layout Itemize
Support Vector Machines 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Our hypothesis space for these was all affine functions on 
\begin_inset Formula $\reals^{d}$
\end_inset

:
\begin_inset Formula 
\[
\ch=\left\{ x\mapsto w^{T}x+b\mid w\in\reals^{d},b\in\reals\right\} .
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What if we want to do prediction on inputs not natively in 
\begin_inset Formula $\reals^{d}$
\end_inset

?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Feature Extraction
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
Mapping an input from 
\begin_inset Formula $\cx$
\end_inset

 to a vector in 
\begin_inset Formula $\reals^{d}$
\end_inset

 is called 
\series bold
feature extraction
\series default
 or 
\series bold
featurization
\series default
.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Archive/2015/Lectures/source/4a.kernels-highlevel/feature-extraction.png
	lyxscale 60
	width 90text%

\end_inset


\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
e.g.
 Quadratic feature map: 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\begin_inset Formula 
\[
\ensuremath{\phi(x)=(x_{1},\ldots,x_{d},x_{1}^{2},\ldots,x_{d}^{2},\sqrt{2}x_{1}x_{2},\ldots,\sqrt{2}x_{i}x_{j},\ldots\sqrt{2}x_{d-1}x_{d})^{T}}.
\]

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Models with Explicit Feature Map
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space: 
\begin_inset Formula $\cx$
\end_inset

 (no assumptions)
\end_layout

\begin_layout Itemize
Introduce 
\series bold
feature map
\series default
 
\begin_inset Formula $\psi:\cx\to\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
The feature map maps into the 
\series bold
feature space
\series default
 
\begin_inset Formula $\reals^{d}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Hypothesis space of affine functions on feature space:
\begin_inset Formula 
\[
\ch=\left\{ x\mapsto w^{T}\psi(x)+b\mid w\in\reals^{d},b\in\reals\right\} .
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Geometric Example: Two class problem, nonlinear boundary
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/features/circularBoundary.png
	lyxscale 30
	height 50theight%

\end_inset


\end_layout

\begin_layout Itemize
With linear feature map 
\begin_inset Formula $\phi(x)=\left(x_{1},x_{2}\right)$
\end_inset

 and linear models, can't separate regions
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
With appropriate nonlinearity 
\begin_inset Formula $\phi(x)=\left(x_{1},x_{2},x_{1}^{2}+x_{2}^{2}\right)$
\end_inset

, piece of cake.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Video: 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://youtu.be/3liCbRZPrZA
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Expressivity of Hypothesis Space
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider a linear hypothesis space with a feature map 
\begin_inset Formula $\phi:\cx\to\reals^{d}$
\end_inset

:
\begin_inset Formula 
\[
\cf=\left\{ f(x)=w^{T}\phi(x)\right\} 
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/features/featuresMakeHypothesisSpace.png
	lyxscale 50
	width 75col%
	groupId percyPics

\end_inset

 
\end_layout

\begin_layout Standard
We can grow the linear hypothesis space 
\begin_inset Formula $\cf$
\end_inset

 by adding more features.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Models Need Big Feature Spaces
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
To get 
\series bold
expressive
\series default
 hypothesis spaces using linear models, 
\end_layout

\begin_deeper
\begin_layout Itemize
need high-dimensional feature spaces 
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Suppose we start with 
\begin_inset Formula $x=\left(1,x_{1},\ldots,x_{d}\right)\in\reals^{d+1}=\cx$
\end_inset

.
\end_layout

\begin_layout Itemize
We want to add all monomials up to degree 
\begin_inset Formula $M$
\end_inset

: 
\begin_inset Formula $x_{1}^{p_{1}}\cdots x_{d}^{p_{d}}$
\end_inset

, with 
\begin_inset Formula $p_{1}+\cdots+p_{d}\le M$
\end_inset

.
\end_layout

\begin_layout Itemize
How many features will we end up with?
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
\begin_inset Formula ${M+d \choose M}$
\end_inset

 (
\begin_inset Quotes eld
\end_inset

flower shop problem
\begin_inset Quotes erd
\end_inset

 from combinatorics)
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $d=40$
\end_inset

 and 
\begin_inset Formula $M=8$
\end_inset

, we get 
\begin_inset Formula $377348994$
\end_inset

 features.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
That will make some extremely large matrices...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Big Feature Spaces
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Very large feature spaces have two problems:
\end_layout

\begin_layout Enumerate
Overfitting
\end_layout

\begin_layout Enumerate
Memory and computational costs 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Overfitting we handle with regularization.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Kernel methods
\begin_inset Quotes erd
\end_inset

 can (sometimes) help with memory and computational costs.
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
In practice, kernel methods most applicable for linear methods with 
\begin_inset Formula $\ell_{2}$
\end_inset

 regularization.
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Kernel Methods: Motivation
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Review: Linear SVM and Dual
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The [featurized] SVM prediction function is the solution to
\begin_inset Formula 
\[
\min_{w\in\reals^{d},b\in\reals}\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\left(1-y_{i}\left[w^{T}\psi(x_{i})+b\right]\right)_{+}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Found it is equivalent to solve the dual problem to get 
\begin_inset Formula $\alpha^{*}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\sup_{\alpha} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}\psi\left(x_{j}\right)^{T}\psi(x_{i})\\
\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
 & \quad & \alpha_{i}\in\left[0,\frac{c}{n}\right]\;i=1,\ldots,n.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Notice: 
\begin_inset Formula $\psi\left(x\right)$
\end_inset

's only show up as inner products with other 
\begin_inset Formula $x$
\end_inset

's.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Some Methods Can Be 
\begin_inset Quotes eld
\end_inset

Kernelized
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A method is 
\series bold
kernelized 
\series default
if inputs only appear inside inner products: 
\begin_inset Formula $\left\langle \psi(x),\psi(x')\right\rangle $
\end_inset

 for 
\begin_inset Formula $x,x'\in\cx$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The 
\series bold
kernel function
\series default
 corresponding to 
\begin_inset Formula $\psi$
\end_inset

 and inner product 
\begin_inset Formula $\left\langle \cdot,\cdot\right\rangle $
\end_inset

 is 
\begin_inset Formula 
\[
k(x,x')=\left\langle \psi(x),\psi(x')\right\rangle .
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Why introduce this new notation 
\begin_inset Formula $k(x,x')$
\end_inset

?
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Turns out, we can often evaluate 
\begin_inset Formula $k(x,x')$
\end_inset

 directly,
\end_layout

\begin_deeper
\begin_layout Itemize
without explicitly computing 
\begin_inset Formula $\psi(x)$
\end_inset

 and 
\begin_inset Formula $\psi(x')$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
For large feature spaces, can be much faster.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernel Evaluation Can Be Fast
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Example
Quadratic feature map for 
\begin_inset Formula $x=\left(x_{1},\ldots,x_{d}\right)\in\reals^{d}$
\end_inset

.
\begin_inset Formula 
\[
\ensuremath{\phi(x)=(x_{1},\ldots,x_{d},x_{1}^{2},\ldots,x_{d}^{2},\sqrt{2}x_{1}x_{2},\ldots,\sqrt{2}x_{i}x_{j},\ldots\sqrt{2}x_{d-1}x_{d})^{T}}
\]

\end_inset

has dimension 
\begin_inset Formula $O(d^{2})\pause$
\end_inset

, but for any 
\begin_inset Formula $x,x'\in\reals^{d}$
\end_inset


\begin_inset Formula 
\[
k(x,x')=\left\langle \phi(x),\phi(x')\right\rangle =\left\langle x,x'\right\rangle +\left\langle x,x'\right\rangle ^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Naively explicit computation of 
\begin_inset Formula $k(x,x')$
\end_inset

: 
\begin_inset Formula $O(d^{2})$
\end_inset


\end_layout

\begin_layout Itemize
Implicit computation of 
\begin_inset Formula $k(x,x')$
\end_inset

: 
\begin_inset Formula $O(d)$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernels as Similarity Scores
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Often useful to think of the kernel function as a 
\series bold
similarity score
\series default
.
\end_layout

\begin_layout Itemize
But this is not a mathematically precise statement.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
There are many ways to design a similarity score.
 
\end_layout

\begin_deeper
\begin_layout Itemize
We will use 
\series bold
Mercer kernels, 
\series default
which correspond to inner products in some feature space.
\end_layout

\begin_layout Itemize
Has many mathematical benefits.
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
What are the Benefits of Kernelization?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Computational (e.g.
 when feature space dimension 
\begin_inset Formula $d$
\end_inset

 larger than sample size 
\begin_inset Formula $n$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
(WAIT– not sure we're prepared yet for this n vs d claim)
\end_layout

\end_inset

).
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Access to infinite-dimensional feature spaces.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Allows thinking in terms of 
\begin_inset Quotes eld
\end_inset

similarity
\begin_inset Quotes erd
\end_inset

 rather than features.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
(debatable)
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Section
Example: SVM 
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
SVM Dual
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Recall the SVM dual optimization problem for training set 
\begin_inset Formula $(x_{1},y_{1}),\ldots,(x_{n},y_{n})$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\sup_{\alpha} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i}\\
\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
 & \quad & \alpha_{i}\in\left[0,\frac{c}{n}\right]\;i=1,\ldots,n.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Can replace 
\begin_inset Formula $x_{j}^{T}x_{i}$
\end_inset

 by an arbitrary kernel 
\begin_inset Formula $k(x_{j},x_{i})$
\end_inset

.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
What kernel are we currently using?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space: 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Feature space: 
\begin_inset Formula $\ch=\reals^{d}$
\end_inset

, with standard inner product
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Feature map
\begin_inset Formula 
\[
\psi(x)=x
\]

\end_inset


\end_layout

\begin_layout Itemize
Kernel: 
\begin_inset Formula 
\[
k(x,x')=x^{T}x'
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Matrix (or the Gram Matrix)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
For points of 
\begin_inset Formula $x_{1},\ldots,x_{n}\in\cx$
\end_inset

 and an inner product 
\begin_inset Formula $\left\langle \cdot,\cdot\right\rangle $
\end_inset

 on 
\begin_inset Formula $\cx$
\end_inset

, the 
\series bold
kernel matrix
\series default
 or the 
\series bold
Gram matrix
\series default
 is defined as
\begin_inset Formula 
\[
K=\begin{pmatrix}\left\langle x_{i},x_{j}\right\rangle \end{pmatrix}_{i,j}=\begin{pmatrix}\left\langle x_{1},x_{1}\right\rangle  & \cdots & \left\langle x_{1},x_{n}\right\rangle \\
\vdots & \ddots & \cdots\\
\left\langle x_{n},x_{1}\right\rangle  & \cdots & \left\langle x_{n},x_{n}\right\rangle 
\end{pmatrix}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
Then for the standard Euclidean inner product 
\begin_inset Formula $\left\langle x_{i},x_{j}\right\rangle =x_{i}^{T}x_{j}$
\end_inset

, we have
\begin_inset Formula 
\[
K=XX^{T}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
SVM Dual with Kernel Matrix
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\sup_{\alpha} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}K_{ji}\\
\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
 & \quad & \alpha_{i}\in\left[0,\frac{c}{n}\right]\;i=1,\ldots,n.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Once our algorithm works with kernel matrices, we can change kernel just
 by changing the matrix.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Size of matrix: 
\begin_inset Formula $n\times n$
\end_inset

, where 
\begin_inset Formula $n$
\end_inset

 is the number of data points.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Recall with ridge regression, we worked with 
\begin_inset Formula $X^{T}X$
\end_inset

, which is 
\begin_inset Formula $d\times d$
\end_inset

, where 
\begin_inset Formula $d$
\end_inset

 is feature space dimension.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Some Nonlinear Kernels 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Quadratic Kernel in 
\begin_inset Formula $\reals^{2}$
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space: 
\begin_inset Formula $\cx=\reals^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Feature space: 
\begin_inset Formula $\ch=\reals^{5}$
\end_inset


\end_layout

\begin_layout Itemize
Feature map:
\begin_inset Formula 
\[
\psi:(x_{1},x_{2})\mapsto\left(x_{1},x_{2},x_{1}^{2},x_{2}^{2},\sqrt{2}x_{1}x_{2}\right)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Gives us ability to represent conic section boundaries.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Define kernel as inner product in feature space.
 For any 
\begin_inset Formula $w,x\in\reals^{2}$
\end_inset

:f
\begin_inset Formula 
\begin{eqnarray*}
k(w,x) & = & \langle\psi(w),\psi(x)\rangle\\
\pause & = & w_{1}x_{1}+w_{2}x_{2}+w_{1}^{2}x_{1}^{2}+w_{2}^{2}x_{2}^{2}+2w_{1}w_{2}x_{1}x_{2}\\
\pause & = & w_{1}x_{1}+w_{2}x_{2}+(w_{1}x_{1})^{2}+(w_{2}x_{2})^{2}+2(w_{1}x_{1})(w_{2}x_{2})\\
\pause & = & \langle w,x\rangle+\langle w,x\rangle^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Based on Guillaume Obozinski's Statistical Machine Learning course
 at Louvain, Feb 2014.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Quadratic Kernel in 
\begin_inset Formula $\reals^{d}$
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Feature space: 
\begin_inset Formula $\ch=\reals^{D}$
\end_inset

, where 
\begin_inset Formula $D=d+{d \choose 2}\approx d^{2}/2$
\end_inset

.
\end_layout

\begin_layout Itemize
Feature map:
\begin_inset Formula 
\[
\ensuremath{\phi(x)=(x_{1},\ldots,x_{d},x_{1}^{2},\ldots,x_{d}^{2},\sqrt{2}x_{1}x_{2},\ldots,\sqrt{2}x_{i}x_{j},\ldots\sqrt{2}x_{d-1}x_{d})^{T}}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then for 
\begin_inset Formula $\forall x,x'\in\reals^{d}$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
k(x,x') & = & \left\langle \phi(x),\phi(x')\right\rangle \\
\pause & = & \left\langle x,x'\right\rangle +\left\langle x,x'\right\rangle ^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Computation for inner product with explicit mapping: 
\begin_inset Formula $O(d^{2})$
\end_inset


\end_layout

\begin_layout Itemize
Computation for implicit kernel calculation: 
\begin_inset Formula $O(d)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Based on Guillaume Obozinski's Statistical Machine Learning course
 at Louvain, Feb 2014.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
String Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Computer virus finding: want to find a sequence of characters 
\begin_inset Formula $v$
\end_inset

 in a file that indicates file contains a virus.
\end_layout

\begin_layout Itemize
Alphabet set 
\begin_inset Formula $\Sigma$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\cx$
\end_inset

 is set of all finite-length strings over 
\begin_inset Formula $\Sigma$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\cx_{d}$
\end_inset

 is set of strings over 
\begin_inset Formula $\Sigma$
\end_inset

 of length at most 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Itemize
Feature map - indicator variable for every substring of length at most 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Polynomial Kernel in 
\begin_inset Formula $\reals^{d}$
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Kernel function:
\begin_inset Formula 
\[
k(x,x')=\left(1+\left\langle x,x'\right\rangle \right)^{M}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Corresponds to a feature map with all monomials up to degree 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For any 
\begin_inset Formula $M$
\end_inset

, computing the kernel has same computational cost
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Cost of explicit inner product computation grows rapidly in 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Radial Basis Function (RBF) / Gaussian Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset

.
 
\begin_inset Formula $\forall x,x'\in\reals^{d}$
\end_inset

,
\begin_inset Formula 
\[
k(w,x)=\exp\left(-\frac{\|x-x'\|^{2}}{2\sigma^{2}}\right),
\]

\end_inset

where 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is known as the bandwidth parameter.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Does it act like a similarity score?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Why 
\begin_inset Quotes eld
\end_inset

radial
\begin_inset Quotes erd
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Have we departed from our 
\begin_inset Quotes eld
\end_inset

inner product of feature vector
\begin_inset Quotes erd
\end_inset

 recipe?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Yes and no: corresponds to an infinite dimensional feature vector
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Probably the most common nonlinear kernel.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Kernel Trick: Overview
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The 
\begin_inset Quotes eld
\end_inset

Kernel Trick
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Given a kernelized ML algorithm.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Can swap out the inner product for a new kernel function.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
New kernel may correspond to a high dimensional feature space.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Once kernel matrix is computed, computational cost depends on number of
 data points, rather than the dimension of feature space.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
Swapping out a linear kernel for a new kernel is called the 
\series bold
kernel trick
\series default
.
\end_layout

\end_deeper
\begin_layout Section
Inner Product Spaces and Projections (Hilbert Spaces)
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
This development is based on Luenberger's 
\emph on
Optimization by Vector Space Methods.
\end_layout

\end_inset

 
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Inner Product Space (or 
\begin_inset Quotes eld
\end_inset

Pre-Hilbert
\begin_inset Quotes erd
\end_inset

 Spaces)
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
An
\series bold
 inner product space
\series default
 (over reals) is a vector space 
\begin_inset Formula $\cv$
\end_inset

 and an 
\series bold
inner product
\series default
, which is a mapping
\begin_inset Formula 
\[
\left\langle \cdot,\cdot\right\rangle :\cv\times\cv\to\reals
\]

\end_inset

that has the following properties 
\begin_inset Formula $\forall x,y,z\in\cv$
\end_inset

 and 
\begin_inset Formula $a,b\in\reals$
\end_inset

:
\end_layout

\begin_layout Itemize
Symmetry: 
\begin_inset Formula $\left\langle x,y\right\rangle =\left\langle y,x\right\rangle $
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Linearity: 
\begin_inset Formula $\left\langle ax+by,z\right\rangle =a\left\langle x,z\right\rangle +b\left\langle y,z\right\rangle $
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Positive-definiteness: 
\begin_inset Formula $\left\langle x,x\right\rangle \ge0$
\end_inset

 and 
\begin_inset Formula $\left\langle x,x\right\rangle =0\iff x=0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Norm from Inner Product
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
For an inner product space, we define a norm as
\begin_inset Formula 
\[
\|x\|=\sqrt{\left\langle x,x\right\rangle }.
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Example
\begin_inset Formula $\reals^{d}$
\end_inset

 with standard Euclidean inner product is an inner product space:
\begin_inset Formula 
\[
\left\langle x,y\right\rangle :=x^{T}y\qquad\forall x,y\in\reals^{d}.
\]

\end_inset

Norm is
\begin_inset Formula 
\[
\|x\|=\sqrt{x^{T}x}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
What norms can we get from an inner product?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Parallelogram Law]
\end_layout

\end_inset

 A norm 
\begin_inset Formula $\|\cdot\|$
\end_inset

 can be written in terms of an inner product on 
\begin_inset Formula $\cv$
\end_inset

 iff 
\begin_inset Formula $\forall x,x'\in\cv$
\end_inset

 
\begin_inset Formula 
\[
2\|x\|^{2}+2\|x'\|^{2}=\|x+x'\|^{2}+\|x-x'\|^{2},
\]

\end_inset

and if it can, the inner product is given by the
\series bold
 polarization identity
\series default

\begin_inset Formula 
\[
\left\langle x,x'\right\rangle =\frac{||x||^{2}+||x'||^{2}-||x-x'||^{2}}{2}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Example
\begin_inset Formula $\ell_{1}$
\end_inset

 norm on 
\begin_inset Formula $\reals^{d}$
\end_inset

 is NOT generated by an inner product.
 [Exercise] 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Standard
Is 
\begin_inset Formula $\ell_{2}$
\end_inset

 norm on 
\begin_inset Formula $\reals^{d}$
\end_inset

 generated by an inner product?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Pythagorean Theorem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
Two vectors are 
\series bold
orthogonal
\series default
 if 
\begin_inset Formula $\left\langle x,x'\right\rangle =0$
\end_inset

.
 We denote this by 
\begin_inset Formula $x\perp x'$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Definition
\begin_inset Formula $x$
\end_inset

 is orthogonal to a set 
\begin_inset Formula $S$
\end_inset

, i.e.
 
\begin_inset Formula $x\perp S$
\end_inset

, if 
\begin_inset Formula $x\perp s$
\end_inset

 for all 
\begin_inset Formula $x\in S$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Pythagorean Theorem]
\end_layout

\end_inset

 If 
\begin_inset Formula $x\perp x'$
\end_inset

, then 
\begin_inset Formula $\|x+x'\|^{2}=\|x\|^{2}+\|x'\|^{2}.$
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Proof
We have
\begin_inset Formula 
\begin{eqnarray*}
\|x+x'\|^{2} & = & \left\langle x+x',x+x'\right\rangle \\
\pause & = & \left\langle x,x\right\rangle +\left\langle x,x'\right\rangle +\left\langle x',x\right\rangle +\left\langle x',x'\right\rangle \\
\pause & = & \|x\|^{2}+\|x'\|^{2}.
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Projection onto a Plane (Rough Definition)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Choose some 
\begin_inset Formula $x\in\cv$
\end_inset

.
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $M$
\end_inset

 be a subspace of inner product space 
\begin_inset Formula $\cv$
\end_inset

.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula $m_{0}$
\end_inset

 is the 
\series bold
projection of 
\begin_inset Formula $x$
\end_inset

 onto 
\begin_inset Formula $M$
\end_inset

,
\end_layout

\begin_deeper
\begin_layout Itemize
if 
\begin_inset Formula $m_{0}\in M$
\end_inset

 and is the closest point to 
\begin_inset Formula $x$
\end_inset

 in 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
In math: For all 
\begin_inset Formula $m\in M$
\end_inset

, 
\begin_inset Formula 
\[
\|x-m_{0}\|\le\|x-m\|.
\]

\end_inset


\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
To show: projections exist and characterized by 
\begin_inset Formula $x-m_{0}\perp M$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Hilbert Space
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Projections exist for all finite-dimensional inner product spaces.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We want to allow infinite-dimensional spaces.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Need an extra condition called 
\series bold
completeness
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
A space is 
\series bold
complete
\series default
 if all Cauchy sequences in the space converge.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Definition
A 
\series bold
Hilbert space 
\series default
is a complete inner product space.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Example
Any finite dimensional inner product space is a Hilbert space.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Projection Theorem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Classical Projection Theorem]
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\ch$
\end_inset

 a Hilbert space
\end_layout

\begin_layout Itemize
\begin_inset Formula $M$
\end_inset

 a closed subspace of 
\begin_inset Formula $\ch$
\end_inset

 (picture a hyperplane through the origin)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For any 
\begin_inset Formula $x\in\ch$
\end_inset

, there 
\series bold
exists a unique 
\series default

\begin_inset Formula $m_{0}\in M$
\end_inset

 for which 
\begin_inset Formula 
\[
\|x-m_{0}\|\le\|x-m\|\;\forall m\in M.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
This 
\begin_inset Formula $m_{0}$
\end_inset

 is called the 
\series bold
[orthogonal] projection of 
\begin_inset Formula $x$
\end_inset

 onto 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Furthermore, 
\begin_inset Formula $m_{0}\in M$
\end_inset

 is the projection of 
\begin_inset Formula $x$
\end_inset

 onto 
\begin_inset Formula $M$
\end_inset

 iff
\begin_inset Formula 
\[
x-m_{0}\perp M.
\]

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Orthogonal Complements
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
Consider 
\begin_inset Formula $S\subset\cv$
\end_inset

, for an inner product space 
\begin_inset Formula $\cv$
\end_inset

.
 The set
\begin_inset Formula 
\[
S^{\perp}=\left\{ v\in\cv\mid v\perp S\right\} 
\]

\end_inset

is called the 
\series bold
orthogonal complement
\series default
 of 
\begin_inset Formula $S$
\end_inset

 [in 
\begin_inset Formula $\cv$
\end_inset

].
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Theorem
\begin_inset Formula $S^{\perp}$
\end_inset

 is a closed subspace of 
\begin_inset Formula $\cv$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Theorem

\end_layout

\begin_layout Theorem
If 
\begin_inset Formula $M$
\end_inset

 is a closed subspace of a Hilbert space 
\begin_inset Formula $\ch$
\end_inset

, then every 
\begin_inset Formula $x\in\ch$
\end_inset

 has a unique representation of the form
\begin_inset Formula 
\[
x=m+m^{\perp},
\]

\end_inset

where 
\begin_inset Formula $m\in M$
\end_inset

 and 
\begin_inset Formula $m^{\perp}\in M^{\perp}$
\end_inset

.
 
\end_layout

\end_deeper
\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Orthogonal Decomposition
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
If 
\begin_inset Formula $M$
\end_inset

 is a closed subspace of a Hilbert space 
\begin_inset Formula $\ch$
\end_inset

, then every 
\begin_inset Formula $x\in\ch$
\end_inset

 has a unique representation of the form
\begin_inset Formula 
\[
x=m+m^{\perp},
\]

\end_inset

where 
\begin_inset Formula $m\in M$
\end_inset

 and 
\begin_inset Formula $m^{\perp}\perp M$
\end_inset

.
 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Theorem

\end_layout

\begin_layout Proof
[We'll prove existence.] 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $m=\proj_{M}x$
\end_inset

.
 
\end_layout

\begin_layout Itemize
By Projection Theorem, 
\begin_inset Formula $m^{\perp}=x-m\perp M$
\end_inset

.
 
\end_layout

\begin_layout Itemize
So 
\begin_inset Formula 
\begin{eqnarray*}
x & = & m+x-m\\
 & = & m+m^{\perp}
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\end_deeper
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Projection Reduces Norm
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $M$
\end_inset

 be a closed subspace of 
\begin_inset Formula $\ch$
\end_inset

.
 For any 
\begin_inset Formula $x\in\ch$
\end_inset

, let 
\begin_inset Formula $m_{0}=\proj_{M}x$
\end_inset

 be the projection of 
\begin_inset Formula $x$
\end_inset

 onto 
\begin_inset Formula $M$
\end_inset

.
 Then
\begin_inset Formula 
\[
\|m_{0}\|\le\|x\|,
\]

\end_inset

with equality only when 
\begin_inset Formula $m_{0}=x$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Proof
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\|x\|^{2} & = & \|m_{0}+(x-m_{0})\|^{2}\pause\;(\mbox{note: }x-m_{0}\perp m_{0}\text{ by Projection theorem})\\
 & = & \|m_{0}\|^{2}+\|x-m_{0}\|^{2}\mbox{\pause}\ \mbox{by Pythagorean theorem}\\
\pause\|m_{0}\|^{2} & = & \|x\|^{2}-\|x-m_{0}\|^{2}\pause
\end{eqnarray*}

\end_inset

Then 
\begin_inset Formula $\|x-m_{0}\|^{2}\ge0$
\end_inset

 implies 
\begin_inset Formula $\|m_{0}\|^{2}\le\|x\|^{2}\pause$
\end_inset

.
 If 
\begin_inset Formula $\|x-m_{0}\|^{2}=0$
\end_inset

, then 
\begin_inset Formula $x=m_{0}$
\end_inset

, by definition of norm.
\end_layout

\end_deeper
\end_deeper
\begin_layout Section
Representer Theorem
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generalize from SVM Objective
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Featurized SVM objective: 
\begin_inset Formula 
\[
\min_{w\in\reals^{d}}\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\max\left(0,1-y_{i}\left[\left\langle w,\psi\left(x_{i}\right)\right\rangle \right]\right).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Generalized objective
\series default
: 
\begin_inset Formula 
\[
\min_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,\psi(x_{1})\right\rangle ,\ldots,\left\langle w,\psi(x_{n})\right\rangle \right),\pause
\]

\end_inset

where
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $R:\reals^{\ge0}\to\reals$
\end_inset

 is nondecreasing (
\series bold
Regularization term
\series default
)
\end_layout

\begin_layout Itemize
and 
\begin_inset Formula $L:\reals^{n}\to\reals$
\end_inset

 is arbitrary.
 (
\series bold
Loss term
\series default
)
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
General Objective Function for Linear Hypothesis Space (Details)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Generalized objective
\series default
: 
\begin_inset Formula 
\[
\min_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,\psi(x_{1})\right\rangle ,\ldots,\left\langle w,\psi(x_{n})\right\rangle \right),
\]

\end_inset

where
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $w,\psi(x_{1}),\ldots,\psi(x_{n})\in\ch$
\end_inset

 for some Hilbert space 
\begin_inset Formula $\ch$
\end_inset

.
 (We typically have 
\begin_inset Formula $\ch=\reals^{d}.)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\|\cdot\|$
\end_inset

 is the norm corresponding to the inner product of 
\begin_inset Formula $\ch$
\end_inset

.
 (i.e.
 
\begin_inset Formula $\|w\|=\sqrt{\left\langle w,w\right\rangle }$
\end_inset

) 
\end_layout

\begin_layout Itemize
\begin_inset Formula $R:[0,\infty)\to\reals$
\end_inset

 is nondecreasing (
\series bold
Regularization term
\series default
), and
\end_layout

\begin_layout Itemize
\begin_inset Formula $L:\reals^{n}\to\reals$
\end_inset

 is arbitrary (
\series bold
Loss term
\series default
).
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
General Objective Function for Linear Hypothesis Space (Details)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Generalized objective
\series default
: 
\begin_inset Formula 
\[
\min_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,\psi(x_{1})\right\rangle ,\ldots,\left\langle w,\psi(x_{n})\right\rangle \right),
\]

\end_inset


\end_layout

\begin_layout Itemize
What's 
\begin_inset Quotes eld
\end_inset

linear
\begin_inset Quotes erd
\end_inset

? 
\end_layout

\begin_layout Itemize
The prediction/score function 
\begin_inset Formula $x\mapsto\left\langle w,\psi(x_{i})\right\rangle $
\end_inset

 is linear – in what?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
in parameter vector 
\begin_inset Formula $w$
\end_inset

, and
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
in the feature vector 
\begin_inset Formula $\psi(x_{i})$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Why? [Real-valued] inner products are linear in each argument.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
The important part is the linearity in the parameter
\series default
 
\begin_inset Formula $w$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
When we discuss neural networks, we'll mention a 
\begin_inset Quotes eld
\end_inset

linear network
\begin_inset Quotes erd
\end_inset

 in which prediction functions are linear in the feature vector 
\begin_inset Formula $\psi(x)$
\end_inset

, but nonlinear in the parameter vector 
\begin_inset Formula $w$
\end_inset

.
 In other words, we have something like 
\begin_inset Formula 
\[
\min_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle f(w),\psi(x_{1})\right\rangle ,\ldots,\left\langle f(w),\psi(x_{n})\right\rangle \right),
\]

\end_inset

 for some (known) nonlinear function 
\begin_inset Formula $f$
\end_inset

.
 
\series bold
Our discussion will not apply to this situation
\series default
.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
General Objective Function for Linear Hypothesis Space (Details)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Generalized objective
\series default
: 
\begin_inset Formula 
\[
\min_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,\psi(x_{1})\right\rangle ,\ldots,\left\langle w,\psi(x_{n})\right\rangle \right),
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Ridge regression and SVM are of this form.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
What if we penalize with 
\begin_inset Formula $\lambda\|w\|_{2}$
\end_inset

 instead of 
\begin_inset Formula $\lambda\|w\|_{2}^{2}$
\end_inset

? 
\begin_inset Formula $\pause$
\end_inset

Yes!.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What if we use lasso regression? 
\begin_inset Formula $\pause$
\end_inset

No! 
\begin_inset Formula $\ell_{1}$
\end_inset

 norm does not correspond to an inner product.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status collapsed

\begin_layout Plain Layout
The Representer Theorem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Representer Theorem]
\end_layout

\end_inset

 Let 
\begin_inset Formula 
\[
J(w)=R\left(\|w\|\right)+L\left(\left\langle w,\psi(x_{1})\right\rangle ,\ldots,\left\langle w,\psi(x_{n})\right\rangle \right),
\]

\end_inset

where
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $w,\psi(x_{1}),\ldots,\psi(x_{n})\in\ch$
\end_inset

 for some Hilbert space 
\begin_inset Formula $\ch$
\end_inset

.
 (We typically have 
\begin_inset Formula $\ch=\reals^{d}.)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\|\cdot\|$
\end_inset

 is the norm corresponding to the inner product of 
\begin_inset Formula $\ch$
\end_inset

.
 (i.e.
 
\begin_inset Formula $\|w\|=\sqrt{\left\langle w,w\right\rangle }$
\end_inset

) 
\end_layout

\begin_layout Itemize
\begin_inset Formula $R:\reals^{\ge0}\to\reals$
\end_inset

 is nondecreasing (
\series bold
Regularization term
\series default
), and
\end_layout

\begin_layout Itemize
\begin_inset Formula $L:\reals^{n}\to\reals$
\end_inset

 is arbitrary (
\series bold
Loss term
\series default
).
\end_layout

\end_deeper
\begin_layout Theorem
If 
\begin_inset Formula $J(w)$
\end_inset

 has a minimizer, then it has a minimizer of the form 
\begin_inset Formula $w^{*}=\sum_{i=1}^{n}\alpha_{i}\psi(x_{i}).$
\end_inset


\begin_inset Newline newline
\end_inset

[If 
\begin_inset Formula $R$
\end_inset

 is strictly increasing, then all minimizers have this form.
 (Proof in homework.)]
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status collapsed

\begin_layout Plain Layout
The Representer Theorem (Proof)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Let 
\begin_inset Formula $w^{*}$
\end_inset

 be a minimizer.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Let 
\begin_inset Formula $M=\linspan\left(\psi(x_{1}),\ldots,\psi(x_{n})\right)$
\end_inset

.
 [the 
\series bold

\begin_inset Quotes eld
\end_inset

span of the data
\series default

\begin_inset Quotes erd
\end_inset

]
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Let 
\begin_inset Formula $w=\proj_{M}w^{*}$
\end_inset

.
 So 
\begin_inset Formula $\exists\alpha$
\end_inset

 s.t.
 
\begin_inset Formula $w=\sum_{i=1}^{n}\alpha_{i}\psi(x_{i})$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "enu:residual-is-orthogonal"

\end_inset

Then 
\begin_inset Formula $w^{\perp}:=w^{*}-w$
\end_inset

 is orthogonal to 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Projections decrease norms: 
\begin_inset Formula $\|w\|\le\|w^{*}\|$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Since 
\begin_inset Formula $R$
\end_inset

 is nondecreasing, 
\begin_inset Formula $R(\|w\|)\le R(\|w^{*}\|)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
By 
\begin_inset CommandInset ref
LatexCommand eqref
reference "enu:residual-is-orthogonal"

\end_inset

, 
\begin_inset Formula $\left\langle w^{*},\psi(x_{i})\right\rangle =\pause\left\langle w+w^{\perp},\psi(x_{i})\right\rangle =\pause\left\langle w,\psi(x_{i})\right\rangle $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset Formula $L\left(\left\langle w^{*},\psi(x_{1})\right\rangle ,\ldots,\left\langle w^{*},\psi(x_{n})\right\rangle \right)=L\left(\left\langle w,\psi(x_{1})\right\rangle ,\ldots,\left\langle w,\psi(x_{n})\right\rangle \right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset Formula $J(w)\le J(w^{*})$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Therefore 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $w=\sum_{i=1}^{n}\alpha_{i}\psi(x_{i})$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 is also a minimizer.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
Q.E.D.
\end_layout

\end_deeper
\begin_layout Section
Using Representer Theorem to Kernelize
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized Predictions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider 
\begin_inset Formula $w=\sum_{i=1}^{n}\alpha_{i}\psi\left(x_{i}\right)$
\end_inset

.
 (As representer theorem implies.)
\end_layout

\begin_layout Itemize
How do we make predictions for a given 
\begin_inset Formula $x\in\cx$
\end_inset

?
\begin_inset Formula 
\begin{eqnarray*}
\pause f(x)=\left\langle w,\psi(x)\right\rangle \pause & = & \left\langle \sum_{i=1}^{n}\alpha_{i}\psi\left(x_{i}\right),\psi(x)\right\rangle \\
\pause & = & \sum_{i=1}^{n}\alpha_{i}\left\langle \psi(x_{i}),\psi(x)\right\rangle \\
\pause & = & \sum_{i=1}^{n}\alpha_{i}k(x_{i},x)
\end{eqnarray*}

\end_inset

 
\series bold
Note
\series default
: 
\begin_inset Formula $f(x)$
\end_inset

 is a linear combination of 
\begin_inset Formula $k(x_{1},x),\ldots,k(x_{n},x)$
\end_inset

, all considered as functions of 
\begin_inset Formula $x$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized Regularization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider 
\begin_inset Formula $w=\sum_{i=1}^{n}\alpha_{i}\psi\left(x_{i}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
What does 
\begin_inset Formula $R(\|w\|)$
\end_inset

 look like?
\begin_inset Formula 
\begin{eqnarray*}
\pause\|w\|^{2} & = & \left\langle w,w\right\rangle \\
 & = & \left\langle \sum_{i=1}^{n}\alpha_{i}\psi\left(x_{i}\right),\sum_{j=1}^{n}\alpha_{j}\psi\left(x_{j}\right)\right\rangle \\
\pause & = & \sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}\left\langle \psi(x_{i}),\psi(x_{j})\right\rangle \\
\pause & = & \sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}k(x_{i},x_{j})\pause
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
(You should recognize the last expression as a quadratic form.)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Matrix (a.k.a.
 Gram Matrix)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
The 
\series bold
kernel matrix
\series default
 or 
\series bold
Gram matrix 
\series default
for a kernel 
\begin_inset Formula $k$
\end_inset

 on a set 
\begin_inset Formula $\left\{ x_{1},\ldots,x_{n}\right\} $
\end_inset

 is
\begin_inset Formula 
\[
K=\begin{pmatrix}k(x_{i},x_{j})\end{pmatrix}_{i,j}=\begin{pmatrix}k(x_{1},x_{1}) & \cdots & k(x_{1},x_{n})\\
\vdots & \ddots & \cdots\\
k(x_{n},x_{1}) & \cdots & k(x_{n},x_{n})
\end{pmatrix}\in\reals^{n\times n}.
\]

\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized Regularization: Matrix Form
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider 
\begin_inset Formula $w=\sum_{i=1}^{n}\alpha_{i}\psi\left(x_{i}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
What does 
\begin_inset Formula $R(\|w\|)$
\end_inset

 look like?
\begin_inset Formula 
\begin{eqnarray*}
\|w\|^{2} & = & \sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}k(x_{i},x_{j})\\
\pause & = & \alpha^{T}K\alpha
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
So 
\begin_inset Formula $R(\|w\|)=R\left(\sqrt{\alpha^{T}K\alpha}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized Predictions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Write 
\begin_inset Formula $f_{\alpha}(x)=\sum_{i=1}^{n}\alpha_{i}k(x,x_{i})$
\end_inset

.
 (Switched from 
\begin_inset Formula $k(x_{i},x)$
\end_inset

 by symmetry of inner product.)
\end_layout

\begin_layout Itemize
Predictions on the training points have a particularly simple form: 
\begin_inset Formula 
\begin{eqnarray*}
\pause\begin{pmatrix}f_{\alpha}(x_{1})\\
\vdots\\
f_{\alpha}(x_{n})
\end{pmatrix} & = & \begin{pmatrix}\alpha_{1}k(x_{1},x_{1})+\cdots+\alpha_{n}k(x_{1},x_{n})\\
\vdots\\
\alpha_{1}k(x_{n},x_{1})+\cdots+\alpha_{n}k(x_{n},x_{n})
\end{pmatrix}\\
\pause & = & \begin{pmatrix}k(x_{1},x_{1}) & \cdots & k(x_{1},x_{n})\\
\vdots & \ddots & \cdots\\
k(x_{n},x_{1}) & \cdots & k(x_{n},x_{n})
\end{pmatrix}\begin{pmatrix}\alpha_{1}\\
\vdots\\
\alpha_{n}
\end{pmatrix}\\
\pause & = & K\alpha
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized Objective
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Substituting 
\series bold

\begin_inset Formula 
\begin{eqnarray*}
w & = & \sum_{i=1}^{n}\alpha_{i}\psi\left(x_{i}\right)
\end{eqnarray*}

\end_inset


\series default
into generalized objective, we get 
\begin_inset Formula 
\[
\min_{\alpha\in\reals^{n}}R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
No direct access to 
\begin_inset Formula $\psi(x_{i})$
\end_inset

.
 
\end_layout

\begin_layout Itemize
All references are via kernel matrix 
\begin_inset Formula $K$
\end_inset

.
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
(Assumes 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $L$
\end_inset

 do not hide any references to 
\begin_inset Formula $\psi(x_{i})$
\end_inset

.) 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
This is the 
\series bold
kernelized objective function.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized SVM 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The SVM objective:
\begin_inset Formula 
\[
\min_{w\in\ch}\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\left(1-y_{i}\left[\left\langle w,\psi\left(x_{i}\right)\right\rangle \right]\right)_{+}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Kernelizing yields
\begin_inset Formula 
\[
\min_{\alpha\in\reals^{n}}\frac{1}{2}\alpha^{T}K\alpha+\frac{c}{n}\sum_{i=1}^{n}\left(1-y_{i}\left(K\alpha\right)_{i}\right)_{+}
\]

\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized Ridge Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Ridge Regression:
\begin_inset Formula 
\[
\min_{w\in\reals^{d}}\frac{1}{n}\sum_{i=1}^{n}\left(w^{T}x_{i}-y_{i}\right)^{2}+\lambda\|w\|^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Featurized Ridge Regression
\begin_inset Formula 
\[
\min_{w\in\ch}\frac{1}{n}\sum_{i=1}^{n}\left(\left\langle w,\psi(x_{i})\right\rangle -y_{i}\right)^{2}+\lambda\|w\|^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Kernelized Ridge Regression
\begin_inset Formula 
\[
\min_{\alpha\in\reals^{n}}\frac{1}{n}\|K\alpha-y\|^{2}+\lambda\alpha^{T}K\alpha,
\]

\end_inset

where 
\begin_inset Formula $y=\left(y_{1},\ldots,y_{n}\right)^{T}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Prediction Functions with RBF Kernel
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Radial Basis Function (RBF) / Gaussian Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\begin_inset Formula 
\[
k(w,x)=\exp\left(-\frac{\|w-x\|^{2}}{2\sigma^{2}}\right),
\]

\end_inset

where 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is known as the bandwidth parameter.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Does it act like a similarity score?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Why 
\begin_inset Quotes eld
\end_inset

radial
\begin_inset Quotes erd
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Have we departed from our 
\begin_inset Quotes eld
\end_inset

inner product of feature vector
\begin_inset Quotes erd
\end_inset

 recipe?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Yes and no: corresponds to an infinite dimensional feature vector
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Probably the most common nonlinear kernel.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
RBF Basis
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals$
\end_inset


\end_layout

\begin_layout Itemize
Output space: 
\begin_inset Formula $\cy=\reals$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
RBF kernel 
\begin_inset Formula $k(w,x)=\exp\left(-\left(w-x\right)^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Suppose we have 6 training examples: 
\begin_inset Formula $x_{i}\in\left\{ -6,-4,-3,0,2,4\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If representer theorem applies, then
\begin_inset Formula 
\[
f(x)=\sum_{i=1}^{6}\alpha_{i}k(x_{i},x).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $f$
\end_inset

 is a linear combination of 6 basis functions of form 
\begin_inset Formula $k(x_{i},\cdot)$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/kernels/kernel-fns-1.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset

 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
RBF Predictions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Basis functions
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/kernels/kernel-fns-1.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Predictions of the form 
\begin_inset Formula $f(x)=\sum_{i=1}^{6}\alpha_{i}k(x_{i},x)$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/kernels/kernel-fns-2.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
When kernelizing with RBF kernel, prediction functions always look this
 way.
\end_layout

\begin_layout Itemize
(Whether we get 
\begin_inset Formula $w$
\end_inset

 from SVM, ridge regression, etc...)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
RBF Feature Space: The Sequence Space 
\begin_inset Formula $\ell_{2}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
To work with infinite dimensional feature vectors, we need a space with
 certain properties.
\end_layout

\begin_deeper
\begin_layout Itemize
an inner product
\end_layout

\begin_layout Itemize
a norm related to the inner product 
\end_layout

\begin_layout Itemize
projection theorem: 
\begin_inset Formula $x=x_{\perp}+x_{\|}$
\end_inset

 where 
\begin_inset Formula $x_{\|}\in S=\linspan(w_{1},\ldots,w_{n})$
\end_inset

 and 
\begin_inset Formula $\left\langle x_{\perp},s\right\rangle =0\quad\forall s\in S$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Itemize
Basically, we need a Hilbert space.
\end_layout

\begin_layout Definition
\begin_inset Formula $\ell_{2}$
\end_inset

 is the space of all real-valued sequences: 
\begin_inset Formula $\left(x_{0},x_{1},x_{2},x_{3},\ldots\right)$
\end_inset

 with 
\begin_inset Formula $\sum_{i=0}^{\infty}x_{i}^{2}<\infty$
\end_inset

.
 
\end_layout

\begin_layout Theorem
With the the inner product 
\begin_inset Formula $\left\langle x,x'\right\rangle =\sum_{i=0}^{\infty}x_{i}x'_{i}$
\end_inset

, 
\begin_inset Formula $\ell_{2}$
\end_inset

 is a 
\series bold
Hilbert space
\series default
.
\end_layout

\begin_layout Theorem

\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status collapsed

\begin_layout Plain Layout
We know the inner product between 
\begin_inset Formula $x,x'\in\ell_{2}$
\end_inset

 is well-defined, as follows.
 First, note the following: For any numbers 
\begin_inset Formula $a,b\in\reals$
\end_inset

, we have 
\begin_inset Formula $\left|ab\right|\le\left|a\right|\left|b\right|\le\left(a^{2}+b^{2}\right)/2$
\end_inset

, since: 
\begin_inset Formula 
\begin{eqnarray*}
\left(\left|a\right|-\left|b\right|\right)^{2} & \ge & 0\\
\implies a^{2}+b^{2} & \ge & 2\left|a\right|\left|b\right|
\end{eqnarray*}

\end_inset

 Then let 
\begin_inset Formula $M=\sum_{i=0}^{\infty}x_{i}^{2}$
\end_inset

 and 
\begin_inset Formula $M'=\sum_{i=0}^{\infty}\left(x_{i}'\right)^{2}$
\end_inset

.
\begin_inset Formula 
\begin{eqnarray*}
\sum_{i=0}^{n}\left|x_{i}x_{i}'\right| & \le & \sum_{i=0}^{\infty}\left|x_{i}x_{i}'\right|\\
 & \le & \frac{1}{2}\left[\sum_{i=0}^{\infty}\left(x_{i}^{2}\right)+\sum_{i=0}^{\infty}\left(x_{i}'\right)^{2}\right]\le\left(M+M'\right)/2
\end{eqnarray*}

\end_inset

Thus 
\begin_inset Formula $\sum_{i=0}^{\infty}x_{i}x_{i}'$
\end_inset

 converges (since it is an absolutely convergent series).
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Infinite Dimensional Feature Vector for RBF
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider RBF kernel (1-dim): 
\begin_inset Formula $k(x,x')=\exp\left(-\left(x-x'\right)^{2}/2\right)$
\end_inset


\end_layout

\begin_layout Itemize
We claim that 
\begin_inset Formula $\psi:\reals\to\ell_{2}$
\end_inset

 defined by 
\begin_inset Formula 
\[
\left[\psi(x)\right]_{n}=\frac{1}{\sqrt{n!}}e^{-x^{2}/2}x^{n}
\]

\end_inset

 gives the 
\series bold

\begin_inset Quotes eld
\end_inset

infinite-dimensional feature vector
\begin_inset Quotes erd
\end_inset

 corresponding to RBF kernel
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Is this mapping even well-defined? Is 
\begin_inset Formula $\psi(x)$
\end_inset

 even an element of 
\begin_inset Formula $\ell_{2}$
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Yes: 
\begin_inset Formula 
\[
\sum_{n=0}^{\infty}\frac{1}{n!}e^{-x^{2}}x^{2n}=e^{-x^{2}}\sum_{n=0}^{\infty}\frac{\left(x^{2}\right)^{n}}{n!}=1<\infty
\]

\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Infinite Dimensional Feature Vector for RBF
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Does feature vector 
\begin_inset Formula $\left[\psi(x)\right]_{n}=\frac{1}{\sqrt{n!}}e^{-x^{2}/2}x^{n}$
\end_inset

 actually correspond to the RBF kernel?
\end_layout

\begin_layout Itemize
Yes! Proof:
\begin_inset Formula 
\begin{eqnarray*}
\left\langle \psi(x),\psi(x')\right\rangle  & = & \sum_{n=0}^{\infty}\frac{1}{n!}e^{-\left(x^{2}+\left(x'\right)^{2}\right)/2}x^{n}\left(x'\right)^{n}\\
 & = & e^{-\left(x^{2}+\left(x'\right)^{2}\right)/2}\sum_{n=0}^{\infty}\frac{\left(xx'\right)^{n}}{n!}\\
 & = & \exp\left(-\left[x^{2}+\left(x'\right)^{2}\right]/2\right)\exp\left(xx'\right)\\
 & = & \exp\left(-\left[(x-x')^{2}/2\right]\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
QED 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
When is 
\begin_inset Formula $k(x,x')$
\end_inset

 a kernel function? (Mercer's Theorem)
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
How to Get Kernels?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Explicitly construct 
\begin_inset Formula $\psi(x):\cx\to\reals^{d}$
\end_inset

 and define 
\begin_inset Formula $k(x,x')=\psi(x)^{T}\psi(x')$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Directly define the kernel function 
\begin_inset Formula $k(x,x')$
\end_inset

, and verify it corresponds to 
\begin_inset Formula $\left\langle \psi(x),\psi(x')\right\rangle $
\end_inset

 for some 
\begin_inset Formula $\psi$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
There are many theorems to help us with the second approach
\end_layout

\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Positive Semidefinite Matrices
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A real, symmetric matrix 
\begin_inset Formula $M\in\reals^{n\times n}$
\end_inset

 is 
\series bold
positive semidefinite (psd)
\series default
 if for any 
\begin_inset Formula $x\in\reals^{n}$
\end_inset

, 
\begin_inset Formula 
\[
x^{T}Mx\ge0.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Theorem
The following conditions are each necessary and sufficient for 
\begin_inset Formula $M$
\end_inset

 to be positive semidefinite:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $M$
\end_inset

 has a 
\begin_inset Quotes eld
\end_inset

square root
\begin_inset Quotes erd
\end_inset

, i.e.
 there exists 
\begin_inset Formula $R$
\end_inset

 s.t.
 
\begin_inset Formula $M=R^{T}R$
\end_inset

.
\end_layout

\begin_layout Itemize
All eigenvalues of 
\begin_inset Formula $M$
\end_inset

 are greater than or equal to 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Positive Semidefinite Function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A symmetric kernel function 
\begin_inset Formula $k:\cx\times\cx\to\reals$
\end_inset

 is 
\series bold
positive semidefinite (psd)
\series default
 if for any finite set 
\begin_inset Formula $\left\{ x_{1},\ldots,x_{n}\right\} \in\cx$
\end_inset

, the kernel matrix on this set 
\begin_inset Formula 
\[
K=\begin{pmatrix}k(x_{i},x_{j})\end{pmatrix}_{i,j}=\begin{pmatrix}k(x_{1},x_{1}) & \cdots & k(x_{1},x_{n})\\
\vdots & \ddots & \cdots\\
k(x_{n},x_{1}) & \cdots & k(x_{n},x_{n})
\end{pmatrix}
\]

\end_inset

is a positive semidefinite matrix.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Mercer's Theorem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
A symmetric function 
\begin_inset Formula $k(x,x')$
\end_inset

 can be expressed as an inner product
\begin_inset Formula 
\[
k(x,x')=\left\langle \psi(x),\psi(x')\right\rangle 
\]

\end_inset

for some 
\begin_inset Formula $\psi$
\end_inset

 if and only if 
\begin_inset Formula $k(x,x')$
\end_inset

 is 
\series bold
positive semidefinite.
\begin_inset Note Note
status open

\begin_layout Theorem

\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Proof
[Sketch] Suppose 
\begin_inset Formula $k(w,x)$
\end_inset

 is psd.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
Let 
\end_layout

\end_deeper
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generating New Kernels from Old
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k,k_{1},k_{2}:\cx\times\cx\to\reals$
\end_inset

 are psd kernels.
 Then so are the following:
\begin_inset Formula 
\begin{eqnarray*}
k_{\mbox{new}}(x,x') & = & k_{1}(x,x')+k_{2}(x,x')\\
k_{\mbox{new}}(x,x') & = & \alpha k(x,x')\\
k_{\mbox{new}}(x,x') & = & f(x)f(x')\mbox{ for any function \ensuremath{f(\cdot)}}\\
k_{\mbox{new}}(x,x') & = & k_{1}(x,x')k_{2}(x,x')
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
See Appendix for details.
\end_layout

\begin_layout Itemize
Lots more theorems to help you construct new kernels from old...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
\start_of_appendix
Details on New Kernels from Old
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Additive Closure
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k_{1}$
\end_inset

 and 
\begin_inset Formula $k_{2}$
\end_inset

 are psd kernels with feature maps 
\begin_inset Formula $\phi_{1}$
\end_inset

 and 
\begin_inset Formula $\phi_{2}$
\end_inset

, respectively.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula 
\[
k_{1}(x,x')+k_{2}(x,x')
\]

\end_inset

is a psd kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Concatenate the feature vectors to get 
\begin_inset Formula 
\[
\phi(x)=\left(\phi_{1}(x),\phi_{2}(x)\right).
\]

\end_inset

Then 
\begin_inset Formula $\phi$
\end_inset

 is a feature map for 
\begin_inset Formula $k_{1}+k_{2}$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closure under Positive Scaling
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k$
\end_inset

 is a psd kernel with feature maps 
\begin_inset Formula $\phi$
\end_inset

.
\end_layout

\begin_layout Itemize
Then for any 
\begin_inset Formula $\alpha>0$
\end_inset

, 
\begin_inset Formula 
\[
\alpha k
\]

\end_inset

is a psd kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Note that 
\begin_inset Formula 
\[
\phi(x)=\sqrt{\alpha}\phi(x)
\]

\end_inset

 is a feature map for 
\begin_inset Formula $\alpha k$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Scalar Function Gives a Kernel
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For any function 
\begin_inset Formula $f(x)$
\end_inset

, 
\begin_inset Formula 
\[
k(x,x')=f(x)f(x')
\]

\end_inset

is a kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Let 
\begin_inset Formula $f(x)$
\end_inset

 be the feature mapping.
 (It maps into a 1-dimensional feature space.)
\begin_inset Formula 
\[
\left\langle f(x),f(x')\right\rangle =f(x)f(x')=k(x,x').
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closure under Hadamard Products
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k_{1}$
\end_inset

 and 
\begin_inset Formula $k_{2}$
\end_inset

 are psd kernels with feature maps 
\begin_inset Formula $\phi_{1}$
\end_inset

 and 
\begin_inset Formula $\phi_{2}$
\end_inset

, respectively.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula 
\[
k_{1}(x,x')k_{2}(x,x')
\]

\end_inset

is a psd kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Take the outer product of the feature vectors: 
\begin_inset Formula 
\[
\phi(x)=\phi_{1}(x)\left[\phi_{2}(x)\right]^{T}.
\]

\end_inset

Note that 
\begin_inset Formula $\phi(x)$
\end_inset

 is a matrix.
\end_layout

\begin_layout Itemize
Continued...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closure under Hadamard Products
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Then
\begin_inset Formula 
\begin{eqnarray*}
\left\langle \phi(x),\phi(x')\right\rangle  & = & \sum_{i,j}\phi(x)\phi(x')\\
 & = & \sum_{i,j}\left[\phi_{1}(x)\left[\phi_{2}(x)\right]^{T}\right]_{ij}\left[\phi_{1}(x')\left[\phi_{2}(x')\right]^{T}\right]_{ij}\\
 & = & \sum_{i,j}\left[\phi_{1}(x)\right]_{i}\left[\phi_{2}(x)\right]_{j}\left[\phi_{1}(x')\right]_{i}\left[\phi_{2}(x')\right]_{j}\\
 & = & \left(\sum_{i}\left[\phi_{1}(x)\right]_{i}\left[\phi_{1}(x')\right]_{i}\right)\left(\sum_{j}\left[\phi_{2}(x)\right]_{j}\left[\phi_{2}(x')\right]_{j}\right)\\
 & = & k_{1}(x,x')k_{2}(x,x')
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\end_body
\end_document
