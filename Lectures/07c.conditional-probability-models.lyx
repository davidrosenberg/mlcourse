#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin /Users/drosen/Dropbox/repos/mlcourse/Lectures/
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty


% Set Color ==============================
\definecolor{BBGblue}{RGB}{13,157,219}
\definecolor{BBGgreen}{RGB}{77,170,80}


\setbeamercolor{title}{fg=BBGblue}
%\setbeamercolor{frametitle}{fg=BBGblue}
\setbeamercolor{frametitle}{fg=BBGblue}

\setbeamercolor{background canvas}{fg=BBGblue, bg=white}
\setbeamercolor{background}{fg=black, bg=BBGblue}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=black, bg=BBGblue}

\setbeamertemplate{headline}{}

\setbeamercolor{parttitle}{fg=BBGblue}
\setbeamercolor{sectiontitle}{fg=BBGblue}
\setbeamercolor{sectionname}{fg=BBGblue}
\setbeamercolor{section page}{fg=BBGblue}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
\setbeamercolor{section title}{fg=BBGblue}
 \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\usebeamercolor[fg]{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\end_preamble
\options aspectratio=169,handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "allcolors=BBGblue,urlcolor=BBGgreen"
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\boxbgcolor #ff31d8
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\dom}{\textrm{\textbf{dom} }}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\rank}{\text{\textbf{rank }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\conv}{\textrm{\textbf{conv} }}
\end_inset


\begin_inset FormulaMacro
\newcommand{\relint}{\text{\textbf{relint }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\aff}{\text{\textbf{aff }}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Title
Conditional Probability Models
\begin_inset Argument 1
status open

\begin_layout Plain Layout
ML 101
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David S.
 Rosenberg 
\end_layout

\begin_layout Date
November 9, 2017
\end_layout

\begin_layout Institute
Bloomberg ML EDU
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Just in article version
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

TODO: Must tie back log loss for bernoulli regression with logistic loss
 to the logistic regression.
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

What if our output space is itself a probability distribution?  KL divergence
 or cross-entropy loss; do we get maximium likelihood as a special case
 if we have degenerate distributions?
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Plots courtesy of Ningshan Zhang.}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Overview and Disclaimer
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Probabilistic Models vs GLMs
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Today we'll be talking about 
\series bold
linear probabilistic models
\series default
.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Most books and software libraries related to this topic are actually about
 
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
generalized linear models
\series default
 (GLMs).
 
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
GLMs are a special case of what we're talking about today.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
They're 
\begin_inset Quotes eld
\end_inset

special
\begin_inset Quotes erd
\end_inset

 because
\end_layout

\begin_deeper
\begin_layout Itemize
they're a restriction of our setting, but more importantly
\end_layout

\begin_layout Itemize
we can state theorems for GLMs, and
\end_layout

\begin_layout Itemize
all GLMs can be implemented in essentially the same way.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
However, a full development of GLMs requires a fair bit of additional machinery.
\end_layout

\begin_layout Itemize
I don't believe the machinery is worth the payoff at this level.
\end_layout

\end_deeper
\begin_layout Section
Generalized Regression
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generalized Regression / Conditional Distribution Estimation
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Given 
\begin_inset Formula $x$
\end_inset

, predict
\emph on
 probability distribution 
\emph default

\begin_inset Formula $p(y\mid x)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
How do we represent the probability distribution?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We'll consider 
\emph on
parametric families 
\emph default
of distributions.
\end_layout

\begin_deeper
\begin_layout Itemize
distribution represented by parameter vector
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Examples:
\end_layout

\begin_deeper
\begin_layout Enumerate
Logistic regression (Bernoulli distribution)
\end_layout

\begin_layout Enumerate
Probit regression (Bernoulli distribution)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Poisson regression (Poisson distribution)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Linear regression (Normal distribution, fixed variance)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Generalized Linear Models (GLM) (encompasses all of the above)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Generalized Additive Models (GAM)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Gradient Boosting Machines (GBM) / AnyBoost [in a few weeks]
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Bernoulli Regression
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Probabilistic Binary Classifiers
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Setting: 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset

, 
\begin_inset Formula $\cy=\left\{ 0,1\right\} $
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For each 
\begin_inset Formula $x$
\end_inset

, need to predict a distribution on 
\begin_inset Formula $\cy=\left\{ 0,1\right\} $
\end_inset

.
\end_layout

\begin_layout Itemize
How can we define a distribution supported on 
\begin_inset Formula $\left\{ 0,1\right\} $
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Sufficient to specify the 
\series bold
Bernoulli parameter
\series default
 
\begin_inset Formula $\theta=p(y=1\mid x)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We can refer to this distribution as 
\begin_inset Formula $\text{Bernoulli}(\theta)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Probabilistic Classifiers
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Setting: 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset

, 
\begin_inset Formula $\cy=\left\{ 0,1\right\} $
\end_inset


\end_layout

\begin_layout Itemize
Want prediction function to map each 
\begin_inset Formula $x\in\reals^{d}$
\end_inset

 to the right 
\begin_inset Formula $\theta\in\left[0,1\right]$
\end_inset

.
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Plain Layout
[At this point, a binary classification problem on the board with 
\begin_inset Formula $\cx=\reals$
\end_inset

, draw the true conditional probability line.
 Draw some class labels 
\begin_inset Formula $0/1$
\end_inset

 and put them on the Y-axis.
\end_layout

\begin_layout Itemize
Somehow we need to map 
\begin_inset Formula $x\in\reals^{d}$
\end_inset

 to some 
\begin_inset Formula $\theta\in[0,1]$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We first 
\series bold
extract
\series default
 
\series bold
information 
\series default
from 
\begin_inset Formula $x\in\reals^{d}$
\end_inset

 and summarize in a single number.
\end_layout

\begin_deeper
\begin_layout Itemize
That number is analogous to the 
\series bold
score
\series default
 in classification.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
For a 
\series bold
linear method
\series default
, this extraction is done with a linear function:
\begin_inset Formula 
\[
\underbrace{x}_{\in\reals^{D}}\mapsto\underbrace{w^{T}x}_{\in\reals}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
As usual, 
\begin_inset Formula $x\mapsto w^{T}x$
\end_inset

 will include affine functions if we include a constant feature in 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $w^{T}x$
\end_inset

 is called the 
\series bold
linear predictor.
 
\end_layout

\begin_layout Itemize
Still need to map this to 
\begin_inset Formula $[0,1]$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Transfer Function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Need a function to map the linear predictor in 
\begin_inset Formula $\reals$
\end_inset

 to 
\begin_inset Formula $\left[0,1\right]$
\end_inset

:
\begin_inset Formula 
\[
\underbrace{x}_{\in\reals^{D}}\mapsto\underbrace{w^{T}x}_{\in\reals}\mapsto\underbrace{f(w^{T}x)}_{\in[0,1]}=\theta,
\]

\end_inset

where 
\begin_inset Formula $f:\reals\to[0,1]$
\end_inset

.
 We'll call 
\begin_inset Formula $f$
\end_inset

 the 
\series bold
transfer 
\series default
function.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
So prediction function is 
\begin_inset Formula $x\mapsto f(w^{T}x)$
\end_inset

, which gives value for 
\begin_inset Formula $\theta=p(y=1\mid x)$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout AlertBlock
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Terminology Alert
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
In generalized linear models (GLMs), if 
\begin_inset Formula $\theta$
\end_inset

 is the distribution mean, then 
\begin_inset Formula $f$
\end_inset

 is called the 
\series bold
\color red
response function
\series default
 or 
\series bold
inverse link 
\series default
function.
 
\series bold
\color inherit
Transfer function is not standard terminology
\series default
\color red
, but we're avoiding the heavy set of definitions needed for a full development
 of GLMs, which is actually more restrictive than our current framework.
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Frame
\begin_inset Note Note
status open

\begin_layout Plain Layout
Not sure where I saw 
\begin_inset Quotes eld
\end_inset

transfer function
\begin_inset Quotes erd
\end_inset

.
 
\begin_inset Quotes eld
\end_inset

Response function
\begin_inset Quotes erd
\end_inset

 is from Jordan's book.
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Should I give a whole slide explaining that GLMs and GLM packages are what
 you'll see in practice?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Transfer Functions for Bernoulli
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Two commonly used transfer functions to map from 
\begin_inset Formula $w^{T}x$
\end_inset

 to 
\begin_inset Formula $\theta$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename ../Figures/GLM/bernoulliInverseLinkFunctions.pdf
	width 60text%

\end_inset


\end_layout

\begin_layout Itemize
Logistic function: 
\begin_inset Formula $f(\eta)=\frac{1}{1+e^{-\eta}}$
\end_inset

 
\begin_inset Formula $\pause\implies$
\end_inset

 Logistic Regression
\end_layout

\begin_layout Itemize
Normal CDF 
\begin_inset Formula $f(\eta)=\int_{-\infty}^{\eta}\frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}$
\end_inset


\begin_inset Formula $\pause\implies$
\end_inset

 Probit Regression
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Learning
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\cy=\pause\left\{ 0,1\right\} $
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\ca=[0,1]$
\end_inset

 (Representing Bernoulli
\begin_inset Formula $(\theta)$
\end_inset

 distributions by 
\begin_inset Formula $\theta\in\left[0,1\right]$
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\ch=\left\{ x\mapsto f(w^{T}x)\mid w\in\reals^{d}\right\} $
\end_inset

 (Each prediction function represented by 
\begin_inset Formula $w\in\reals^{d}.)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We can choose 
\begin_inset Formula $w$
\end_inset

 using maximum likelihood...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bernoulli Regression: Likelihood Scoring
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have data 
\begin_inset Formula $\cd=\left\{ (x_{1},y_{1}),\ldots,(x_{n},y_{n})\right\} $
\end_inset

.
\end_layout

\begin_layout Itemize
Compute the model likelihood for 
\begin_inset Formula $\cd$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
p_{w}(\cd) & = & \prod_{i=1}^{n}p_{w}(y_{i}\mid x_{i})\text{ [by independence]}\\
\pause & = & \prod_{i=1}^{n}\left[f(w^{T}x_{i})\right]^{y_{i}}\left[1-f(w^{T}x_{i})\right]^{1-y_{i}}.
\end{eqnarray*}

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Draw table on board showing 
\begin_inset Formula $x_{i}$
\end_inset

,
\begin_inset Formula $y_{i}\in\left\{ 0,1\right\} $
\end_inset

, 
\begin_inset Formula $\hat{\theta}_{i}=f(w^{T}x),p(y_{i};\hat{\theta}_{i})$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Huh? Remember 
\begin_inset Formula $y_{i}\in\left\{ 0,1\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Easier to work with the log-likelihood:
\begin_inset Formula 
\[
\log p_{w}(\cd)=\sum_{i=1}^{n}y_{i}\log f(w^{T}x_{i})+\left(1-y_{i}\right)\log\left[1-f(w^{T}x_{i})\right]
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bernoulli Regression: MLE
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Maximum Likelihood Estimation (MLE) finds 
\begin_inset Formula $w$
\end_inset

 maximizing 
\begin_inset Formula $\log p_{w}(\cd)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Equivalently, minimize the objective function
\begin_inset Formula 
\[
J(w)=-\left[\sum_{i=1}^{n}y_{i}\log f(w^{T}x_{i})+\left(1-y_{i}\right)\log\left[1-f(w^{T}x_{i})\right]\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For differentiable 
\begin_inset Formula $f$
\end_inset

,
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $J(w)$
\end_inset

 is differentiable, and we can use our standard tools.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Homework: Derive the SGD step directions for logistic regression and [harder]
 probit regression.
\end_layout

\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Frame

\end_layout

\begin_layout Section
Poisson Regression
\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\poi}{\text{Poisson}}
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Poisson Regression: Setup
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset

, Output space 
\begin_inset Formula $\cy=\left\{ 0,1,2,3,4,\dots\right\} $
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
In Poisson regression, prediction functions produce a Poisson distribution.
\end_layout

\begin_deeper
\begin_layout Itemize
Represent Poisson
\begin_inset Formula $\left(\lambda\right)$
\end_inset

 distribution by the mean parameter 
\begin_inset Formula $\lambda\in\left(0,\infty\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Action space 
\begin_inset Formula $\ca=\left(0,\infty\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
In Poisson regression, 
\begin_inset Formula $x$
\end_inset

 enters 
\series bold
linearly:
\series default
 
\begin_inset Formula $x\mapsto\underbrace{w^{T}x}_{\reals}\mapsto\lambda=\underbrace{f(w^{T}x)}_{(0,\infty)}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What can we use as the transfer function 
\begin_inset Formula $f:\reals\to\left(0,\infty\right)$
\end_inset

?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Poisson Regression: Transfer Function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In Poisson regression, 
\begin_inset Formula $x$
\end_inset

 enters 
\series bold
linearly:
\series default
 
\begin_inset Formula 
\[
x\mapsto\underbrace{w^{T}x}_{\reals}\mapsto\lambda=\underbrace{f(w^{T}x)}_{(0,\infty)}.
\]

\end_inset


\end_layout

\begin_layout Itemize
Standard approach is to take
\begin_inset Formula 
\[
f(w^{T}x)=\exp\left(w^{T}x\right).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Note that range of 
\begin_inset Formula $f(w^{T}x)\in\left(0,\infty\right)$
\end_inset

, (appropriate for the Poisson parameter).
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Poisson Regression: Likelihood Scoring
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have data 
\begin_inset Formula $\cd=\left\{ (x_{1},y_{1}),\ldots,(x_{n},y_{n})\right\} $
\end_inset

.
\end_layout

\begin_layout Itemize
Recall the log-likelihood for Poisson is:
\begin_inset Formula 
\begin{eqnarray*}
\log p(\cd,\lambda) & = & \sum_{i=1}^{n}\left[y_{i}\log\lambda-\lambda-\log\left(y_{i}!\right)\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Plugging in 
\begin_inset Formula $f(w^{T}x)=\exp\left(w^{T}x\right)$
\end_inset

 for 
\begin_inset Formula $\lambda$
\end_inset

, we get
\begin_inset Formula 
\begin{eqnarray*}
\log p(\cd,\lambda) & = & \sum_{i=1}^{n}\left[y_{i}\log\left[\exp\left(w^{T}x\right)\right]-\exp\left(w^{T}x\right)-\log\left(y_{i}!\right)\right]\\
 & = & \sum_{i=1}^{n}\left[y_{i}w^{T}x-\exp\left(w^{T}x\right)-\log\left(y_{i}!\right)\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Maximize this w.r.t.
 
\begin_inset Formula $w$
\end_inset

 to get our Poisson regression fit.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
No closed form for optimum, but it's concave, so easy to optimize.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Poisson Regression Example
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/drosen/Dropbox/repos/mlcourse/Figures/conditional-probability-models/poissonBumps2.pdf
	height 70pheight%

\end_inset


\end_layout

\begin_layout Plain Layout
e.g.
 Phone call counts per day for a startup company, over 300 days.
\end_layout

\end_deeper
\begin_layout Plain Layout
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
What About Nonlinear Score Functions
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Poisson Count Example
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/drosen/Dropbox/repos/mlcourse/Figures/conditional-probability-models/poissonBumps.pdf
	height 70pheight%

\end_inset


\end_layout

\end_deeper
\begin_layout Plain Layout
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Let's Use Gradient Boosting
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Recall the log-likelihood for Poisson regression
\begin_inset Formula 
\begin{eqnarray*}
\log p(\cd,\lambda) & = & \sum_{i=1}^{n}\left[y_{i}w^{T}x_{i}-\exp\left(w^{T}x_{i}\right)-\log\left(y_{i}!\right)\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Let's replace 
\begin_inset Formula $w^{T}x$
\end_inset

 by a general function 
\begin_inset Formula $f(x)$
\end_inset

:
\begin_inset Formula 
\[
J(f)=\sum_{i=1}^{n}\left[y_{i}f(x_{i})-\exp\left(f(x_{i})\right)-\log\left(y_{i}!\right)\right]
\]

\end_inset


\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Section
Conditional Gaussian Regression
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gaussian Linear Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset

, Output space 
\begin_inset Formula $\cy=\reals$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
In Gaussian regression, prediction functions produce a distribution 
\begin_inset Formula $\cn(\mu,\sigma^{2})$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Assume 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is known.
\end_layout

\end_deeper
\begin_layout Itemize
Represent 
\begin_inset Formula $\cn(\mu,\sigma^{2})$
\end_inset

 by the mean parameter 
\begin_inset Formula $\mu\in\reals$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Action space 
\begin_inset Formula $\ca=\reals$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
In Gaussian linear regression, 
\begin_inset Formula $x$
\end_inset

 enters 
\series bold
linearly:
\series default
 
\begin_inset Formula $x\mapsto\underbrace{w^{T}x}_{\reals}\mapsto\mu=\underbrace{f(w^{T}x)}_{\reals}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Since 
\begin_inset Formula $\mu\in\reals$
\end_inset

, we can take the identity link function: 
\begin_inset Formula $f(w^{T}x)=w^{T}x$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gaussian Regression: Likelihood Scoring
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have data 
\begin_inset Formula $\cd=\left\{ (x_{1},y_{1}),\ldots,(x_{n},y_{n})\right\} $
\end_inset

.
\end_layout

\begin_layout Itemize
Compute the model likelihood for 
\begin_inset Formula $\cd$
\end_inset

:
\begin_inset Formula 
\begin{align*}
p_{w}(\cd)= & \prod_{i=1}^{n}p_{w}(y_{i}\mid x_{i})\text{ [by independence]}
\end{align*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Maximum Likelihood Estimation (MLE) finds 
\begin_inset Formula $w$
\end_inset

 maximizing 
\begin_inset Formula $p_{w}(\cd)$
\end_inset

.
\end_layout

\begin_layout Itemize
Equivalently, maximize the data log-likelihood:
\begin_inset Formula 
\[
\minimizer w=\argmax_{w\in\reals^{d}}\sum_{i=1}^{n}\log p_{w}(y_{i}\mid x_{i})
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let's start solving this!
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gaussian Regression: MLE
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The conditional log-likelihood is:
\begin_inset Formula 
\begin{eqnarray*}
 &  & \sum_{i=1}^{n}\log p_{w}(y_{i}\mid x_{i})\\
\pause & = & \sum_{i=1}^{n}\log\left[\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(y_{i}-w^{T}x_{i})^{2}}{2\sigma^{2}}\right)\right]\\
\pause & = & \underbrace{\sum_{i=1}^{n}\log\left[\frac{1}{\sigma\sqrt{2\pi}}\right]}_{\mbox{independent of }w}+\sum_{i=1}^{n}\left(-\frac{(y_{i}-w^{T}x_{i})^{2}}{2\sigma^{2}}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
MLE is the 
\begin_inset Formula $w$
\end_inset

 where this is maximized.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Note that 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is irrelevant to finding the maximizing 
\begin_inset Formula $w$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can drop the negative sign and make it a minimization problem.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gaussian Regression: MLE
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The MLE is 
\begin_inset Formula 
\begin{align*}
\minimizer w= & \argmin_{w\in\reals^{d}}\sum_{i=1}^{n}(y_{i}-w^{T}x_{i})^{2}
\end{align*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
This is exactly the objective function for least squares.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
From here, can use usual approaches to solve for 
\begin_inset Formula $w^{*}$
\end_inset

(SGD, linear algebra, calculus, etc.) 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
NOTE: Parameter vector 
\begin_inset Formula $w$
\end_inset

 only interacts with 
\begin_inset Formula $x$
\end_inset

 by an inner product
\end_layout

\begin_layout Itemize
NOTE: The variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

 falls out of the objective function.
\end_layout

\begin_deeper
\begin_layout Itemize
Can use maximum likelihood to estimate 
\begin_inset Formula $\sigma^{2}$
\end_inset

 as well?
\end_layout

\end_deeper
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Multinomial Logistic Regression
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Multinomial Logistic Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Setting: 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset

, 
\begin_inset Formula $\cy=\left\{ 1,\ldots,k\right\} $
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
For each 
\begin_inset Formula $x$
\end_inset

, we want to produce a distribution on 
\begin_inset Formula $k$
\end_inset

 classes.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Such a distribution is called a 
\begin_inset Quotes eld
\end_inset


\series bold
multinoulli
\series default

\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset


\series bold
categorical
\series default

\begin_inset Quotes erd
\end_inset

 distribution.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Represent categorical distribution by probability vector 
\begin_inset Formula $\theta=\left(\theta_{1},\ldots,\theta_{k}\right)\in\reals^{k}$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\sum_{i=1}^{k}\theta_{i}=1$
\end_inset

 and 
\begin_inset Formula $\theta_{i}\ge0$
\end_inset

 for 
\begin_inset Formula $i=1,\ldots,k$
\end_inset

 (i.e.
 
\begin_inset Formula $\theta$
\end_inset

 represents a 
\series bold
distribution
\series default
) and
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
So 
\begin_inset Formula $\forall y\in\left\{ 1,\ldots,k\right\} $
\end_inset

, 
\begin_inset Formula $p(y)=\theta_{y}$
\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
That is, for each 
\begin_inset Formula $x$
\end_inset

 and each 
\begin_inset Formula $y\in\left\{ 1,\ldots,k\right\} $
\end_inset

, we want to produce a probability where 
\begin_inset Formula $\sum_{y=1}^{k}\theta_{y}=1$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Multinomial Logistic Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
From each 
\begin_inset Formula $x$
\end_inset

, we compute a linear score function for each class: 
\begin_inset Formula 
\[
x\mapsto\left(\left\langle w_{1},x\right\rangle ,\ldots,\left\langle w_{k},x\right\rangle \right)\in\reals^{k}
\]

\end_inset


\end_layout

\begin_layout Itemize
We need to map this 
\begin_inset Formula $\reals^{k}$
\end_inset

 vector into a probability vector.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Use the 
\series bold
softmax function:
\begin_inset Formula 
\[
\left(\left\langle w_{1},x\right\rangle ,\ldots,\left\langle w_{k},x\right\rangle \right)\mapsto\theta=\left(\frac{\exp\left(w_{1}^{T}x\right)}{\sum_{i=1}^{k}\exp\left(w_{i}^{T}x\right)},\ldots,\frac{\exp\left(w_{k}^{T}x\right)}{\sum_{i=1}^{k}\exp\left(w_{i}^{T}x\right)}\right)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Note that 
\begin_inset Formula $\theta\in\reals^{k}$
\end_inset

 and
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i} & > & 0\qquad i=1,\ldots,k\\
\sum_{i=1}^{k}\theta_{i} & = & 1
\end{eqnarray*}

\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Multinomial Logistic Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Putting this together, we write multinomial logistic regression as
\begin_inset Formula 
\[
p(y\mid x)=\frac{\exp\left(w_{y}^{T}x\right)}{\sum_{i=1}^{k}\exp\left(w_{i}^{T}x\right)},
\]

\end_inset

where we've introduced parameter vectors 
\begin_inset Formula $w_{1},\ldots,w_{k}\in\reals^{d}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Do we still see score functions in here?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can view 
\begin_inset Formula $x\mapsto w_{y}^{T}x$
\end_inset

 as the score for class 
\begin_inset Formula $y$
\end_inset

, for 
\begin_inset Formula $y\in\left\{ 1,\ldots,k\right\} $
\end_inset

.
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
We can also 
\begin_inset Quotes eld
\end_inset

flatten
\begin_inset Quotes erd
\end_inset

 this as we did for multiclass classification.
\end_layout

\begin_deeper
\begin_layout Itemize
Introduce a class-sensitive feature vector 
\begin_inset Formula $\Psi(x,y)\in\reals^{d\times k}$
\end_inset


\end_layout

\begin_layout Itemize
Parameter vector 
\begin_inset Formula $w\in\reals^{d\times k}$
\end_inset

.
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
How do we do learning here? What parameters are we estimatimg?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Our model is specified once we have 
\begin_inset Formula $w_{1},\ldots,w_{k}\in\reals^{d}$
\end_inset

.
\end_layout

\begin_layout Itemize
Find parameter settings maximizing the log-likelihood of data 
\begin_inset Formula $\cd$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
This objective function is concave in 
\begin_inset Formula $w$
\end_inset

's and straightforward to optimize.
\end_layout

\end_deeper
\begin_layout Section
Maximum Likelihood as ERM
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generalized Regression as Statistical Learning
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\begin_layout Itemize
Outcome space 
\begin_inset Formula $\cy$
\end_inset


\end_layout

\begin_layout Itemize
All pairs 
\begin_inset Formula $(x,y)$
\end_inset

 are independent with distribution 
\begin_inset Formula $P_{\cx\times\cy}$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Action space
\series default
 
\begin_inset Formula $\ca=\left\{ p(y)\mid p\text{ is a probability density or mass function on }\cy\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Hypothesis space 
\begin_inset Formula $\ch$
\end_inset

 contains decision functions 
\begin_inset Formula $f:\cx\to\ca$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Given an 
\begin_inset Formula $x\in\cx$
\end_inset

, predict a probability distribution 
\begin_inset Formula $p(y)$
\end_inset

 on 
\begin_inset Formula $\cy$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Maximum likelihood estimation for dataset 
\begin_inset Formula $\cd=\left((x_{1},y_{1}),\ldots,(x_{n},y_{n}\right)$
\end_inset

 is
\begin_inset Formula 
\[
\hat{f}_{\text{MLE}}=\argmax_{f\in\ch}\sum_{i=1}^{n}\log\left[f(x_{i})(y_{i})\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout ExampleBlock
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Exercise
\end_layout

\end_inset


\end_layout

\begin_layout ExampleBlock
Write the MLE optimization as empirical risk minimization.
 What's the loss?
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generalized Regression as Statistical Learning
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Take loss 
\begin_inset Formula $\ell:\ca\times\cy\to\reals$
\end_inset

 for a predicted PDF or PMF 
\begin_inset Formula $p(y)$
\end_inset

 and outcome 
\begin_inset Formula $y$
\end_inset

 to be
\begin_inset Formula 
\[
\ell(p,y)=-\log p(y)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The risk of decision function 
\begin_inset Formula $f:\cx\to\ca$
\end_inset

 is
\begin_inset Formula 
\[
R(f)=-\ex_{x,y}\log\left[f(x)(y)\right],
\]

\end_inset

where 
\begin_inset Formula $f(x)$
\end_inset

 is a PDF or PMF on 
\begin_inset Formula $\cy$
\end_inset

, and we're evaluating it on 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generalized Regression as Statistical Learning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The empirical risk of 
\begin_inset Formula $f$
\end_inset

 for a sample 
\begin_inset Formula $\cd=\left\{ y_{1},\ldots,y_{n}\right\} \in\cy$
\end_inset

 is 
\begin_inset Formula 
\[
\hat{R}(f)=-\frac{1}{n}\sum_{i=1}^{n}\log\left[f(x_{i})\right](y_{i}).
\]

\end_inset

This is called the negative 
\series bold
conditional log-likelihood
\series default
.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Thus for the negative log-likelihood loss, ERM and MLE are equivalent
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
A Note on Notation
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Hypothesis spaces contain decision functions 
\begin_inset Formula $f:\cx\to\ca$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Given an 
\begin_inset Formula $x\in\cx$
\end_inset

, predict a probability distribution 
\begin_inset Formula $p(y)$
\end_inset

 on 
\begin_inset Formula $\cy$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $f$
\end_inset

 be a decision function.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
In regression, 
\begin_inset Formula $f(x)\in\reals$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
In hard classification, 
\begin_inset Formula $f(x)\in\left\{ -1,1\right\} $
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For generalized regression, 
\begin_inset Formula $f(x)\in?$
\end_inset


\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
\begin_inset Formula $f(x)$
\end_inset

 is a PDF or PMF on 
\begin_inset Formula $\cy$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If 
\begin_inset Formula $p=f(x)$
\end_inset

, can evaluate 
\begin_inset Formula $p(y)$
\end_inset

 for predicted probability of 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Or just write 
\begin_inset Formula $[f(x)](y)$
\end_inset

 or even 
\begin_inset Formula $f(x)(y)$
\end_inset

.
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Deviance Defined
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
We have used likelihood as our performance measure.
\end_layout

\begin_layout Itemize
An alternative measure used commonly in statistics is called 
\series bold
deviance
\series default
.
\end_layout

\begin_layout Itemize
For model fitting, it leads to the same results.
\end_layout

\begin_layout Itemize
As above, for any input 
\begin_inset Formula $x$
\end_inset

 we predict parameter 
\begin_inset Formula $\theta$
\end_inset

 for a parametric family 
\begin_inset Formula $p(y;\theta)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Plain Layout
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
How General A Distribution Can We Use?
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Uniform Example?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/drosen/Dropbox/repos/mlcourse/Figures/conditional-probability-models/uniform.pdf
	height 70pheight%

\end_inset

 
\end_layout

\begin_layout Itemize
Can't use it in GBM: likelihood not differentiable (not continuous)
\end_layout

\begin_layout Itemize
Motivates using 
\begin_inset Quotes eld
\end_inset

exponential family
\begin_inset Quotes erd
\end_inset

 distributions (a bit about this next time)
\end_layout

\end_deeper
\end_inset


\end_layout

\end_body
\end_document
