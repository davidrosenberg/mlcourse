#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin /Users/drosen/Dropbox/repos/mlcourse/Lectures/
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty


\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\end_preamble
\options handout,aspectratio=169
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\dom}{\textrm{\textbf{dom} }}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\rank}{\text{\textbf{rank }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\conv}{\textrm{\textbf{conv} }}
\end_inset


\begin_inset FormulaMacro
\newcommand{\relint}{\text{\textbf{relint }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\aff}{\text{\textbf{aff }}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Title
Gradient Boosting (Continued)
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003 
\begin_inset Note Note
status open

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David Rosenberg 
\end_layout

\begin_layout Date
April 4, 2016
\end_layout

\begin_layout Institute
New York University
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Just in article version
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Plots courtesy of Ningshan Zhang.}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Boosting Fits an Additive Model 
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Adaptive Basis Function Model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
AdaBoost produces a classification score function of the form 
\begin_inset Formula 
\[
\sum_{m=1}^{M}\alpha_{m}G_{m}(x)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
each 
\begin_inset Formula $G_{m}$
\end_inset

 is a 
\series bold
base classifier
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
The 
\begin_inset Formula $G_{m}$
\end_inset

's are like basis functions, but they are learned from the data.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let's move beyond classification models...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Adaptive Basis Function Model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Base hypothesis space
\series default
 
\begin_inset Formula $\ch$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
An 
\series bold
adaptive basis function expansion 
\series default
over 
\begin_inset Formula $\ch$
\end_inset

 is
\begin_inset Formula 
\[
f(x)=\sum_{m=1}^{M}\nu_{m}h_{m}(x),
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{m}\in\ch$
\end_inset

 chosen in a learning process (
\begin_inset Quotes eld
\end_inset

adaptive
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Itemize
\begin_inset Formula $\nu_{m}\in\reals$
\end_inset

 are 
\series bold
expansion coefficients.

\series default
 
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Note: 
\series default
We are taking linear combination of outputs of 
\begin_inset Formula $h_{m}(x)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Functions in 
\begin_inset Formula $h_{m}\in\ch$
\end_inset

 must produce values in 
\begin_inset Formula $\reals$
\end_inset

 (or a vector space) 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
How to fit an adaptive basis function model?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Loss function
\series default
: 
\begin_inset Formula $\ell(y,\hat{y})$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Base hypothesis space
\series default
: 
\begin_inset Formula $\ch$
\end_inset

 of 
\series bold
real-valued
\series default
 functions
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The combined hypothesis space is then
\begin_inset Formula 
\[
\cf_{M}=\left\{ f(x)=\sum_{m=1}^{M}\nu_{m}h_{m}(x)\right\} .
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Find 
\begin_inset Formula $f\in\cf_{M}$
\end_inset

 that minimizes some objective function 
\begin_inset Formula $J(f)$
\end_inset

.
 e.g.
 penalized ERM: 
\begin_inset Formula 
\[
\hat{f}=\argmin_{f\in\cf_{M}}\left[\frac{1}{n}\sum_{i=1}^{n}\ell\left(y_{i},f(x_{i})\right)+\Omega(f)\right].
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
To fit this, we'll proceed in stages, adding a new 
\begin_inset Formula $h_{m}$
\end_inset

 in every stage.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Forward Stagewise Additive Modeling (FSAM)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Start with 
\begin_inset Formula $f_{0}\equiv0$
\end_inset

.
\end_layout

\begin_layout Itemize
After 
\begin_inset Formula $m-1$
\end_inset

 stages, we have
\begin_inset Formula 
\[
f_{m-1}=\sum_{i=1}^{m-1}\nu_{i}h_{i},
\]

\end_inset

where 
\begin_inset Formula $h_{1},\ldots,h_{m-1}\in\ch$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Want to find 
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
step direction
\series default
 
\begin_inset Formula $h_{m}\in\ch$
\end_inset

 and
\end_layout

\begin_layout Itemize

\series bold
step size 
\series default

\begin_inset Formula $\nu_{i}>0$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
so that 
\begin_inset Formula 
\[
f_{m}=f_{m-1}+\nu_{i}h_{m}
\]

\end_inset

improves objective function value by as much as possible.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Forward Stagewise Additive Modeling for ERM
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Initialize 
\begin_inset Formula $f_{0}(x)=0$
\end_inset

.
\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $m=1$
\end_inset

 to 
\begin_inset Formula $M$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Enumerate
Compute:
\begin_inset Formula 
\[
\left(\nu_{m},h_{m}\right)=\argmin_{\nu\in\reals,h\in\ch}\sum_{i=1}^{n}\ell\left(y_{i},f_{m-1}(x_{i})\underbrace{+\nu h(x_{i})}_{\text{new piece}}\right).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Set 
\begin_inset Formula $f_{m}=f_{m-1}+\nu_{m}h$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
Return: 
\begin_inset Formula $f_{M}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Exponential Loss and AdaBoost
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Take loss function to be 
\begin_inset Formula 
\[
\ell(y,f(x))=\exp\left(-yf(x)\right).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\ch$
\end_inset

 be our base hypothesis space of classifiers 
\begin_inset Formula $h:\cx\to\left\{ -1,1\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then Forward Stagewise Additive Modeling (FSAM) reduces to AdaBoost!
\end_layout

\begin_deeper
\begin_layout Itemize
Proof on Homework #6 (and see HTF Section 10.4).
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Only difference:
\end_layout

\begin_deeper
\begin_layout Itemize
AdaBoost gets whichever 
\begin_inset Formula $G_{m}$
\end_inset

 the base learner returns from 
\begin_inset Formula $\ch$
\end_inset

 – no guarantees it's best in 
\begin_inset Formula $\ch$
\end_inset

.
\end_layout

\begin_layout Itemize
FSAM explicitly requires getting the best in 
\begin_inset Formula $\ch$
\end_inset


\begin_inset Formula 
\[
G_{m}=\argmin_{G\in\ch}\sum_{i=1}^{N}w_{i}^{(m)}\ind{y_{i}\neq G(x_{i})}
\]

\end_inset

 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Gradient Boosting / 
\begin_inset Quotes eld
\end_inset

Anyboost
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
FSAM Looks Like Iterative Optimization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The FSAM step
\begin_inset Formula 
\[
\left(\nu_{m},h_{m}\right)=\argmin_{\nu\in\reals,h\in\ch}\sum_{i=1}^{n}\ell\left(y_{i},f_{m-1}(x_{i})\underbrace{+\nu h(x_{i})}_{\text{new piece}}\right).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Hard part: finding the 
\series bold
best
\series default
 
\series bold
step direction
\series default
 
\begin_inset Formula $h$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What if we looked for the 
\series bold
locally best 
\series default
step direction?
\end_layout

\begin_deeper
\begin_layout Itemize
like in gradient descent
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
Approach:
\end_layout

\begin_deeper
\begin_layout Itemize
Choose 
\begin_inset Formula $h_{m}$
\end_inset

 to be something like a gradient in function space.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Roughly speaking, it will be the functional gradient projected onto 
\begin_inset Formula $\cf$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

Functional
\begin_inset Quotes erd
\end_inset

 Gradient Descent 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
We want to minimize
\begin_inset Formula 
\[
J(f)=\sum_{i=1}^{n}\ell\left(y_{i},f(x_{i})\right).
\]

\end_inset


\end_layout

\begin_layout Itemize
Only depends on 
\begin_inset Formula $f$
\end_inset

 at the 
\begin_inset Formula $n$
\end_inset

 training points.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Define
\begin_inset Formula 
\[
{\bf f}=\left(f(x_{1}),\ldots,f(x_{n})\right)^{T}
\]

\end_inset

and write the objective function as 
\begin_inset Formula 
\[
J({\bf f})=\sum_{i=1}^{n}\ell\left(y_{i,}{\bf f}_{i}\right).
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Functional Gradient Descent: Unconstrained Step Direction
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider gradient descent on 
\begin_inset Formula 
\[
J({\bf f})=\sum_{i=1}^{n}\ell\left(y_{i,}{\bf f}_{i}\right).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The 
\series bold
negative gradient step direction
\series default
 at 
\begin_inset Formula ${\bf f}$
\end_inset

 is
\begin_inset Formula 
\[
-{\bf g}=-\del_{\vf}J({\bf f}),
\]

\end_inset

which we can easily calculate.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Eventually we need more than just 
\begin_inset Formula ${\bf f}$
\end_inset

, which is just predictions on training.
\end_layout

\begin_layout Itemize
We'll need a full function 
\begin_inset Formula $f:\cx\to\reals$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Pause

\end_layout

\begin_layout Itemize
This is just about how to adjust the values of 
\begin_inset Formula $f$
\end_inset

 at the training data.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
How to keep 
\begin_inset Formula $f\in\cf$
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Solve both of these problems by projecting 
\begin_inset Formula $-{\bf g}_{m}$
\end_inset

 into 
\begin_inset Formula $\cf$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Unconstrained Functional Gradient Stepping
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/unconstrained-fnl-gradient-steps-SeniElderFigB.1.png
	lyxscale 50
	height 60theight%

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula $R(\mathbf{f})$
\end_inset

 is the empirical risk, where 
\begin_inset Formula $\mathbf{f}=\left(f(x_{1}),f(x_{2})\right)$
\end_inset

 are predictions on training set.
\begin_inset Newline newline
\end_inset

Issue: 
\begin_inset Formula $\hat{{\bf f}}_{M}$
\end_inset

 only defined at training points.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Seni and Elder's 
\backslash
emph{Ensemble Methods in Data Mining}, Fig B.1.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Functional Gradient Descent: Projection Step
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Unconstrained step direction is
\begin_inset Formula 
\[
-{\bf g}=-\del_{\vf}J({\bf f}).
\]

\end_inset


\end_layout

\begin_layout Itemize
Suppose 
\begin_inset Formula $\ch$
\end_inset

 is our base hypothesis space.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Find 
\begin_inset Formula $h\in\ch$
\end_inset

 that is closest to 
\begin_inset Formula $-{\bf g}$
\end_inset

 at the training points, in the 
\begin_inset Formula $\ell^{2}$
\end_inset

 sense:
\begin_inset Formula 
\[
\min_{h\in\ch}\sum_{i=1}^{n}\left(-{\bf g}_{i}-h(x_{i})\right)^{2}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
This is a least squares regression problem.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\ch$
\end_inset

 should have 
\series bold
real-valued
\series default
 functions.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
So the 
\begin_inset Formula $h$
\end_inset

 that best approximates 
\begin_inset Formula $-{\bf g}$
\end_inset

 is our step direction.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

Projected
\begin_inset Quotes erd
\end_inset

 Functional Gradient Stepping
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename /Users/drosen/Dropbox/repos/mlcourse/Figures/boosting/projected-fnl-grad-SeniElderFigB.2.png
	lyxscale 50
	height 55theight%

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula $T(x;p)\in\ch$
\end_inset

 is our actual step direction – like the projection of 
\begin_inset Formula $\mbox{-{\bf g}=-}\del R(\mathbf{f})$
\end_inset

 onto 
\begin_inset Formula $\ch$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Seni and Elder's 
\backslash
emph{Ensemble Methods in Data Mining}, Fig B.2.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Functional Gradient Descent: Step Size
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Finally, we choose a stepsize.
 
\end_layout

\begin_layout Itemize
Option 1 (Line search – 
\emph on
not sure this is actually used in practice
\emph default
):
\begin_inset Formula 
\[
\nu_{m}=\argmin_{\nu>0}\sum_{i=1}^{n}\ell\left\{ y_{i},f_{m-1}(x_{i})+\nu h_{m}(x_{i})\right\} .
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Option 2: (Shrinkage parameter)
\end_layout

\begin_deeper
\begin_layout Itemize
We consider 
\begin_inset Formula $\nu=1$
\end_inset

 to be the full gradient step.
\end_layout

\begin_layout Itemize
Choose a fixed 
\begin_inset Formula $\nu\in(0,1)$
\end_inset

 – called a 
\series bold
shrinkage parameter.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
A value of 
\begin_inset Formula $\nu=0.1$
\end_inset

 is typical – optimize as a hyperparameter .
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Gradient Boosting Machine Ingredients (Recap)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Take any [sub]differentiable loss function.
\end_layout

\begin_layout Itemize
Choose a base hypothesis space for regression.
\end_layout

\begin_layout Itemize
Choose number of steps (or a stopping criterion).
\end_layout

\begin_layout Itemize
Choose step size methodology.
\end_layout

\begin_layout Itemize
Then you're good to go!
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Gradient Tree Boosting
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Tree Boosting
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Common form of gradient boosting machine takes
\begin_inset Formula 
\[
\ch=\left\{ \mbox{regression trees of size \ensuremath{J}}\right\} ,
\]

\end_inset

where 
\begin_inset Formula $J$
\end_inset

 is the number of terminal nodes.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $J=2$
\end_inset

 gives decision stumps
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
HTF recommends 
\begin_inset Formula $4\le J\le8$
\end_inset

 (but some recent results use much larger trees)
\end_layout

\begin_layout Itemize
Software packages:
\end_layout

\begin_deeper
\begin_layout Itemize
Gradient tree boosting is implemented by the 
\series bold
gbm package
\series default
 for R
\end_layout

\begin_layout Itemize
as 
\family typewriter
\size footnotesize
GradientBoostingClassifier
\family default
\size default
 and 
\family typewriter
\size footnotesize
GradientBoostingRegressor
\family default
\size default
 in 
\series bold
sklearn
\end_layout

\begin_layout Itemize

\series bold
xgboost
\series default
 and 
\series bold
lightGBM
\series default
 are state of the art for speed and performance
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
For trees, there are other tweaks on the algorithm one can do
\end_layout

\begin_deeper
\begin_layout Itemize
See HTF 10.9-10.12 
\end_layout

\end_deeper
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Section
GBM Regression with Stumps
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Sinc Function: Our Dataset
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/sinc-fn-data.png
	lyxscale 70
	height 60theight%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Natekin and Knoll's "Gradient boosting machines, a tutorial"}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Minimizing Square Loss with Ensemble of Decision Stumps
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/sinc-fit-1step10steps.png
	height 30theight%

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/sinc-fit-50steps100steps.png
	height 30theight%

\end_inset

 
\end_layout

\begin_layout Standard
Decision stumps with 
\begin_inset Formula $1,10,50$
\end_inset

, and 
\begin_inset Formula $100$
\end_inset

 steps, step size 
\begin_inset Formula $\lambda=1$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Figure 3 from Natekin and Knoll's "Gradient boosting machines, a tutorial"}
}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Step Size as Regularization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename /Users/drosen/Dropbox/repos/mlcourse/Figures/boosting/sinc-regression-train-validation.png
	lyxscale 35
	width 90text%

\end_inset

 
\end_layout

\begin_layout Standard
Performance vs rounds of boosting and step size.
 (Left is training set, right is validation set)
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Figure 5 from Natekin and Knoll's "Gradient boosting machines, a tutorial"}
}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Rule of Thumb
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The smaller the step size, the more steps you'll need.
\end_layout

\begin_layout Itemize
But never seems to make results worse, and often better.
\end_layout

\begin_layout Itemize
So make your step size as small as you have patience for.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Variations on Gradient Boosting
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Stochastic Gradient Boosting
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For each stage, 
\end_layout

\begin_deeper
\begin_layout Itemize
choose random subset of data for computing projected gradient step.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Typically, about 50% of the dataset size, can be much smaller for large
 training set.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
Fraction is called the 
\series bold
bag fraction
\series default
.
\end_layout

\end_deeper
\begin_layout Itemize
Why do this?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Subsample percentage is additional regularization parameter – may help overfitti
ng.
\end_layout

\begin_layout Itemize
Faster.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
We can view this is a 
\series bold
minibatch method
\series default
.
 
\end_layout

\begin_deeper
\begin_layout Itemize
we're estimating the 
\begin_inset Quotes eld
\end_inset

true
\begin_inset Quotes erd
\end_inset

 step direction (the projected gradient) using a subset of data 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Introduced by Friedman (1999) in 
\backslash
href{http://statweb.stanford.edu/~jhf/ftp/stobst.pdf}{Stochastic Gradient Boosting}.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Some Comments on Bag Size
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Justification for 50% of dataset size:
\end_layout

\begin_deeper
\begin_layout Itemize
In bagging, sampling 50% without replacement gives very similar results
 to full bootstrap sample
\end_layout

\begin_layout Itemize
See Buja and Stuetzle's 
\begin_inset CommandInset href
LatexCommand href
name "Observations on Bagging"
target "http://stat.wharton.upenn.edu/~buja/PAPERS/sinica-bagging-buja-stuetzle.pdf"

\end_inset

.
\end_layout

\begin_layout Itemize
So if we're subsampling because we're inspired by bagging, this makes sense.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
But if we think of stochastic gradient boosting as a minibatch method,
\end_layout

\begin_deeper
\begin_layout Itemize
then makes little sense to choose batch size as a fixed percent of dataset
 size,
\end_layout

\begin_layout Itemize
especially for large datasets.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bag as Minibatch
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Just as we argued for minibatch SGD, 
\end_layout

\begin_deeper
\begin_layout Itemize
sample size needed for a good estimate of step direction is independent
 of training set size
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Minibatch size should depend on 
\end_layout

\begin_deeper
\begin_layout Itemize
the complexity of base hypothesis space
\end_layout

\begin_layout Itemize
the complexity of the target function (Bayes decision function)
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Seems like an interesting area for both practical and theoretical pursuit.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Newton Step Direction
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Second order Taylor expansion of 
\begin_inset Formula $J({\bf f})$
\end_inset

 at 
\begin_inset Formula ${\bf f}_{0}$
\end_inset

:
\begin_inset Formula 
\[
J({\bf f}_{0}+{\bf f})=J({\bf f}_{0})+\left[\del_{{\bf f}}J({\bf f}_{0})\right]^{T}{\bf f}+\frac{1}{2}{\bf f}^{T}\left[\del_{{\bf f}}^{2}J({\bf f}_{0})\right]{\bf f}
\]

\end_inset


\end_layout

\begin_layout Itemize
The loss for predicting 
\begin_inset Formula ${\bf f}_{i}$
\end_inset

 on the 
\begin_inset Formula $i$
\end_inset

th data point is 
\begin_inset Formula $\ell\left(y_{i},{\bf f}_{i}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Denote the derivative of this loss w.r.t.
 
\begin_inset Formula ${\bf f}_{i}$
\end_inset

 is 
\begin_inset Formula $g_{i}=\partial_{{\bf f}_{i}}\ell\left(y_{i},{\bf f}_{i}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
The second derivative w.r.t.
 
\begin_inset Formula ${\bf f}_{i}$
\end_inset

 is 
\begin_inset Formula $h_{i}=\partial_{{\bf f}_{i}}^{2}\ell\left(y_{i},{\bf f}_{i}\right)$
\end_inset

.
 
\end_layout

\begin_layout Itemize
So
\begin_inset Formula 
\begin{eqnarray*}
J({\bf f}) & = & \sum_{i=1}^{n}\ell\left(y_{i,}{\bf f}_{i}\right)\\
\del_{{\bf f}}J({\bf f}) & = & {\bf g}\\
\del_{{\bf f}}^{2}J({\bf f}_{0}) & = & \diag\left(\partial_{{\bf f}_{1}}^{2}\ell\left(y_{1},{\bf f}_{1}\right),\ldots,\partial_{{\bf f}_{n}}^{2}\ell\left(y_{n},{\bf f}_{n}\right)\right)
\end{eqnarray*}

\end_inset

of For GBM, we find the closest 
\begin_inset Formula $h\in\cf$
\end_inset

 to the negative gradient
\begin_inset Formula 
\[
-{\bf g}=-\del_{\vf}J({\bf f}).
\]

\end_inset


\end_layout

\begin_layout Itemize
This is a 
\begin_inset Quotes eld
\end_inset

first order
\begin_inset Quotes erd
\end_inset

 method.
 
\end_layout

\begin_layout Itemize
Newton method is a 
\begin_inset Quotes eld
\end_inset

second order method
\begin_inset Quotes erd
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
Approximate 
\begin_inset Formula $J({\bf f})$
\end_inset

 using 2nd order Taylor expansion (i.e 
\end_layout

\begin_layout Itemize
Step direction is towards the 
\end_layout

\end_deeper
\end_deeper
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Column / Feature Subsampling for Regularization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Similar to random forest, randomly choose a subset of features for each
 round.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
XGBoost paper says: 
\begin_inset Quotes eld
\end_inset

According to user feedback, using column sub-sampling prevents overfitting
 even more so than the traditional row sub-sampling.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Zhao Xing (top Kaggle competitor) finds optimal percentage to be 20%-100%
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Newton Step Direction
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For GBM, we find the closest 
\begin_inset Formula $h\in\cf$
\end_inset

 to the negative gradient
\begin_inset Formula 
\[
-{\bf g}=-\del_{\vf}J({\bf f}).
\]

\end_inset


\end_layout

\begin_layout Itemize
This is a 
\begin_inset Quotes eld
\end_inset

first order
\begin_inset Quotes erd
\end_inset

 method.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Newton's method is a 
\begin_inset Quotes eld
\end_inset

second order method
\begin_inset Quotes erd
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
Find 2nd order (quadratic) approximation to 
\begin_inset Formula $J$
\end_inset

 at 
\begin_inset Formula $\vf$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Requires computing gradient and Hessian of 
\begin_inset Formula $J$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Newton step direction points towards minimizer of the quadratic.
\end_layout

\begin_layout Itemize
Minimizer of quadratic is easy to find in closed form
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Boosting methods with projected Newton step direction:
\end_layout

\begin_deeper
\begin_layout Itemize
LogitBoost (logistic loss function)
\end_layout

\begin_layout Itemize
XGBoost (any loss – uses regression trees for base classifier)
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
XGBoost
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Adds explicit penalty term on tree complexity to the empirical risk:
\begin_inset Formula 
\[
\Omega(h)=\gamma T+\frac{1}{2}\lambda\sum_{i=1}^{T}w_{j}^{2},
\]

\end_inset

where 
\begin_inset Formula $h\in\ch$
\end_inset

 is a regression tree from our base hypothesis space and
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $T$
\end_inset

 is the number of leaf nodes and
\end_layout

\begin_layout Itemize
\begin_inset Formula $w_{j}$
\end_inset

 is the prediction in the 
\begin_inset Formula $j$
\end_inset

'th node
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Objective function at step 
\begin_inset Formula $m$
\end_inset

:
\begin_inset Formula 
\[
J(h)=\text{2nd Order Approximation to empirical risk}(h)+\Omega(h)
\]

\end_inset


\end_layout

\begin_layout Itemize
In XGBoost, they also use this objective to decide on tree splits
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
See 
\begin_inset CommandInset href
LatexCommand href
name "XGBoost Introduction"
target "http://xgboost.readthedocs.io/en/latest/model.html"

\end_inset

 for a nice introduction.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Section
Questions
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Hypothesis Space for AdaBoost
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Base hypothesis space: 
\begin_inset Formula $\ch$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consists of hard-classification functions
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $h\in\ch$
\end_inset

, we have 
\begin_inset Formula $h:\cx\to\left\{ -1,1\right\} $
\end_inset

\SpecialChar endofsentence

\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The hypothesis for AdaBoost after 
\begin_inset Formula $T$
\end_inset

 rounds is
\begin_inset Formula 
\[
\cc_{T}=\left\{ h(x)=\sign\left(\sum_{i=1}^{T}\alpha_{i}h_{i}(x)\right)\mid\alpha_{i}\in\reals,h_{i}\in\ch\,\forall i=1,\ldots,T\right\} .
\]

\end_inset

 
\end_layout

\end_deeper
\end_inset


\begin_inset Note Note
status open

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Questions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Plain Layout
1) expressivity / complexity of boosting decision stumps
\end_layout

\begin_layout Plain Layout
2) generalization bounds in terms of complexity of base hypothesis space
 
\end_layout

\begin_layout Plain Layout
5) Population minimizer of exponential loss 
\end_layout

\begin_layout Plain Layout
9) regularization by subsampling (of rows) 
\begin_inset Quotes eld
\end_inset

bag fraction
\begin_inset Quotes erd
\end_inset

 ; frequent default is 50%...
 but what if we have tons of data?[what happens in the limit of a single
 or very few data points?] – possible issue is that we mess up predictions
 at other points...
 [homework problem???]
\end_layout

\begin_layout Plain Layout
12) What's the hypothesis space for boosting methods? [universal?]
\end_layout

\end_deeper
\end_inset


\end_layout

\end_body
\end_document
