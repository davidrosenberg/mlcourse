#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin /Users/drosen/Dropbox/repos/mlcourse/Lectures/
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\end_preamble
\options handout,aspectratio=169
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Title
EM Algorithm for Latent Variable Models
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003 
\begin_inset Note Note
status open

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David Rosenberg 
\end_layout

\begin_layout Date
May 2, 2017
\end_layout

\begin_layout Institute
New York University
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Just in article version
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Gaussian Mixture Models (Review)
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Example: Old Faithful Geyser
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/clustering/oldfaithful.png
	lyxscale 20
	width 65text%

\end_inset


\end_layout

\begin_layout Itemize
Looks like two clusters.
\end_layout

\begin_layout Itemize
How to find these clusters algorithmically?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Probabilistic Model for Clustering
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Let's consider a 
\series bold
generative model
\series default
 for the data.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Suppose 
\end_layout

\begin_deeper
\begin_layout Enumerate
There are 
\begin_inset Formula $k$
\end_inset

 clusters.
\end_layout

\begin_layout Enumerate
We have a probability density for each cluster.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Generate a point 
\begin_inset Formula $x$
\end_inset

 as follows
\end_layout

\begin_deeper
\begin_layout Enumerate
Choose a random cluster 
\begin_inset Formula $z\in\left\{ 1,2,\ldots,k\right\} $
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Choose a point 
\begin_inset Formula $x$
\end_inset

 from the distribution for cluster 
\begin_inset Formula $z$
\end_inset

.
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gaussian Mixture Model (
\begin_inset Formula $k=3$
\end_inset

)
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Enumerate
Choose 
\begin_inset Formula $z\in\left\{ 1,2,3\right\} $
\end_inset

 with 
\begin_inset Formula $p(1)=p(2)=p(3)=\frac{1}{3}$
\end_inset

.
 
\end_layout

\begin_layout Enumerate
Choose 
\begin_inset Formula $x\mid z\sim\cn\left(X\mid\mu_{z},\Sigma_{z}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/clustering/mixture-3-gaussians.png
	lyxscale 30
	height 60theight%

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gaussian Mixture Model Parameters (
\begin_inset Formula $k$
\end_inset

 Components)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Formula 
\begin{eqnarray*}
\text{Cluster probabilities}: &  & \pi=\left(\pi_{1},\ldots,\pi_{k}\right)\\
\text{Cluster means}: &  & \mu=\left(\mu_{1},\ldots,\mu_{k}\right)\\
\text{Cluster covariance matrices:} &  & \Sigma=\left(\Sigma_{1},\ldots\Sigma_{k}\right)
\end{eqnarray*}

\end_inset


\begin_inset Graphics
	filename ../Figures/clustering/mixture-3-gaussians.png
	lyxscale 20
	height 35theight%

\end_inset

 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Standard
For now, 
\series bold
suppose all these parameters are known
\series default
.
 
\begin_inset Newline newline
\end_inset

We'll discuss how to 
\series bold
learn
\series default
 or 
\series bold
estimate 
\series default
them later.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The GMM 
\begin_inset Quotes eld
\end_inset

Inference
\begin_inset Quotes erd
\end_inset

 Problem
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize

\emph on
Suppose
\emph default
 we know all the model parameters 
\begin_inset Formula $\pi,\mu,\Sigma$
\end_inset

, and thus 
\begin_inset Formula $p(x,z)$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The 
\series bold
inference problem
\series default
: We observe 
\begin_inset Formula $x$
\end_inset

.
 We want to know its cluster 
\begin_inset Formula $z$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We can get a
\series bold
 soft cluster assignment 
\series default
from the conditional distribution:
\begin_inset Formula 
\[
p(z\mid x)=p(x,z)/p(x)
\]

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Could say: can't compute this because 
\begin_inset Formula $p(x)$
\end_inset

 is hard.
 But we don't actually need 
\begin_inset Formula $p(x)$
\end_inset

.
 Since 
\begin_inset Formula $p(z\mid x)\propto p(x,z)$
\end_inset

, which is easy.
 And we can just renormalize over 
\begin_inset Formula $z$
\end_inset

, which is a simple sum for discrete 
\begin_inset Formula $z$
\end_inset

.
 Moreover, in this case, calculating 
\begin_inset Formula $p(x)$
\end_inset

 is NOT hard, because the latent variable space is very small.
 (Just the number of clusters.) It IS hard in situations where we have a
 high dimensional latent space and we have exponentially many possible assignmen
ts of 
\begin_inset Formula $z$
\end_inset

 we need to sum over.
\end_layout

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
A 
\series bold
hard cluster assignment 
\series default
is given by
\begin_inset Formula 
\[
z^{*}=\argmax_{z\in\left\{ 1,\ldots,k\right\} }p(z\mid x).
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
So if we know the model parameters, we can compute 
\begin_inset Formula $p(z\mid x)$
\end_inset

, and clustering is trival.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The GMM 
\begin_inset Quotes eld
\end_inset

Learning
\begin_inset Quotes erd
\end_inset

 Problem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Given data 
\begin_inset Formula $x_{1},\ldots,x_{n}$
\end_inset

 drawn i.i.d.
 from a GMM,
\end_layout

\begin_layout Itemize
Estimate the parameters: 
\begin_inset Formula 
\begin{eqnarray*}
\text{Cluster probabilities}: &  & \pi=\left(\pi_{1},\ldots,\pi_{k}\right)\\
\text{Cluster means}: &  & \mu=\left(\mu_{1},\ldots,\mu_{k}\right)\\
\text{Cluster covariance matrices:} &  & \Sigma=\left(\Sigma_{1},\ldots\Sigma_{k}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Traditional approach is maximum [marginal] likelihood:
\begin_inset Formula 
\[
\left(\pi,\mu,\Sigma\right)=\argmax_{\pi,\mu,\Sigma}p(x_{1},\ldots,x_{n})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Unfortunately, this is very difficult.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Note that the fully observed problem is easy.
 That is:
\begin_inset Formula 
\[
\left(\pi,\mu,\Sigma\right)=\argmax_{\pi,\mu,\Sigma}p(x_{1},\ldots,x_{n},z_{1},\ldots,z_{n})
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gaussian Mixture Model: Joint Distribution 
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout ColumnsTopAligned

\end_layout

\begin_deeper
\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

.25
\backslash
textwidth
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Figures/clustering/gmm-bayesnet.png
	lyxscale 30
	width 40col%

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

.75
\backslash
textwidth
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Factorize the joint distribution:
\begin_inset Formula 
\begin{eqnarray*}
p(x,z) & = & p(z)p(x\mid z)\\
\pause & = & \pi_{z}\cn\left(x\mid\mu_{z},\Sigma_{z}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
\begin_inset Formula $\pi_{z}$
\end_inset

 is probability of choosing cluster 
\begin_inset Formula $z$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $x\mid z$
\end_inset

 has distribution 
\begin_inset Formula $\cn(\mu_{z},\Sigma_{z})$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $z$
\end_inset

 corresponding to 
\begin_inset Formula $x$
\end_inset

 is the true cluster assignment.
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Suppose we know all the model parameters 
\begin_inset Formula $\pi_{z},\mu_{z},\Sigma_{z}$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Then we can easily compute the joint 
\begin_inset Formula $p(x,z)$
\end_inset

 and the conditional 
\begin_inset Formula $p(z\mid x)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Plain Layout
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Latent Variable Model 
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
We observe 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Itemize
We don't observe 
\begin_inset Formula $z$
\end_inset

.
 (Cluster assignment).
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Cluster assignment 
\begin_inset Formula $z$
\end_inset

 is called a 
\series bold
hidden
\series default
 
\series bold
variable
\series default
 or 
\series bold
latent variable
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Definition
A
\series bold
 latent variable model 
\series default
is a probability model for which certain variables are never observed.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Plain Layout
e.g.
 The Gaussian mixture model is a latent variable model.
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Section
EM Algorithm for Latent Variable Models 
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
General Latent Variable Model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Two sets of random variables: 
\begin_inset Formula $z$
\end_inset

 and 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $z$
\end_inset

 consists of unobserved 
\series bold
hidden variables
\series default
.
\end_layout

\begin_layout Itemize
\begin_inset Formula $x$
\end_inset

 consists of 
\series bold
observed variables
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Joint probability model parameterized by 
\begin_inset Formula $\theta\in\Theta$
\end_inset

:
\begin_inset Formula 
\[
p(x,z\mid\theta)
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Definition
A
\series bold
 latent variable model 
\series default
is a probability model for which certain variables are never observed.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
e.g.
 The Gaussian mixture model is a latent variable model.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Complete and Incomplete Data
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have a data set 
\begin_inset Formula $\cd=\left(x_{1},\ldots,x_{n}\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
To simplify notation, take 
\begin_inset Formula $x$
\end_inset

 to represent the entire dataset
\begin_inset Formula 
\[
x=\left(x_{1},\ldots,x_{n}\right),
\]

\end_inset

and 
\begin_inset Formula $z$
\end_inset

 to represent the corresponding unobserved variables
\begin_inset Formula 
\[
z=\left(z_{1},\ldots,z_{n}\right).
\]

\end_inset


\end_layout

\begin_layout Itemize
An observation of 
\begin_inset Formula $x$
\end_inset

 is called an 
\series bold
incomplete data set
\series default
.
\end_layout

\begin_layout Itemize
An observation 
\begin_inset Formula $\left(x,z\right)$
\end_inset

 is called a 
\series bold
complete data set
\series default
.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Log-Likelihood and Terminology
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Note that
\begin_inset Formula 
\[
\argmax_{\theta}p(x\mid\theta)=\argmax_{\theta}\left[\log p(x\mid\theta)\right].
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Often easier to work with this 
\begin_inset Quotes eld
\end_inset

l
\series bold
og-likelihood
\series default

\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We often call 
\begin_inset Formula $p(x)$
\end_inset

 the 
\series bold
marginal likelihood
\series default
, 
\end_layout

\begin_deeper
\begin_layout Itemize
because it is 
\begin_inset Formula $p(x,z)$
\end_inset

 with 
\begin_inset Formula $z$
\end_inset

 
\begin_inset Quotes eld
\end_inset

marginalized out
\begin_inset Quotes erd
\end_inset

:
\begin_inset Formula 
\[
p(x)=\sum_{z}p(x,z)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
We often call 
\begin_inset Formula $p(x,y)$
\end_inset

 the 
\series bold
joint
\series default
.
 (for 
\begin_inset Quotes eld
\end_inset

joint distribution
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Similarly, 
\begin_inset Formula $\log p(x)$
\end_inset

 is the 
\series bold
marginal log-likelihood
\series default
.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Assumptions for EM Algorithm
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Assumption for the EM algorithm
\series default
:
\end_layout

\begin_deeper
\begin_layout Itemize
Optimization for complete data is relatively easy
\begin_inset Formula 
\[
\argmax_{\theta\in\Theta}\;\log p(x,z\mid\theta)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Clearly true for Gaussian mixture model.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Plain Layout
[We'll actually need slightly more than this....]
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\end_inset

 
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Our Objectives
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Learning problem
\series default
: Given incomplete dataset 
\begin_inset Formula $\cd=x=\left(x_{1},\ldots,x_{n}\right)$
\end_inset

, find MLE
\begin_inset Formula 
\[
\hat{\theta}=\argmax_{\theta}p(\cd\mid\theta).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Inference problem
\series default
: Given 
\begin_inset Formula $x$
\end_inset

, find conditional distribution over 
\begin_inset Formula $z$
\end_inset

:
\begin_inset Formula 
\[
p\left(z_{i}\mid x_{i},\theta\right).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For Gaussian mixture model, learning is hard, inference is easy.
\end_layout

\begin_layout Itemize
For more complicated models, inference can also be hard.
 (See DSGA-1005)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Log-Likelihood and Terminology
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Note that
\begin_inset Formula 
\[
\argmax_{\theta}p(x\mid\theta)=\argmax_{\theta}\left[\log p(x\mid\theta)\right].
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Often easier to work with this 
\begin_inset Quotes eld
\end_inset


\series bold
log-likelihood
\series default

\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We often call 
\begin_inset Formula $p(x)$
\end_inset

 the 
\series bold
marginal likelihood
\series default
, 
\end_layout

\begin_deeper
\begin_layout Itemize
because it is 
\begin_inset Formula $p(x,z)$
\end_inset

 with 
\begin_inset Formula $z$
\end_inset

 
\begin_inset Quotes eld
\end_inset

marginalized out
\begin_inset Quotes erd
\end_inset

:
\begin_inset Formula 
\[
p(x)=\sum_{z}p(x,z)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
We often call 
\begin_inset Formula $p(x,y)$
\end_inset

 the 
\series bold
joint
\series default
.
 (for 
\begin_inset Quotes eld
\end_inset

joint distribution
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Similarly, 
\begin_inset Formula $\log p(x)$
\end_inset

 is the 
\series bold
marginal log-likelihood
\series default
.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The EM Algorithm 
\series bold
Key Idea
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Marginal log-likelihood is hard to optimize:
\begin_inset Formula 
\[
\max_{\theta}\;\log p(x\mid\theta)
\]

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Formula 
\[
\max_{\theta}\;\log\left\{ \sum_{z}p(x,z\mid\theta)\right\} 
\]

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Typically 
\series default
the complete data log-likelihood is easy to optimize:
\begin_inset Formula 
\[
\max_{\theta}\;\log p(x,z\mid\theta)
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
What if we had a 
\series bold
distribution 
\series default

\begin_inset Formula $q(z)$
\end_inset

 for the latent variables 
\begin_inset Formula $z$
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then maximize the 
\series bold
expected complete data log-likelihood
\series default
:
\begin_inset Formula 
\[
\max_{\theta}\:\sum_{z}q(z)\log p(x,z\mid\theta)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
EM 
\series bold
assumes
\series default
 this maximization is relatively easy.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Lower Bound for Marginal Log-Likelihood
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $q(z)$
\end_inset

 be any PMF on 
\begin_inset Formula $\cz$
\end_inset

, the support of 
\begin_inset Formula $z$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\log p(x\mid\theta) & = & \log\left[\sum_{z}p(x,z\mid\theta)\right]\\
\pause & = & \log\left[\sum_{z}q(z)\left(\frac{p(x,z\mid\theta)}{q(z)}\right)\right]\mbox{\quad\ (log of an expectation)}\\
\pause & \ge & \underbrace{\sum_{z}q(z)\log\left(\frac{p(x,z\mid\theta)}{q(z)}\right)}_{\cl(q,\theta)}\mbox{\quad(expectation of log)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Inequality is by Jensen's, by concavity of the log.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
This inequality is the basis
\series bold
 
\series default
for
\series bold
 
\begin_inset Quotes eld
\end_inset

variational methods
\begin_inset Quotes erd
\end_inset


\series default
, of which EM is a basic example.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The ELBO
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For any PMF 
\begin_inset Formula $q(z)$
\end_inset

, we have a lower bound on the marginal log-likelihood 
\begin_inset Formula 
\[
\log p(x\mid\theta)\ge\underbrace{\sum_{z}q(z)\log\left(\frac{p(x,z\mid\theta)}{q(z)}\right)}_{\cl(q,\theta)}
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Marginal log likelihood 
\begin_inset Formula $\log p(x\mid\theta)$
\end_inset

 also called the 
\series bold
evidence
\series default
.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
\begin_inset Formula $\cl(q,\theta)$
\end_inset

 is the 
\series bold
evidence lower bound
\series default
, or 
\begin_inset Quotes eld
\end_inset


\series bold
ELBO
\begin_inset Quotes erd
\end_inset

.
\series default

\begin_inset Note Note
status collapsed

\begin_layout Itemize
[NOT SURE if this is standard terminology outside of setting when we're
 doing variational inference in Bayesian setting,...
 but giving it anyway
\end_layout

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Standard
In EM algorithm (and variational methods more generally), we maximize 
\begin_inset Formula $\cl(q,\theta)$
\end_inset

 over 
\begin_inset Formula $q$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
MLE, EM, and the ELBO
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For any PMF 
\begin_inset Formula $q(z)$
\end_inset

, we have a lower bound on the marginal log-likelihood 
\begin_inset Formula 
\[
\log p(x\mid\theta)\ge\cl(q,\theta).
\]

\end_inset


\end_layout

\begin_layout Itemize
The MLE is defined as a maximum over 
\begin_inset Formula $\theta$
\end_inset

:
\begin_inset Formula 
\[
\hat{\theta}_{\text{MLE}}=\argmax_{\theta}\log p(x\mid\theta).
\]

\end_inset


\end_layout

\begin_layout Itemize
In EM algorithm, we maximize the lower bound (ELBO) over 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

:
\begin_inset Formula 
\[
\hat{\theta}_{\text{EM}}=\argmax_{\theta}\left[\max_{q}\cl(q,\theta)\right]
\]

\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
A Family of Lower Bounds
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For each 
\begin_inset Formula $q$
\end_inset

, we get a lower bound function: 
\begin_inset Formula $\log p(x\mid\theta)\ge\cl(q,\theta)\;\forall\theta.$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Two lower bounds (blue and green curves), 
\series bold
as functions of 
\series default

\begin_inset Formula $\theta$
\end_inset

:
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/EM-algorithm/lowerBounds-Bishop9.14.png
	lyxscale 50
	height 50theight%

\end_inset

 
\end_layout

\begin_layout Itemize
Ideally, we'd find the maximum of the red curve.
 Maximum of green is close.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Bishop's 
\backslash
emph{Pattern recognition and machine learning}, Figure 9.14.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EM: Coordinate Ascent on Lower Bound
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Choose sequence of 
\begin_inset Formula $q$
\end_inset

's and 
\begin_inset Formula $\theta$
\end_inset

's by 
\begin_inset Quotes eld
\end_inset


\series bold
coordinate ascent
\series default

\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
EM Algorithm (high level):
\end_layout

\begin_deeper
\begin_layout Enumerate
Choose initial 
\begin_inset Formula $\theta^{\text{old}}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $q^{*}=\argmax_{q}\cl(q,\theta^{\text{old}})$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Let 
\begin_inset Formula $\theta^{\text{new}}=\argmax_{\theta}\cl(q^{*},\theta^{\text{old}})$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Go to step 2, until converged.
\end_layout

\end_deeper
\begin_layout Itemize
Will show:
\series bold
 
\begin_inset Formula $p(x\mid\theta^{\text{new}})\ge p(x\mid\theta^{\text{old}})$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Get sequence of 
\begin_inset Formula $\theta$
\end_inset

's with monotonically increasing likelihood.
\series default

\begin_inset Note Note
status open

\begin_layout Itemize
Why is this a good idea?
\end_layout

\begin_deeper
\begin_layout Itemize
In many situations, relatively easier to find 
\begin_inset Formula $q^{\text{new}}$
\end_inset

 and 
\begin_inset Formula $\theta^{\text{new}}$
\end_inset

.
\end_layout

\begin_layout Itemize
e.g.
 GMM
\end_layout

\end_deeper
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EM: Coordinate Ascent on Lower Bound
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/EM-algorithm/EM-twosteps-Bishop9.14.png
	lyxscale 50
	height 50theight%

\end_inset

 
\end_layout

\begin_layout Enumerate
Start at 
\begin_inset Formula $\theta^{\text{old}}.$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Find 
\begin_inset Formula $q$
\end_inset

 giving best lower bound at 
\begin_inset Formula $\theta^{\text{old}}$
\end_inset


\begin_inset Formula $\implies$
\end_inset

 
\begin_inset Formula $\cl(q,\theta)$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset Formula $\theta^{\text{new}}=\argmax_{\theta}\cl(q,\theta)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Bishop's 
\backslash
emph{Pattern recognition and machine learning}, Figure 9.14.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EM: Next Steps
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
We now give 2 different re-expressions of 
\begin_inset Formula $\cl(q,\theta)$
\end_inset

 that make it easy to compute
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\argmax_{q}\cl(q,\theta)$
\end_inset

, for a given 
\begin_inset Formula $\theta$
\end_inset

, and
\end_layout

\begin_layout Itemize
\begin_inset Formula $\argmax_{\theta}\cl(q,\theta)$
\end_inset

, for a given 
\begin_inset Formula $q$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
ELBO in Terms of KL Divergence and Entropy
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let's investigate the lower bound:
\begin_inset Formula 
\begin{eqnarray*}
\cl(q,\theta) & = & \sum_{z}q(z)\log\left(\frac{p(x,z\mid\theta)}{q(z)}\right)\\
\pause & = & \sum_{z}q(z)\log\left(\frac{p(z\mid x,\theta)p(x\mid\theta)}{q(z)}\right)\\
\pause & = & \sum_{z}q(z)\log\left(\frac{p(z\mid x,\theta)}{q(z)}\right)+\sum_{z}q(z)\log p(x\mid\theta)\\
\pause & = & -\kl[q(z),p(z\mid x,\theta)]+\log p(x\mid\theta)\pause
\end{eqnarray*}

\end_inset

 
\end_layout

\begin_layout Itemize
Amazing! We get back an equality for the marginal likelihood:
\begin_inset Formula 
\[
\log p(x\mid\theta)=\cl(q,\theta)+\kl[q(z),p(z\mid x,\theta)]
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Maxizing over 
\begin_inset Formula $q$
\end_inset

 for fixed 
\begin_inset Formula $\theta=\theta^{\text{old}}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Find 
\begin_inset Formula $q$
\end_inset

 maximizing
\begin_inset Formula 
\begin{eqnarray*}
\cl(q,\theta^{\text{old}}) & = & -\kl[q(z),p(z\mid x,\theta^{\text{old}})]+\underbrace{\log p(x\mid\theta^{\text{old}})\pause}_{\text{no }q\text{ here}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Recall 
\begin_inset Formula $\kl(p\|q)\ge0$
\end_inset

, and 
\begin_inset Formula $\kl(p\|p)=0$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Best 
\begin_inset Formula $q$
\end_inset

 is 
\begin_inset Formula $q^{*}(z)=p(z\mid x,\theta^{\text{old}})$
\end_inset

 and
\begin_inset Formula 
\[
\pause\cl(q^{*},\theta^{\text{old}})=-\underbrace{\kl[p(z\mid x,\theta^{\text{old}}),p(z\mid x,\theta^{\text{old}})]}_{=0}+\log p(x\mid\theta^{\text{old}})
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Summary:
\begin_inset Formula 
\begin{eqnarray*}
\log p(x\mid\theta^{\text{old}}) & = & \cl(q^{*},\theta^{\text{old}})\quad(\text{tangent at }\theta^{\text{old}}).\\
\pause\log p(x\mid\theta) & \ge & \cl(q^{*},\theta)\quad\forall\theta
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Tight lower bound for any chosen 
\begin_inset Formula $\theta$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/EM-algorithm/EM-twosteps-Bishop9.14.png
	lyxscale 50
	height 50theight%

\end_inset

 
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $\theta^{\text{old}}$
\end_inset

, take 
\begin_inset Formula $q(z)=p(z\mid x,\theta^{\text{old}})$
\end_inset

.
 Then
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\log p(x\mid\theta)\ge\cl(q,\theta)$
\end_inset

 
\begin_inset Formula $\forall\theta$
\end_inset

.
 [Global lower bound].
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\log p(x\mid\theta^{\text{old}})=\cl(q,\theta^{\text{old}})$
\end_inset

.
 [Lower bound is 
\series bold
tight 
\series default
at 
\series bold

\begin_inset Formula $\theta^{\text{old}}$
\end_inset


\series default
.]
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Bishop's 
\backslash
emph{Pattern recognition and machine learning}, Figure 9.14.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Maximizing over 
\begin_inset Formula $\theta$
\end_inset

 for fixed 
\begin_inset Formula $q$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider maximizing the lower bound 
\begin_inset Formula $\cl(q,\theta)$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\pause\cl(q,\theta) & = & \sum_{z}q(z)\log\left(\frac{p(x,z\mid\theta)}{q(z)}\right)\\
\pause & = & \underbrace{\sum_{z}q(z)\log p(x,z\mid\theta)}_{\ex\left[\text{complete data log-likelihood}\right]}-\underbrace{\sum_{z}q(z)\log q(z)}_{\text{no }\theta\text{ here}}\pause
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Maximizing 
\begin_inset Formula $\cl(q,\theta)$
\end_inset

 equivalent to maximizing 
\begin_inset Formula $\ex\left[\text{complete data log-likelihood}\right]$
\end_inset

 (for fixed 
\begin_inset Formula $q$
\end_inset

).
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
General EM Algorithm
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Choose initial 
\begin_inset Formula $\theta^{\text{old}}$
\end_inset

.
\end_layout

\begin_layout Enumerate

\series bold
Expectation Step
\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $q^{*}(z)=p(z\mid x,\theta^{\text{old}})$
\end_inset

.
 [
\begin_inset Formula $q^{*}$
\end_inset

 gives best lower bound at 
\begin_inset Formula $\theta^{\text{old}}$
\end_inset

]
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let
\begin_inset Formula 
\[
J(\theta):=\cl(q^{*},\theta)=\underbrace{\sum_{z}q^{*}(z)\log\left(\frac{p(x,z\mid\theta)}{q^{*}(z)}\right)}_{\mbox{\textbf{expectation }w.r.t. }z\sim q^{*}(z)}
\]

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate

\series bold
Maximization Step
\series default
 
\begin_inset Formula 
\[
\theta^{\text{new}}=\argmax_{\theta}J(\theta).\pause
\]

\end_inset

[Equivalent to maximizing expected complete log-likelihood.]
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Go to step 2, until converged.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EM and Variational Methods
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
What makes EM a 
\series bold
variational method
\series default
?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We're maximizing a lower bound that is a 
\series bold
functional 
\series default
of distribution 
\begin_inset Formula $q$
\end_inset

.
\end_layout

\begin_layout Itemize
Calculus of variations is the traditional tool.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then 
\begin_inset Formula $J(\theta)$
\end_inset

 is our 
\series bold
variational lower bound 
\series default
(or approximation) to 
\begin_inset Formula $\log p(x\mid\theta)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\end_inset


\end_layout

\begin_layout Section
Does EM Work?
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EM Gives Monotonically Increasing Likelihood: By Picture
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/EM-algorithm/EM-twosteps-Bishop9.14.png
	lyxscale 50
	height 55theight%

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Bishop's 
\backslash
emph{Pattern recognition and machine learning}, Figure 9.14.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EM Gives Monotonically Increasing Likelihood: By Math
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Start at 
\begin_inset Formula $\theta^{\text{old}}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Choose 
\begin_inset Formula $q^{*}(z)=\argmax_{q}\cl(q,\theta^{\text{old}})$
\end_inset

.
 We've shown
\begin_inset Formula 
\[
\log p(x\mid\theta^{\text{old}})=\cl(q^{*},\theta^{\text{old}})
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Choose 
\begin_inset Formula $\theta^{\text{new}}=\argmax_{\theta}\cl(q^{*},\theta^{\text{old}})$
\end_inset

.
 So
\begin_inset Formula 
\begin{eqnarray*}
\cl(q^{*},\theta^{\text{new}}) & \ge & \cl(q^{*},\theta^{\text{old}}).
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
Putting it together, we get
\begin_inset Formula 
\begin{eqnarray*}
\log p(x\mid\theta^{\text{new}}) & \ge & \cl(q^{*},\theta^{\text{new}})\qquad\cl\text{ is a lower bound}\\
\pause & \ge & \cl(q^{*},\theta^{\text{old}})\qquad\mbox{By definition of }\theta^{\text{new}}\\
\pause & = & \log p(x\mid\theta^{\text{old}})\qquad\text{Bound is tight at }\theta^{\text{old}}.
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Suppose We Maximize the ELBO...
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have found a 
\series bold
global maximum
\series default
 of 
\begin_inset Formula $\cl(q,\theta)$
\end_inset

:
\begin_inset Formula 
\[
L(q^{*},\theta^{*})\ge L(q,\theta)\;\forall q,\theta,
\]

\end_inset

where of course
\begin_inset Formula 
\[
q^{*}(z)=p(z\mid x,\theta^{*}).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Claim: 
\begin_inset Formula $\theta^{*}$
\end_inset

 is a global maximum of 
\begin_inset Formula $\log p(x\mid\theta^{*})$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: For any 
\begin_inset Formula $\theta'$
\end_inset

, we showed that for 
\begin_inset Formula $q'(z)=p(z\mid x,\theta')$
\end_inset

 we have
\begin_inset Formula 
\begin{eqnarray*}
\log p(x\mid\theta') & = & \cl(q',\theta')+\kl[q',p(z\mid x,\theta')]\\
 & = & \cl(q',\theta')\\
 & \le & \cl(q^{*},\theta^{*})\\
 & = & \log p(x\mid\theta^{*})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Continuity argument can give same result for local.
 Bishop says if 
\begin_inset Formula $p(x,z\mid\theta)$
\end_inset

 is continuous function of 
\begin_inset Formula $\theta$
\end_inset

.
 I wonder if we need the continuity of the joint to ensure that reasonable
 thing happen with the 
\begin_inset Formula $q$
\end_inset

 piece? Not clear why we don't just need 
\begin_inset Formula $p(x\mid\theta)$
\end_inset

.
 Need to think about this more...
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Convergence of EM
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\theta_{n}$
\end_inset

 be value of EM algorithm after 
\begin_inset Formula $n$
\end_inset

 steps.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Define 
\begin_inset Quotes eld
\end_inset

transition function
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $M(\cdot)$
\end_inset

 such that 
\begin_inset Formula $\theta_{n+1}=M(\theta_{n})$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Suppose log-likelihood function 
\begin_inset Formula $\ell(\theta)=\log p(x\mid\theta)$
\end_inset

 is differentiable.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $S$
\end_inset

 be the set of stationary points of 
\begin_inset Formula $\ell(\theta)$
\end_inset

.
 (i.e.
 
\begin_inset Formula $\del_{\theta}\ell(\theta)=0$
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Theorem
Under mild regularity conditions
\begin_inset Foot
status open

\begin_layout Plain Layout
For details, see 
\begin_inset Quotes eld
\end_inset

Parameter Convergence for EM and MM Algorithms
\begin_inset Quotes erd
\end_inset

 by Florin Vaida in 
\emph on
Statistica Sinica
\emph default
 (2005).
 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://www3.stat.sinica.edu.tw/statistica/oldpdf/a15n316.pdf
\end_layout

\end_inset


\end_layout

\end_inset

, for any starting point 
\begin_inset Formula $\theta_{0}$
\end_inset

, 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\lim_{n\to\infty}\theta_{n}=\theta^{*}$
\end_inset

 for some stationary point 
\begin_inset Formula $\theta^{*}\in S$
\end_inset

 and
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta^{*}$
\end_inset

 is a fixed point of the EM algorithm, i.e.
 
\begin_inset Formula $M(\theta^{*})=\theta^{*}$
\end_inset

.
 Moreover,
\end_layout

\begin_layout Itemize
\begin_inset Formula $\ell(\theta_{n})$
\end_inset

 strictly increases to 
\begin_inset Formula $\ell(\theta^{*})$
\end_inset

 as 
\begin_inset Formula $n\to\infty$
\end_inset

, unless 
\begin_inset Formula $\theta_{n}\equiv\theta^{*}$
\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
In practice, can run EM multiple times with random starts.
 
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Variations on EM
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EM Gives Us Two New Problems
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The 
\begin_inset Quotes eld
\end_inset

E
\begin_inset Quotes erd
\end_inset

 Step: Computing
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
J(\theta):=\cl(q^{*},\theta)=\sum_{z}q^{*}(z)\log\left(\frac{p(x,z\mid\theta)}{q^{*}(z)}\right)
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The 
\begin_inset Quotes eld
\end_inset

M
\begin_inset Quotes erd
\end_inset

 Step: Computing
\begin_inset Formula 
\[
\theta^{\text{new}}=\argmax_{\theta}J(\theta).\pause
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Either of these can be too hard to do in practice.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generalized EM (GEM)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Addresses the problem of a difficult 
\begin_inset Quotes eld
\end_inset

M
\begin_inset Quotes erd
\end_inset

 step.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Rather than finding 
\begin_inset Formula 
\[
\theta^{\text{new}}=\argmax_{\theta}J(\theta),
\]

\end_inset

find 
\series bold
any
\series default
 
\begin_inset Formula $\theta^{\mbox{new}}$
\end_inset

 for which
\begin_inset Formula 
\[
J(\theta^{\mbox{new}})>J(\theta^{\mbox{old}}).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can use a standard nonlinear optimization strategy
\end_layout

\begin_deeper
\begin_layout Itemize
e.g.
 take a gradient step on 
\begin_inset Formula $J$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
We still get monotonically increasing likelihood.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EM and More General Variational Methods
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Quotes eld
\end_inset

E
\begin_inset Quotes erd
\end_inset

 step is difficult:
\end_layout

\begin_deeper
\begin_layout Itemize
Hard to take expectation w.r.t.
 
\begin_inset Formula $q^{*}(z)=p(z\mid x,\theta^{\text{old}})$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Solution: Restrict to distributions 
\begin_inset Formula $\cq$
\end_inset

 that are easy to work with.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Lower bound now looser:
\begin_inset Formula 
\[
q^{*}=\argmin_{q\in\cq}\kl[q(z),p(z\mid x,\theta^{\text{old}})]
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EM in Bayesian Setting
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have a prior 
\begin_inset Formula $p(\theta$
\end_inset

).
 
\end_layout

\begin_layout Itemize
Want to find MAP estimate: 
\begin_inset Formula $\hat{\theta}_{\text{MAP}}=\argmax_{\theta}p(\theta\mid x)$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\pause p(\theta\mid x) & = & p(x\mid\theta)p(\theta)/p(x)\\
\pause\log p(\theta\mid x) & = & \log p(x\mid\theta)+\log p(\theta)-\log p(x)
\end{eqnarray*}

\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Still can use our lower bound on 
\begin_inset Formula $\log p(x,\theta)$
\end_inset

.
\begin_inset Formula 
\[
J(\theta):=\cl(q^{*},\theta)=\sum_{z}q^{*}(z)\log\left(\frac{p(x,z\mid\theta)}{q^{*}(z)}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
Maximization step becomes 
\begin_inset Formula 
\[
\theta^{\mbox{new}}=\argmax_{\theta}\left[J(\theta)+\log p(\theta)\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Homework: Convince yourself our lower bound is still tight at 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Summer Homework: Gaussian Mixture Model (Hints)
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Homework: Derive EM for GMM from General EM Algorithm
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Subsequent slides may help set things up.
\end_layout

\begin_layout Itemize
Key skills:
\end_layout

\begin_deeper
\begin_layout Itemize
MLE for multivariate Gaussian distributions.
\end_layout

\begin_layout Itemize
Lagrange multipliers 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gaussian Mixture Model (
\begin_inset Formula $k$
\end_inset

 Components)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
GMM Parameters 
\begin_inset Formula 
\begin{eqnarray*}
\text{Cluster probabilities}: &  & \pi=\left(\pi_{1},\ldots,\pi_{k}\right)\\
\text{Cluster means}: &  & \mu=\left(\mu_{1},\ldots,\mu_{k}\right)\\
\text{Cluster covariance matrices:} &  & \Sigma=\left(\Sigma_{1},\ldots\Sigma_{k}\right)
\end{eqnarray*}

\end_inset

 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $\theta=\left(\pi,\mu,\Sigma\right)$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Marginal log-likelihood
\begin_inset Formula 
\begin{eqnarray*}
\log p(x\mid\theta) & = & \log\left\{ \sum_{z=1}^{k}\pi_{z}\cn\left(x\mid\mu_{z},\Sigma_{z}\right)\right\} 
\end{eqnarray*}

\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
\begin_inset Formula $q^{*}(z)$
\end_inset

 are 
\begin_inset Quotes eld
\end_inset

Soft Assignments
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we observe 
\begin_inset Formula $n$
\end_inset

 points: 
\begin_inset Formula $X=\left(x_{1},\ldots,x_{n}\right)\in\reals^{n\times d}$
\end_inset

 .
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $z_{1},\ldots,z_{n}\in\left\{ 1,\ldots,k\right\} $
\end_inset

 be corresponding hidden variables.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Optimal distribution 
\begin_inset Formula $q^{*}$
\end_inset

 is: 
\begin_inset Formula 
\begin{eqnarray*}
q^{*}(z) & = & p(z\mid x,\theta).
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Convenient to define the conditional distribution for 
\begin_inset Formula $z_{i}$
\end_inset

 given 
\begin_inset Formula $x_{i}$
\end_inset

 as
\begin_inset Formula 
\begin{eqnarray*}
\gamma_{i}^{j} & := & p\left(z=j\mid x_{i}\right)\\
\pause & = & \frac{\pi_{j}\cn\left(x_{i}\mid\mu_{j},\Sigma_{j}\right)}{\sum_{c=1}^{k}\pi_{c}\cn\left(x_{i}\mid\mu_{c},\Sigma_{c}\right)}
\end{eqnarray*}

\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Expectation Step
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
The complete log-likelihood is
\begin_inset Formula 
\begin{eqnarray*}
\log p(x,z\mid\theta) & = & \sum_{i=1}^{n}\log\left[\pi_{z}\cn\left(x_{i}\mid\mu_{z},\Sigma_{z}\right)\right]\\
 & = & \sum_{i=1}^{n}\left(\log\pi_{z}+\underbrace{\log\cn\left(x_{i}\mid\mu_{z},\Sigma_{z}\right)}_{\text{simplifies nicely}}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Take the expected complete log-likelihood w.r.t.
 
\begin_inset Formula $q^{*}$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
J(\theta) & = & \sum_{z}q^{*}(z)\log p(x,z\mid\theta)\\
 & = & \sum_{i=1}^{n}\sum_{j=1}^{k}\gamma_{i}^{j}\left[\log\pi_{j}+\log\cn\left(x_{i}\mid\mu_{j},\Sigma_{j}\right)\right]
\end{eqnarray*}

\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Maximization Step
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Find 
\begin_inset Formula $\theta^{*}$
\end_inset

 maximizing 
\begin_inset Formula $J(\theta)$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\mu_{c}^{\text{new}} & = & \frac{1}{n_{c}}\sum_{i=1}^{n}\gamma_{i}^{c}x_{i}\\
\Sigma_{c}^{\text{new}} & = & \frac{1}{n_{c}}\sum_{i=1}^{n}\gamma_{i}^{c}\left(x_{i}-\mu_{\text{MLE}}\right)\left(x_{i}-\mu_{\text{MLE}}\right)^{T}\\
\pi_{c}^{\text{new}} & = & \frac{n_{c}}{n},
\end{eqnarray*}

\end_inset

 for each 
\begin_inset Formula $c=1,\ldots,k$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\end_body
\end_document
