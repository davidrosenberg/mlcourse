#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin /Users/drosen/Dropbox/repos/mlcourse/Lectures/
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty
\end_preamble
\options handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Title
Neural Networks
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003 
\begin_inset Note Note
status open

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David Rosenberg 
\end_layout

\begin_layout Institute
New York University
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Just in article version
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Plots courtesy of Ningshan Zhang.}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Neural Networks Overview
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Objectives
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
What are neural networks?
\end_layout

\begin_layout Itemize
How do they fit into our toolbox?
\end_layout

\begin_layout Itemize
When should we consider using them?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Prediction Functions
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Linear prediction functions: SVM, ridge regression, Lasso
\end_layout

\begin_layout Itemize
Generate the feature vector 
\begin_inset Formula $\phi(x)$
\end_inset

 by hand.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Learn weight vector 
\begin_inset Formula $w$
\end_inset

 from data.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/linear-classifier.png
	lyxscale 40
	width 60text%

\end_inset


\end_layout

\begin_layout Itemize
So
\begin_inset Formula 
\[
\text{score}=w^{T}\phi(x)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Basic Neural Network (Multilayer Perceptron)
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Add an extra layer with a nonlinear transformation:
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/neural-network-percy.png
	lyxscale 40
	width 60text%

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We've introduced 
\series bold
hidden nodes 
\begin_inset Formula $h_{1}$
\end_inset

 
\series default
and 
\begin_inset Formula $h_{2}$
\end_inset

.
\begin_inset Formula 
\[
h_{i}=\sigma\left(v_{i}^{T}\phi(x)\right),
\]

\end_inset

where 
\begin_inset Formula $\sigma$
\end_inset

 is a nonlinear 
\series bold
activation function
\series default
.
 (We'll come back to this.) 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Basic Neural Network
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/neural-network-percy.png
	lyxscale 40
	width 60text%

\end_inset


\end_layout

\begin_layout Itemize
Score is just
\begin_inset Formula 
\begin{eqnarray*}
\text{score} & = & w_{1}h_{1}+w_{2}h_{2}\\
\pause & = & w_{1}\sigma(v_{1}^{T}\phi(x))+w_{2}\sigma\left(v_{2}^{T}\phi(x)\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
This is the basic recipe.
\end_layout

\begin_deeper
\begin_layout Itemize
We can add more hidden nodes.
\end_layout

\begin_layout Itemize
We can add more hidden layers.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Activation Functions
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
The 
\series bold
nonlinearity 
\series default
of the activation function is a key ingredient.
\end_layout

\begin_layout Itemize
The 
\series bold
logistic sigmoid
\series default
 function is one of the more commonly used:
\begin_inset Formula 
\[
\sigma(x)=\frac{1}{1+e^{-x}}.
\]

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/activationFn-Sigmoid.png
	lyxscale 30
	width 60text%

\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Activation Functions
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
More recently, the 
\series bold
rectified linear
\series default
 function has been very popular:
\begin_inset Formula 
\[
\sigma(x)=\max(0,x).
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset


\series bold
RELU
\series default

\begin_inset Quotes erd
\end_inset

 is much faster to calculate, and to calculate its derivatives.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/activationFn-Rectified_Linear.png
	lyxscale 30
	width 60text%

\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Approximation Ability: 
\begin_inset Formula $f(x)=x^{2}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
3 hidden units; logistic activation functions 
\begin_inset Note Note
status open

\begin_layout Plain Layout
The colors in this pic look different in PDF.
 Blue and Red reverse for some reason.
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Blue dots are training points; Dashed lines are hidden unit outputs; Final
 output in Red.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/Figure5.3a.pdf
	lyxscale 150
	width 50text%

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Bishop's 
\backslash
emph{Pattern Recognition and Machine Learning}, Fig 5.3 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Approximation Ability: 
\begin_inset Formula $f(x)=\sin(x)$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
3 hidden units; logistic activation function
\end_layout

\begin_layout Itemize
Blue dots are training points; Dashed lines are hidden unit outputs; Final
 output in Red.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/Figure5.3b.pdf
	lyxscale 60
	width 50text%

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Bishop's 
\backslash
emph{Pattern Recognition and Machine Learning}, Fig 5.3 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Approximation Ability: 
\begin_inset Formula $f(x)=\left|x\right|$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
3 hidden units; logistic activation functions 
\end_layout

\begin_layout Itemize
Blue dots are training points; Dashed lines are hidden unit outputs; Final
 output in Red.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/Figure5.3c.pdf
	lyxscale 60
	width 50text%

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Bishop's 
\backslash
emph{Pattern Recognition and Machine Learning}, Fig 5.3 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Approximation Ability: 
\begin_inset Formula $f(x)=\ind{x>0}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
3 hidden units; logistic activation function
\end_layout

\begin_layout Itemize
Blue dots are training points; Dashed lines are hidden unit outputs; Final
 output in Red.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/Figure5.3d.pdf
	lyxscale 60
	width 50text%

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Bishop's 
\backslash
emph{Pattern Recognition and Machine Learning}, Fig 5.3 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neural Network: Hidden Nodes as Learned Features
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/neural-network-percy.png
	lyxscale 40
	width 60text%

\end_inset


\end_layout

\begin_layout Itemize
Can interpret 
\begin_inset Formula $h_{1}$
\end_inset

 and 
\begin_inset Formula $h_{2}$
\end_inset

 as nonlinear features learned from data.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Facial Recognition: Learned Features
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/deep-learning-vision.png
	lyxscale 50
	height 80theight%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Andrew Ng's CS229 Deep Learning slides (
\backslash
url{http://cs229.stanford.edu/materials/CS229-DeepLearning.pdf})}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neural Network: The Hypothesis Space
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
What hyperparameters describe a neural network?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Number of layers
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Number of nodes in each hidden layer
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Activation function (many to choose from)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Example neural network hypothesis space:
\begin_inset Formula 
\[
\cf=\left\{ f:\reals^{d}\to\reals\mid f\text{ is a NN with 2 hidden layers, 500 nodes in each}\right\} 
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Functions in 
\begin_inset Formula $\cf$
\end_inset

 
\series bold
parameterized by the weights between nodes
\series default
.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neural Network: Loss Functions and Learning
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Neural networks give a 
\series bold
new hypothesis space
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
But we can use all the 
\series bold
same loss functions
\series default
 we've used before.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Optimization method of choice: 
\series bold
mini-batch SGD
\series default
.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
In practice, lots of little tweaks; see e.g.
 AdaGrad and Adam
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neural Network: Objective Function
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
In our simple network, the output score is given by 
\begin_inset Formula 
\begin{eqnarray*}
f(x) & = & w_{1}\sigma(v_{1}^{T}\phi(x))+w_{2}\sigma\left(v_{2}^{T}\phi(x)\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Objective with square loss is then
\begin_inset Formula 
\[
J(w,v)=\sum_{i=1}^{n}\left(y_{i}-f_{w,v}(x_{i})\right)^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Note: 
\begin_inset Formula $J(w,v)$
\end_inset

 is 
\series bold
not convex
\series default
.
\end_layout

\begin_deeper
\begin_layout Itemize
makes optimization much more difficult
\end_layout

\begin_layout Itemize
accounts for many of the 
\begin_inset Quotes eld
\end_inset

tricks of the trade
\begin_inset Quotes erd
\end_inset

 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Learning with Back-Propagation
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Back-propagation is an 
\series bold
algorithm
\series default
 for computing the gradient
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
With lots of chain rule, you could also work out the gradient by hand.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Back-propagation is 
\end_layout

\begin_deeper
\begin_layout Itemize
a clean way to organize the computation of the gradient
\end_layout

\begin_layout Itemize
an efficient way to compute the gradient
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Nice introduction to this perspective:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset CommandInset href
LatexCommand href
name "Stanford CS221 Lecture 3 (2016), Slides 75-94"
target "http://web.stanford.edu/class/cs221/lectures/learning2.pdf#page=59"

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Neural Network Regularization
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neural Network Regularization
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Neural networks are very expressive.
\end_layout

\begin_layout Itemize
Correspond to big hypothesis spaces.
\end_layout

\begin_layout Itemize
Many approaches are used for regularization.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Tikhonov Regularization? Sure.
 
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Can add an 
\begin_inset Formula $\ell_{2}$
\end_inset

 and/or 
\begin_inset Formula $\ell_{1}$
\end_inset

 regularization terms to our objective:
\begin_inset Formula 
\[
J(w,v)=\sum_{i=1}^{n}\left(y_{i}-f_{w,v}(x_{i})\right)^{2}+\lambda_{1}\|w\|^{2}+\lambda_{2}\|v\|^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
In neural network literature, this is often called 
\series bold
weight decay
\series default
.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Regularization by Early Stopping
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
As we train, check performance on validation set every once in a while.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Don't stop immediately after validation error goes back up.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The 
\begin_inset Quotes eld
\end_inset


\series bold
patience
\series default

\begin_inset Quotes erd
\end_inset

 parameter: the number training rounds to continue after finding a minimum
 of validation error.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Start with patience = 10000.
\end_layout

\begin_layout Itemize
Whenever we find a minimum at step 
\begin_inset Formula $T$
\end_inset

,
\end_layout

\begin_deeper
\begin_layout Itemize
Set 
\begin_inset Formula $\text{patience}\gets\text{patience}+cT$
\end_inset

, for some constant 
\begin_inset Formula $c$
\end_inset

.
\end_layout

\begin_layout Itemize
Then run at least patience extra steps before stopping.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{See 
\backslash
url{http://arxiv.org/pdf/1206.5533v2.pdf} for details.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Max-Norm Regularization
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Max-norm regularization
\series default
: Enforce max norm of incoming weight vector at every hidden node to be
 bounded:
\begin_inset Formula 
\[
\|w\|_{2}\le c.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Project any 
\begin_inset Formula $w$
\end_inset

 that's too large onto ball of radius 
\begin_inset Formula $c$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
It's like 
\begin_inset Formula $\ell_{2}$
\end_inset

-complexity control, but locally at each node.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Why? 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
There are heuristic justifications, but proof is in the performance.
\end_layout

\begin_layout Itemize
We'll see below.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{See 
\backslash
url{http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf} for details.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Dropout for Regularization
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
A recent trick for improving generalization performance is 
\series bold
dropout
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
A fixed probability 
\begin_inset Formula $p$
\end_inset

 is chosen.
\end_layout

\begin_layout Itemize
Before every stochastic gradient step, 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
each node is selected for 
\begin_inset Quotes eld
\end_inset

dropout
\begin_inset Quotes erd
\end_inset

 with probability 
\begin_inset Formula $p$
\end_inset


\end_layout

\begin_layout Itemize
a dropout node is removed, along with its links
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
after the stochastic gradient step, all nodes are restored.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/dropout.png
	lyxscale 40
	width 60text%

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Figure from 
\backslash
url{http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf}.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Dropout for Regularization
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
At prediction time
\end_layout

\begin_deeper
\begin_layout Itemize
all nodes are present
\end_layout

\begin_layout Itemize
outgoing weights are multiplied by 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/dropout-reweighting.png
	lyxscale 40
	width 60text%

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Dropout probability set using a validation set, or just set at 
\begin_inset Formula $0.5$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Closer to 
\begin_inset Formula $0.8$
\end_inset

 usually works better for input units.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Figure from 
\backslash
url{http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf}.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Dropout: Why might this help?
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Since any node may randomly disappear, 
\end_layout

\begin_deeper
\begin_layout Itemize
forced to 
\begin_inset Quotes eld
\end_inset

spread the knowledge
\begin_inset Quotes erd
\end_inset

 across the nodes.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Each hidden only gets a randomly chosen sample of its inputs,
\end_layout

\begin_deeper
\begin_layout Itemize
so won't become too reliant on any single input.
\end_layout

\begin_layout Itemize
More robust.
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Dropout: Does it help?
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/dropout-results.png
	lyxscale 40
	width 90text%

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Figure from 
\backslash
url{http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf}.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
How big a network?
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
How many hidden units?
\end_layout

\begin_layout Itemize
With proper regularization, too many doesn't hurt.
\end_layout

\begin_deeper
\begin_layout Itemize
Except in computation time.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Multiple Output Networks
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Multiple Output Neural Networks
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Very easy to add extra outputs to neural network structure.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/multiple-output-nn.png
	lyxscale 50
	width 80text%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Andrew Ng's CS229 Deep Learning slides (
\backslash
url{http://cs229.stanford.edu/materials/CS229-DeepLearning.pdf})}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Multitask Learning
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $\cx=\left\{ \mbox{Natural Images}\right\} $
\end_inset

.
\end_layout

\begin_layout Itemize
We have two tasks:
\end_layout

\begin_deeper
\begin_layout Itemize
Does the image have a cat?
\end_layout

\begin_layout Itemize
Does the image have a dog?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Can have one output for each task.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Seems plausible that basic pixel features would be shared by tasks.
\end_layout

\begin_layout Itemize
Learn them on the same neural network â€“ benefit both tasks.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Single Task with 
\begin_inset Quotes eld
\end_inset

Extra Tasks
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Only one task we're interested in.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Gather data from related tasks.
 
\end_layout

\begin_layout Itemize
Train them along with the task you're interested in.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
No related tasks? Another trick:
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Choose any input feature.
\end_layout

\begin_layout Itemize
Change it's value to zero.
\end_layout

\begin_layout Itemize
Make the prediction problem to predict the value of that feature.
\end_layout

\begin_layout Itemize
Can help make model more robust (not depending too heavily on any single
 input).
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Multiclass Classification
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Could make each class a separate task / output.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Suppose we have 
\begin_inset Formula $K$
\end_inset

 classes.
\end_layout

\begin_layout Itemize
Use a one-hot encoding of each 
\begin_inset Formula $y_{i}\in\left\{ 1,\ldots,K\right\} $
\end_inset

:
\begin_inset Formula 
\[
y_{i}=\left(y_{i1},\ldots,y_{ik}\right)\text{ with }y_{ik}=\ind{y_{i}=k}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $K$
\end_inset

 output scores: 
\begin_inset Formula $f_{1}(x),\ldots,f_{K}(x)$
\end_inset

.
 Each 
\begin_inset Formula $f_{k}$
\end_inset

 is trained to predict 
\begin_inset Formula $1$
\end_inset

 if class is 
\begin_inset Formula $k$
\end_inset

, 
\begin_inset Formula $0$
\end_inset

 otherwise.
\end_layout

\begin_layout Itemize
Predict with 
\begin_inset Formula $f^{*}(x)=\argmax_{k}\left[f_{k}(x)\right].$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Old days: train each output separately, e.g.
 with square loss.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Multiclass Classification: Cross-Entropy Loss
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Network can do better if it 
\begin_inset Quotes eld
\end_inset

knows
\begin_inset Quotes erd
\end_inset

 that classes are mutually exclusive.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Need to introduce a joint loss across the outputs.
 
\end_layout

\begin_layout Itemize
Joint loss function (cross-entropy/deviance):
\begin_inset Formula 
\[
\ell(w,v)=-\sum_{i=1}^{n}\sum_{i=1}^{K}y_{ik}\log f_{k}(x_{i}),
\]

\end_inset

where 
\begin_inset Formula $y_{ik}=\ind{y_{i}=k}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
This is the negative log-likelihood we get for softmax predictions in multinomia
l logistic regression.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Neural Networks for Features 
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
OverFeat: Features
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
OverFeat is a neural network for image classification
\end_layout

\begin_deeper
\begin_layout Itemize
Trained on the huge ImageNet dataset
\end_layout

\begin_layout Itemize
Lots of computing resources used for training the network.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
All those hidden layers of the network are very valuable 
\series bold
features
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Paper: 
\begin_inset Quotes eld
\end_inset


\emph on
CNN Features off-the-shelf: an Astounding Baseline for Recognition
\emph default

\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
Showed that using features from OverFeat makes it easy to achieve state-of-the-a
rt performance on new vision tasks.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{OverFeat code is at 
\backslash
url{https://github.com/sermanet/OverFeat}}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Neural Networks: When and why?
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neural Networks Benefit from Big Data
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/andrew-ng-big-data.png
	lyxscale 50
	width 80text%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Andrew Ng's CS229 Deep Learning slides (
\backslash
url{http://cs229.stanford.edu/materials/CS229-DeepLearning.pdf})}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Big Data Requires Big Resources
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Best results always involve GPU processing.
\end_layout

\begin_layout Itemize
Typically on huge networks.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/neural-networks/google-brain.png
	lyxscale 50
	width 60text%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Andrew Ng's CS229 Deep Learning slides (
\backslash
url{http://cs229.stanford.edu/materials/CS229-DeepLearning.pdf})}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neural Networks: When to Use?
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Computer vision problems
\end_layout

\begin_deeper
\begin_layout Itemize
All state of the art methods use neural networks
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Speech recognition
\end_layout

\begin_deeper
\begin_layout Itemize
All state of the art methods use neural networks
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Natural Language problems
\end_layout

\begin_deeper
\begin_layout Itemize
Maybe.
 State-of-the-art, but not as large a margin.
\end_layout

\begin_layout Itemize
Check out 
\begin_inset Quotes eld
\end_inset

word2vec
\begin_inset Quotes erd
\end_inset

 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://code.google.com/p/word2vec/
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
Represents words using real-valued vectors.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Potentially much better than bag of words.
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\end_body
\end_document
