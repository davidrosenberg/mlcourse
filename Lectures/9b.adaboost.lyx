#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin /Users/drosen/Dropbox/repos/mlcourse/Lectures/
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\end_preamble
\options handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\err}{\text{err}}
\end_inset


\end_layout

\begin_layout Title
Boosting
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003 
\begin_inset Note Note
status open

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David Rosenberg 
\end_layout

\begin_layout Institute
New York University
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Just in article version
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Plots courtesy of Ningshan Zhang.}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Wondering if we should extend the plots for error >.5? This would be a really
 annoying hypothesis space that was always worse than random..
 But they would enter the ensemble with a negative weight.
 Might should at least point this out...
 because it's only a nonnegative combination if error is always <=.5.
 QUESTION: Does AdaBoost work on training data if error is bounded away
 from 0.5?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Notes from Zhou's book - Section 2.4: Freund and Schapire (1997) showed 
\begin_inset Quotes eld
\end_inset

generalization error of AdaBoost is upper bounded by
\begin_inset Quotes erd
\end_inset

 a term that grows with the square root of the VC-dimenison of the base
 hypothesis space and the number of learning rounds, and decreases with
 the square root of m, the number of training examples.
 
\begin_inset Quotes eld
\end_inset

Empirical studies, however, show that AdaBoost often does not overfit; that
 is, the test error often tends to decrase een after the trainnig eror reaches
 zero, even after a large numer of rounds such as 1000.
 .
 ..
 explaining why AdaBoost seems resistant ot overfitting becomes one of the
 cetnral theoretical issues and has attracted much attention..
 [then there's the margin story, which seems to be overturned by Breiman's
 work with arc-gv, but 7 years later is finally explained away by Reyzin
 and Schapire [2006].
 turns out hte margin distribution of AdaBoost is better than that of arc-gv...
 (i.e.
 it's not just about the worst margin, but the distribution of margins).
 
\begin_inset Quotes eld
\end_inset

Gao and Zhou (2012) gave a new generalization error bound based on emprical
 Bernstein inequality ...
 that further defends the margin-baesd explanation aainst Breiman's doubts.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout
A statistical view of boosting developed that AdaBoost is just an optimization
 process that tries to fit an additive model to some surrogate loss function.
 LogitBoost uses log-loss, which should an equivalent probabilistic classifier
 asymptotically.
 But if it's really just an optimization of a loss function, there are other
 ways to minimize the same loss function using an additive model of weak
 learners.
 Resulting algorithm from one line of work is LPBoost, whcih doesn't seem
 to improve on adaboost....
 there seems to be something important in HOW the function is being fit
\end_layout

\end_inset


\end_layout

\begin_layout Section
Boosting Introduction
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Ensembles: Parallel vs Sequential
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Ensemble methods combine multiple models 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Parallel ensembles
\series default
: each model is built independently
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
e.g.
 bagging and random forests
\end_layout

\begin_layout Itemize
Main Idea: Combine many (high complexity, low bias) models to reduce variance
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize

\series bold
Sequential ensembles
\series default
: 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Models are generated sequentially
\end_layout

\begin_layout Itemize
Try to add new models that do well where previous models lack
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Example of sequential ensemble scenario???
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Boosting Question: Weak Learners
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
A 
\series bold
weak learner 
\series default
is a classifier that does slightly better than random.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Weak learners are like 
\begin_inset Quotes eld
\end_inset

rules of thumb
\begin_inset Quotes erd
\end_inset

:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
If an email has 
\begin_inset Quotes eld
\end_inset

Viagra
\begin_inset Quotes erd
\end_inset

 in it, more likely than not it's spam.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Email from a friend is probably not spam.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
A linear decision boundary.
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
Can we extract wisdom from a committee of fools?
\end_layout

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can we 
\series bold
combine
\series default
 a set of weak classifiers to form single classifier that makes accurate
 predictions?
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Posed by Kearns and Valiant (1988,1989): 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Yes! 
\series bold
Boosting
\series default
 solves this problem.
 [Rob Schapire (1990).]
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Pronounced: Sha pee ree (according to Berk Kapicioglu) 
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
AdaBoost
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Called AdaBoost, for adaptive boosting, because we don't need to know the
 
\begin_inset Quotes eld
\end_inset

edge
\begin_inset Quotes erd
\end_inset

 of each weak learner.
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Setting
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider 
\begin_inset Formula $\cy=\left\{ -1,1\right\} $
\end_inset

 (binary classification).
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Suppose we have a 
\series bold
weak learner
\series default
:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Hypothesis space 
\begin_inset Formula $\cf=\left\{ f:\cx\to\left\{ -1,1\right\} \right\} $
\end_inset

.
 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Note
\series default
: not producing a score, but an actual class label.
\end_layout

\end_deeper
\begin_layout Itemize
Algorithm for finding 
\begin_inset Formula $f\in\cf$
\end_inset

 that's better than random on training data.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Typical weak learners:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Decision stumps
\series default
 (tree with a single split)
\end_layout

\begin_layout Itemize
Trees with few terminal nodes
\end_layout

\begin_layout Itemize
Linear decision functions
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Weighted Training Set
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Training set 
\begin_inset Formula $\cd=\left\{ \left(x_{1},y_{1}\right),\ldots,\left(x_{n},y_{n}\right)\right\} $
\end_inset

.
\end_layout

\begin_layout Itemize
Weights 
\begin_inset Formula $\left(w_{1},\ldots,w_{n}\right)$
\end_inset

 associated with each example.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Weighted empirical risk
\series default
:
\begin_inset Formula 
\[
\hat{R}_{n}^{w}(f)=\frac{1}{W}\sum_{i=1}^{n}w_{i}\ell\left(f(x_{i}),y_{i}\right)\quad\mbox{where}\;W=\sum_{i=1}^{n}w_{i}
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can train a model to minimize weighted empirical risk.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What if model cannot conveniently be trained to reweighted data?
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can sample a new data set from 
\begin_inset Formula $\cd$
\end_inset

 with probabilities
\begin_inset Formula $\left(w_{1}/W,\ldots w_{n}/W\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost - Rough Sketch
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Training set 
\begin_inset Formula $\cd=\left\{ \left(x_{1},y_{1}\right),\ldots,\left(x_{n},y_{n}\right)\right\} $
\end_inset

.
\end_layout

\begin_layout Itemize
Start with equal weight on all training points 
\begin_inset Formula $w_{1}=\cdots=w_{n}=1$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Repeat for 
\begin_inset Formula $m=1,\ldots,M$
\end_inset

:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Fit weak classifier 
\begin_inset Formula $G_{m}(x)$
\end_inset

 to weighted training points
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Increase weight on points 
\begin_inset Formula $G_{m}(x)$
\end_inset

 misclassifies 
\end_layout

\end_deeper
\begin_layout Itemize
So far, we've generated 
\begin_inset Formula $M$
\end_inset

 classifiers: 
\begin_inset Formula $G_{1}(x),\ldots,G_{m}(x)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Schematic
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/adaboostSchematic.png
	lyxscale 40
	height 70theight%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From ESL Figure 10.1}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost - Rough Sketch
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Training set 
\begin_inset Formula $\cd=\left\{ \left(x_{1},y_{1}\right),\ldots,\left(x_{n},y_{n}\right)\right\} $
\end_inset

.
\end_layout

\begin_layout Itemize
Start with equal weight on all training points 
\begin_inset Formula $w_{1}=\cdots=w_{n}=1$
\end_inset

.
\end_layout

\begin_layout Itemize
Repeat for 
\begin_inset Formula $m=1,\ldots,M$
\end_inset

:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Fit weak classifier 
\begin_inset Formula $G_{m}(x)$
\end_inset

 to weighted training points
\end_layout

\begin_layout Itemize
Increase weight on points 
\begin_inset Formula $G_{m}(x)$
\end_inset

 misclassifies
\end_layout

\end_deeper
\begin_layout Itemize
Final prediction 
\begin_inset Formula $G(x)=\sign\left[\sum_{m=1}^{M}\alpha_{m}G_{m}(x)\right]$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The 
\begin_inset Formula $\alpha_{m}$
\end_inset

's are nonnegative,
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
larger when 
\begin_inset Formula $G_{m}$
\end_inset

 fits its weighted 
\begin_inset Formula $\cd$
\end_inset

 well
\end_layout

\begin_layout Itemize
smaller when 
\begin_inset Formula $G_{m}$
\end_inset

 fits weighted 
\begin_inset Formula $\cd$
\end_inset

 less well
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Important: Each 
\begin_inset Formula $G_{m}(x)$
\end_inset

 produces a class 
\begin_inset Formula $\left\{ -1,1\right\} $
\end_inset

, not a score.
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Adaboost: Weighted Classification Error
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In round 
\begin_inset Formula $m$
\end_inset

, weak learner gets a weighted training set.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Returns a classifier 
\begin_inset Formula $G_{m}(x)$
\end_inset

 that roughly minimizes weighted 
\begin_inset Formula $0-1$
\end_inset

 error.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
The 
\series bold
weighted 0-1 error
\series default
 of 
\begin_inset Formula $G_{m}(x)$
\end_inset

 is
\begin_inset Formula 
\[
\mbox{err}_{m}=\frac{1}{W}\sum_{i=1}^{n}w_{i}\ind{y_{i}\neq G_{m}(x_{i})}\quad\text{where }W=\sum_{i=1}^{n}w_{i}.
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Notice: 
\begin_inset Formula $\text{err}_{m}\in[0,1]$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We treat the weak learner as a black box.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
It can use any method it wants to find 
\begin_inset Formula $G_{m}(x)$
\end_inset

.
 (e.g.
 SVM, tree, etc.)
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
BUT, for things to work, we need at least 
\begin_inset Formula $\err_{m}<0.5$
\end_inset

.
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Classifier Weights
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Note Note
status open

\begin_layout Plain Layout
Next year, consider multiplying these classifier weights by 
\begin_inset Formula $\frac{1}{2}$
\end_inset

 so that they are the same thing you get when you try to minimize exponential
 loss.
 Now it's slightly confusing.
 You got two different AdaBoost 
\begin_inset Quotes eld
\end_inset

score
\begin_inset Quotes erd
\end_inset

 functions...
 doesn't matter if you're just taking the sign of it, but matters if you're
 trying to produce conditional probabilities.
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The weight of classifier 
\begin_inset Formula $G_{m}(x)$
\end_inset

 is 
\begin_inset Formula $\alpha_{m}=\ln\left(\frac{1-\text{err}_{m}}{\text{err}_{m}}\right).$
\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/adaboostAlphaVsError.png
	lyxscale 40
	height 60theight%
	clip

\end_inset


\end_layout

\begin_layout Itemize
Note that weight 
\begin_inset Formula $\alpha_{m}\to0$
\end_inset

 as weighted error 
\begin_inset Formula $\err_{m}\to0.5$
\end_inset

 (random guessing).
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Example Reweighting
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
We train 
\begin_inset Formula $G_{m}$
\end_inset

 to minimize weighted error, and it achieves 
\begin_inset Formula $\err_{m}$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then 
\begin_inset Formula $\alpha_{m}=\ln\left(\frac{1-\text{err}_{m}}{\text{err}_{m}}\right)$
\end_inset

 is the weight of 
\begin_inset Formula $G_{m}$
\end_inset

 in final ensemble.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $w_{i}$
\end_inset

 is weight of example 
\begin_inset Formula $i$
\end_inset

 before training:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $G_{m}$
\end_inset

 classfies 
\begin_inset Formula $x_{i}$
\end_inset

 correctly, then 
\begin_inset Formula $w_{i}$
\end_inset

 is unchanged.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Otherwise, 
\begin_inset Formula $w_{i}$
\end_inset

 is increased as 
\begin_inset Formula 
\begin{eqnarray*}
w_{i} & \gets & w_{i}e^{\alpha_{m}}\\
\pause & = & w_{i}\left(\frac{1-\text{err}_{m}}{\text{err}_{m}}\right)
\end{eqnarray*}

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
See why this only increases the weight? [at least for 
\begin_inset Formula $\mbox{err}_{m}<0.5$
\end_inset

]
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Adaboost: Example Reweighting
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Any misclassified point has weight adjusted as 
\begin_inset Formula $w_{i}\gets w_{i}\left(\frac{1-\text{err}_{m}}{\text{err}_{m}}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/adaboostWeightUpdate.png
	lyxscale 40
	height 60theight%
	clip

\end_inset


\end_layout

\begin_layout Itemize
The smaller 
\begin_inset Formula $\err_{m}$
\end_inset

, the more we increase weight of misclassified points.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Algorithm
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Given training set 
\begin_inset Formula $\cd=\left\{ \left(x_{1},y_{1}\right),\ldots,\left(x_{n},y_{n}\right)\right\} $
\end_inset

.
\end_layout

\begin_layout Enumerate
Initialize observation weights 
\begin_inset Formula $w_{i}=1/n$
\end_inset

, 
\begin_inset Formula $i=1,2,\ldots,n$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
For 
\begin_inset Formula $m=1$
\end_inset

 to 
\begin_inset Formula $M$
\end_inset

:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Fit weak classifier 
\begin_inset Formula $G_{m}(x)$
\end_inset

 to 
\begin_inset Formula $\cd$
\end_inset

 using weights 
\begin_inset Formula $w_{i}$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Compute weighted empirical 0-1 risk:
\begin_inset Formula 
\[
\mbox{err}_{m}=\frac{1}{W}\sum_{i=1}^{n}w_{i}\ind{y_{i}\neq G_{m}(x_{i})}\quad\text{where }W=\sum_{i=1}^{n}w_{i}.
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Compute 
\begin_inset Formula $\alpha_{m}=\ln\left(\frac{1-\text{err}_{m}}{\text{err}_{m}}\right)$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Set 
\begin_inset Formula $w_{i}\gets w_{i}\cdot\exp\left[\alpha_{m}\ind{y_{i}\neq G_{m}(x_{i})}\right],\quad i=1,2,\ldots,N$
\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
Ouptut 
\begin_inset Formula $G(x)=\sign\left[\sum_{m=1}^{M}\alpha_{m}G_{m}(x)\right]$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost with Decision Stumps
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
After 1 round:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Figures/boosting/fig16.10a.pdf
	lyxscale 70
	height 60theight%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\size footnotesize
Plus size represents weight.
 Blackness represents score for red class.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{KPM Figure 16.10}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost with Decision Stumps
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
After 3 rounds:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Figures/boosting/fig16.10b.pdf
	lyxscale 70
	height 60theight%
	clip

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\size footnotesize
Plus size represents weight.
 Blackness represents score for red class.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{KPM Figure 16.10}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost with Decision Stumps
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
After 120 rounds:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Figures/boosting/fig16.10c.pdf
	lyxscale 70
	height 60theight%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\size footnotesize
Plus size represents weight.
 Blackness represents score for red class.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{KPM Figure 16.10}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Does it actually minimize training error?
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Methods we've seen so far come in two categories:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Convex optimization problems (L1/L2 regression, SVM, kernelized versions)
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
No issue minimizing objective function over hypothesis space
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Trees
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Can always fit data perfectly with big enough tree 
\begin_inset Note Note
status open

\begin_layout Plain Layout
except for exactly duplicate inputs and label noise
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Itemize
AdaBoost is something new - at this point, it's just an algorithm.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
In this sense, it's like the Perceptron algorithm.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Will 
\begin_inset Formula $G(x)$
\end_inset

 even minimize training error?
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Yes
\begin_inset Quotes erd
\end_inset

, if our weak classifiers have an 
\begin_inset Quotes eld
\end_inset


\series bold
edge
\series default

\begin_inset Quotes erd
\end_inset

 over random.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Does it actually minimize training error?
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
As a weak classifier, 
\begin_inset Formula $G_{m}(x)$
\end_inset

 should have 
\begin_inset Formula $\err_{m}<\frac{1}{2}$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Define the 
\series bold
edge 
\series default
of classifier 
\begin_inset Formula $G_{m}(x)$
\end_inset

 at round 
\begin_inset Formula $m$
\end_inset

 to be 
\begin_inset Formula 
\[
\gamma_{m}=\frac{1}{2}-\err_{m}.
\]

\end_inset


\end_layout

\begin_layout Itemize
Measures how much better than random 
\begin_inset Formula $G_{m}$
\end_inset

 performs.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Does it actually minimize training error?
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
The empirical 0-1 risk of the AdaBoost classifier 
\begin_inset Formula $G(x)$
\end_inset

 is bounded as
\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}\ind{y_{i}\neq G(x)}\le\prod_{m=1}^{M}\sqrt{1-4\gamma_{m}^{2}}.
\]

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Formula $\le\exp\left(-2\sum_{m=1}^{M}\gamma_{m}^{2}\right)$
\end_inset

 since
\begin_inset Formula 
\begin{eqnarray*}
\sqrt{1-4\gamma_{m}^{2}} & = & \left(1-4\gamma_{m}^{2}\right)^{\frac{1}{2}}\\
 & \le & \left(e^{-4\gamma_{m}^{2}}\right)^{\frac{1}{2}}=e^{-2\gamma_{m}^{2}}
\end{eqnarray*}

\end_inset

upposedly based on 
\begin_inset Formula $1+x\le e^{x}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{For more details, see the book 
\backslash
emph{Boosting: Foundations and Algorithms} by Schapire and Freund.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Does it actually minimize training error?
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Example
Suppose 
\begin_inset Formula $\err_{m}\le0.4$
\end_inset

 for all 
\begin_inset Formula $m$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula $\gamma_{m}=.5-.4=.1$
\end_inset

, and
\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}\ind{y_{i}\neq G(x)}\le\prod_{m=1}^{M}\sqrt{1-4(.1)^{2}}\approx\left(.98\right)^{M}
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Bound decreases exponentially:
\begin_inset Formula 
\begin{eqnarray*}
.98^{100} & \approx & .133\\
.98^{200} & \approx & .018\\
.98^{300} & \approx & .002
\end{eqnarray*}

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
With a consistent edge, training error decreases very quickly to 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{For more details, see the book 
\backslash
emph{Boosting: Foundations and Algorithms} by Schapire and Freund.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Test Performance of Boosting
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Next year: leave the out unless I give more reasons or caveats.
 I guess the point of this is to say that AdaBoost isn't 
\begin_inset Quotes eld
\end_inset

just
\begin_inset Quotes erd
\end_inset

 minimizing an objective function.
 The way it does it is good too...?
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Typical Train / Test Learning Curves
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Might expect too many rounds of boosting to overfit:
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/typicalTrainTestCurve.png
	lyxscale 40
	height 65theight%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Rob Schapire's NIPS 2007 Boosting tutorial.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Learning Curves for AdaBoost
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In typical performance, AdaBoost is surprisingly resistant to overfitting.
\end_layout

\begin_layout Itemize
Test continues to improve even after training error is zero!
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/actualTrainTestCurves.png
	lyxscale 40
	height 65theight%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Rob Schapire's NIPS 2007 Boosting tutorial.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Boosting Fits an Additive Model 
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Adaptive Basis Function Model
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
AdaBoost produces a classification score function of the form 
\begin_inset Formula 
\[
\sum_{m=1}^{M}\alpha_{m}G_{m}(x)
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
each 
\begin_inset Formula $G_{m}$
\end_inset

 is a 
\series bold
weak classifier
\series default

\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
The 
\begin_inset Formula $G_{m}$
\end_inset

's are like basis functions, but they are learned from the data.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let's move beyond classification models...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Adaptive Basis Function Model
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Base hypothesis space
\series default
 
\begin_inset Formula $\cf$
\end_inset

 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
the 
\begin_inset Quotes eld
\end_inset

weak classifiers
\begin_inset Quotes erd
\end_inset

 in boosting context
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
An 
\series bold
adaptive basis function expansion 
\series default
over 
\begin_inset Formula $\cf$
\end_inset

 is
\begin_inset Formula 
\[
f(x)=\sum_{m=1}^{M}\nu_{m}h_{m}(x),
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{m}\in\cf$
\end_inset

 chosen in a learning process (
\begin_inset Quotes eld
\end_inset

adaptive
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Itemize
\begin_inset Formula $\nu_{m}\in\reals$
\end_inset

 are 
\series bold
expansion coefficients.

\series default
 
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Note: 
\series default
We are taking linear combination of outputs of 
\begin_inset Formula $h_{m}(x)$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Functions in 
\begin_inset Formula $h_{m}\in\cf$
\end_inset

 must produce values in 
\begin_inset Formula $\reals$
\end_inset

 (or a vector space) 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
How to fit an adaptive basis function model?
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Loss function
\series default
: 
\begin_inset Formula $\ell(y,\hat{y})$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Base hypothesis space
\series default
: 
\begin_inset Formula $\cf$
\end_inset

 of 
\series bold
real-valued
\series default
 functions
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Want to find
\begin_inset Formula 
\[
f(x)=\sum_{m=1}^{M}\nu_{m}h_{m}(x)
\]

\end_inset

that 
\series bold
minimizes empirical risk
\series default

\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}\ell\left(y_{i},f(x_{i})\right).
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We'll proceed in stages, adding a new 
\begin_inset Formula $h_{m}$
\end_inset

 in every stage.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Forward Stagewise Additive Modeling (FSAM)
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Start with 
\begin_inset Formula $f_{0}\equiv0$
\end_inset

.
\end_layout

\begin_layout Itemize
After 
\begin_inset Formula $m-1$
\end_inset

 stages, we have
\begin_inset Formula 
\[
f_{m-1}=\sum_{i=1}^{m-1}\nu_{i}h_{i},
\]

\end_inset

where 
\begin_inset Formula $h_{1},\ldots,h_{m-1}\in\cf$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Want to find 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
step direction
\series default
 
\begin_inset Formula $h_{m}\in\cf$
\end_inset

 and
\end_layout

\begin_layout Itemize

\series bold
step size 
\series default

\begin_inset Formula $\nu_{i}>0$
\end_inset

 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
So that 
\begin_inset Formula 
\[
f_{m}=f_{m-1}+\nu_{i}h_{m}
\]

\end_inset

minimizes empirical risk.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Forward Stagewise Additive Modeling
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Initialize 
\begin_inset Formula $f_{0}(x)=0$
\end_inset

.
\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $m=1$
\end_inset

 to 
\begin_inset Formula $M$
\end_inset

:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Enumerate
Compute:
\begin_inset Formula 
\[
\left(\nu_{m},h_{m}\right)=\argmin_{\nu\in\reals,h\in\cf}\sum_{i=1}^{n}\ell\left(y_{i},f_{m-1}(x_{i})\underbrace{+\nu h(x_{i})}_{\text{new piece}}\right).
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Set 
\begin_inset Formula $f_{m}=f_{m-1}+\nu_{m}h$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
Return: 
\begin_inset Formula $f_{M}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Exponential Loss and AdaBoost
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Take loss function to be 
\begin_inset Formula 
\[
\ell(y,f(x))=\exp\left(-yf(x)\right).
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\cf=\left\{ b(x;\gamma)\mid\gamma\in\Gamma\right\} $
\end_inset

 be a hypothesis space of weak classifiers.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then Forward Stagewise Additive Modeling (FSAM) reduces to AdaBoost! (See
 HTF Section 10.4 for proof.)
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Only difference:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
AdaBoost is loose about each 
\begin_inset Formula $G_{m}$
\end_inset

 
\begin_inset Quotes eld
\end_inset

fitting the weighted training data
\begin_inset Quotes erd
\end_inset

 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Just needs to 
\begin_inset Quotes eld
\end_inset

have an edge
\begin_inset Quotes erd
\end_inset

 over random classification
\end_layout

\end_deeper
\begin_layout Itemize
For FSAM we're explicitly looking for
\begin_inset Formula 
\[
G_{m}=\argmin_{G\in\cf}\sum_{i=1}^{N}w_{i}^{(m)}\ind{y_{i}\neq G(x_{i})}
\]

\end_inset

 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Robustness and AdaBoost
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Exponential Loss
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Note that exponential loss puts a very large weight on bad misclassifications.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/loss.Zero_One.Hinge.Logistic_Rescaled.Exponential.png
	lyxscale 25
	height 65theight%

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost / Exponential Loss: Robustness Issues
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
When Bayes error rate is high (e.g.
 
\begin_inset Formula $\pr\left(f^{*}(X)\neq Y\right)=0.25$
\end_inset

)
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Training examples with same input, but different classifications.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Best we can do is predict the most likely class for each 
\begin_inset Formula $X$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Some training predictions 
\series bold
should be wrong
\series default
 (because example doesn't have majority class)
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
AdaBoost / exponential loss puts a lot of focus on geting those right
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Empirically, AdaBoost has degraded performance in situations with 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
high Bayes error rate, or when there's 
\end_layout

\begin_layout Itemize
high 
\begin_inset Quotes eld
\end_inset


\series bold
label noise
\series default

\begin_inset Quotes erd
\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Logistic loss performs better in settings with high Bayes error
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Population Minimizer
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Population Minimizers
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In traditional statistics, the 
\series bold
population 
\series default
refers to
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
the full population of a group, rather than a sample.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
In machine learning, the 
\series bold
population case 
\series default
is the hypothetical case of
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
an infinite training sample from 
\begin_inset Formula $P_{\cx\times\cy}$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
A 
\series bold
population minimizer 
\series default
for a loss function is another name for the risk minimizer.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For the exponential loss 
\begin_inset Formula $\ell(m)=e^{-m}$
\end_inset

, the population minimizer is given by
\begin_inset Formula 
\[
f^{*}(x)=\frac{1}{2}\ln\frac{\pr(Y=1\mid X=x)}{\pr(Y=-1\mid X=x)}
\]

\end_inset


\end_layout

\begin_layout Itemize
(Short proof in KPM 16.4.1)
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
By solving for 
\begin_inset Formula $\pr(Y=1\mid X=x)$
\end_inset

, we can give probabilistic predictions from AdaBoost as well.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Population Minimizers
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
AdaBoost has the robustness issue because of the exponential loss.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Logistic loss 
\begin_inset Formula $\ell(m)=\ln\left(1+e^{-m}\right)$
\end_inset

 has the same population minimizer.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
But works better with high label noise or high Bayes error rate
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
In practice we use regularized logistic regressoin  how do we adjust to
 map 
\begin_inset Formula $\hat{f}(x)$
\end_inset

 to a probability in this case?
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Population minimizer of SVM hinge loss is
\begin_inset Formula 
\[
f^{*}(x)=\sign\left[\pr\left(Y=1\mid X=x\right)-\frac{1}{2}\right].
\]

\end_inset


\end_layout

\begin_layout Itemize
Because of the 
\begin_inset Formula $\sign$
\end_inset

, we cannot solve for the probabilities.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\end_body
\end_document
