#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty


% Set Color ==============================
\definecolor{NYUPurple}{RGB}{87,6,140}
\definecolor{LightPurple}{RGB}{165,11,255}


\setbeamercolor{title}{fg=NYUPurple}
%\setbeamercolor{frametitle}{fg=NYUPurple}
\setbeamercolor{frametitle}{fg=NYUPurple}

\setbeamercolor{background canvas}{fg=NYUPurple, bg=white}
\setbeamercolor{background}{fg=black, bg=NYUPurple}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=gray!20!white, bg=NYUPurple}

\setbeamertemplate{headline}{}

\setbeamercolor{parttitle}{fg=NYUPurple}
\setbeamercolor{sectiontitle}{fg=NYUPurple}
\setbeamercolor{sectionname}{fg=NYUPurple}
\setbeamercolor{section page}{fg=NYUPurple}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
\setbeamercolor{section title}{fg=NYUPurple}
 \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\usebeamercolor[fg]{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\end_preamble
\options aspectratio=169,handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "allcolors=NYUPurple,urlcolor=LightPurple"
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\boxbgcolor #ff31d8
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\dom}{\textrm{\textbf{dom} }}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\rank}{\text{\textbf{rank }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\conv}{\textrm{\textbf{conv} }}
\end_inset


\begin_inset FormulaMacro
\newcommand{\relint}{\text{\textbf{relint }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\aff}{\text{\textbf{aff }}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\err}{\text{err}}
\end_inset


\end_layout

\begin_layout Title
Adaboost
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David S.
 Rosenberg 
\end_layout

\begin_layout Date
April 16, 2019
\end_layout

\begin_layout Institute
CDS, NYU
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Just in article version
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Plots courtesy of Ningshan Zhang.}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Currently using 
\begin_inset Formula $G_{m}$
\end_inset

 instead of 
\begin_inset Formula $h_{m}$
\end_inset

 to refer to base classifier.
 This is inconsistent with usual notation, but is done to align with HTF.
 Would prefer to change formulation to align with more traditional approach
 in boosting literature (e.g.
 freund & schapire's book)...
 will clean this up when that transformation is made.
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Wondering if we should extend the plots for error >.5? This would be a really
 annoying hypothesis space that was always worse than random..
 But they would enter the ensemble with a negative weight.
 Might should at least point this out...
 because it's only a nonnegative combination if error is always <=.5.
 QUESTION: Does AdaBoost work on training data if error is bounded away
 from 0.5?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Notes from Zhou's book - Section 2.4: Freund and Schapire (1997) showed 
\begin_inset Quotes eld
\end_inset

generalization error of AdaBoost is upper bounded by
\begin_inset Quotes erd
\end_inset

 a term that grows with the square root of the VC-dimension of the base
 hypothesis space and the number of learning rounds, and decreases with
 the square root of m, the number of training examples.
 
\begin_inset Quotes eld
\end_inset

Empirical studies, however, show that AdaBoost often does not overfit; that
 is, the test error often tends to decrease even after the training eror
 reaches zero, even after a large number of rounds such as 1000...
 explaining why AdaBoost seems resistant to overfitting becomes one of the
 central theoretical issues and has attracted much attention..
 [then there's the margin story, which seems to be overturned by Breiman's
 work with arc-gv, but 7 years later is finally explained away by Reyzin
 and Schapire [2006].
 turns out the margin distribution of AdaBoost is better than that of arc-gv...
 (i.e.
 it's not just about the worst margin, but the distribution of margins).
 
\begin_inset Quotes eld
\end_inset

Gao and Zhou (2012) gave a new generalization error bound based on empirical
 Bernstein inequality ...
 that further defends the margin-based explanation against Breiman's doubts.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout
A statistical view of boosting developed that AdaBoost is just an optimization
 process that tries to fit an additive model to some surrogate loss function.
 LogitBoost uses log-loss, which should an equivalent probabilistic classifier
 asymptotically.
 But if it's really just an optimization of a loss function, there are other
 ways to minimize the same loss function using an additive model of weak
 learners.
 Resulting algorithm from one line of work is LPBoost, whcih doesn't seem
 to improve on adaboost....
 there seems to be something important in HOW the function is being fit
\end_layout

\end_inset

 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
ryan tibshirani's slides have good note on trees with weighted data and
 variable importance: http://www.stat.cmu.edu/~ryantibs/datamining/lectures/25-boos
t.pdf
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
At first seems funny that reweighting only accounts for misclassifications
 of the most recent classifier.
 But this is a peculiarity of the exponential loss and it wouldn't be that
 way for other losses.
 â€“ Indeed, the reweightings are also proportional to the exponential loss
 [NOT weighted exponential loss] of the previous full ensemble's score function
 (i.e.
 without the sign( )).
 
\end_layout

\end_inset


\end_layout

\begin_layout Section
Boosting Introduction
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Ensembles: Parallel vs Sequential
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Ensemble methods combine multiple models 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Parallel ensembles
\series default
: each model is built independently
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
e.g.
 bagging and random forests
\end_layout

\begin_layout Itemize
Main Idea: Combine many (high complexity, low bias) models to reduce variance
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize

\series bold
Sequential ensembles
\series default
: 
\end_layout

\begin_deeper
\begin_layout Itemize
Models are generated sequentially
\end_layout

\begin_layout Itemize
Try to add new models that do well where previous models lack
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Overview
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
AdaBoost algorithm
\end_layout

\begin_deeper
\begin_layout Itemize
weighted training sets and weighted classification error
\end_layout

\end_deeper
\begin_layout Itemize
AdaBoost minimizes training error
\end_layout

\begin_layout Itemize
AdaBoost train/test learning curves (seems resistant to overfitting)
\end_layout

\begin_layout Itemize
(If time) AdaBoost is minimizing exponential loss function but in a special
 way (forward stagewise additive modeling)
\begin_inset Note Note
status open

\begin_layout Plain Layout
(If time) High-level sketch of the quest to understand why AdaBoost works.
\end_layout

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Next week 
\end_layout

\begin_deeper
\begin_layout Itemize
Gradient Boosting (generalizes beyond exponential loss function)
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Boosting Question: Weak Learners
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
A 
\series bold
weak learner 
\series default
is a classifier that does slightly better than random.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Weak learners are like 
\begin_inset Quotes eld
\end_inset

rules of thumb
\begin_inset Quotes erd
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
If an email has 
\begin_inset Quotes eld
\end_inset

Viagra
\begin_inset Quotes erd
\end_inset

 in it, more likely than not it's spam.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Email from a friend is probably not spam.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
A linear decision boundary.
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
Can we extract wisdom from a committee of fools?
\end_layout

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can we 
\series bold
combine
\series default
 a set of weak classifiers to form single classifier that makes accurate
 predictions?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Posed by Kearns and Valiant (1988,1989): 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Yes! 
\series bold
Boosting
\series default
 solves this problem.
 [Rob Schapire (1990).]
\begin_inset Note Note
status open

\begin_layout Plain Layout
Pronounced: Sha pee ree (according to Berk Kapicioglu) 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
(We mention 
\begin_inset Quotes eld
\end_inset

weak learners
\begin_inset Quotes erd
\end_inset

 for historical context, but we'll avoid this terminology and associated
 assumptions...)
\end_layout

\end_deeper
\begin_layout Section
AdaBoost: The Algorithm
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Called AdaBoost, for adaptive boosting, because we don't need to know the
 
\begin_inset Quotes eld
\end_inset

edge
\begin_inset Quotes erd
\end_inset

 of each weak learner.
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Setting
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
AdaBoost is for 
\series bold
binary classification:
\series default
 
\begin_inset Formula $\cy=\left\{ -1,1\right\} $
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Base hypothesis space
\series default
 
\begin_inset Formula $\ch=\left\{ h:\cx\to\left\{ -1,1\right\} \right\} $
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Note
\series default
: not producing a score, but an actual class label.
\end_layout

\begin_layout Itemize
we'll call it a 
\series bold
base learner
\end_layout

\begin_layout Itemize
(when base learner satisfies certain conditions, it's called a 
\begin_inset Quotes eld
\end_inset

weak learner
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Typical base hypothesis spaces:
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Decision stumps
\series default
 (tree with a single split)
\end_layout

\begin_layout Itemize
Trees with few terminal nodes
\end_layout

\begin_layout Itemize
Linear decision functions
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Weighted Training Set
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Training set 
\begin_inset Formula $\cd=\left(\left(x_{1},y_{1}\right),\ldots,\left(x_{n},y_{n}\right)\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Weights 
\begin_inset Formula $\left(w_{1},\ldots,w_{n}\right)$
\end_inset

 associated with each example.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Weighted empirical risk
\series default
:
\begin_inset Formula 
\[
\hat{R}_{n}^{w}(f)=\frac{1}{W}\sum_{i=1}^{n}w_{i}\ell\left(f(x_{i}),y_{i}\right)\quad\mbox{where}\;W=\sum_{i=1}^{n}w_{i}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can often minimize weighted empirical risk directly
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What if model cannot conveniently be trained to reweighted data?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can sample a new data set from 
\begin_inset Formula $\cd$
\end_inset

 with probabilities
\begin_inset Formula $\left(w_{1}/W,\ldots w_{n}/W\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost - Rough Sketch
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Training set 
\begin_inset Formula $\cd=\left(\left(x_{1},y_{1}\right),\ldots,\left(x_{n},y_{n}\right)\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Start with equal weight on all training points 
\begin_inset Formula $w_{1}=\cdots=w_{n}=1$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Repeat for 
\begin_inset Formula $m=1,\ldots,M$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Find base classifier 
\begin_inset Formula $G_{m}(x)$
\end_inset

 that 
\series bold
tries
\series default
 to fit weighted training data (but may not do that well)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Increase weight on the points 
\begin_inset Formula $G_{m}(x)$
\end_inset

 misclassifies 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
So far, we've generated 
\begin_inset Formula $M$
\end_inset

 classifiers: 
\begin_inset Formula $G_{1},\ldots,G_{M}:\cx\to\left\{ -1,1\right\} $
\end_inset

.
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
Final classifier is 
\begin_inset Formula $G(x)=\sign\left[\sum_{m=1}^{M}\alpha_{m}G_{m}(x)\right]$
\end_inset

, for some weights 
\begin_inset Formula $\alpha_{m}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Schematic
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/adaboostSchematic.png
	lyxscale 60
	height 70theight%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From ESL Figure 10.1}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost - Rough Sketch
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Training set 
\begin_inset Formula $\cd=\left\{ \left(x_{1},y_{1}\right),\ldots,\left(x_{n},y_{n}\right)\right\} $
\end_inset

.
\end_layout

\begin_layout Itemize
Start with equal weight on all training points 
\begin_inset Formula $w_{1}=\cdots=w_{n}=1$
\end_inset

.
\end_layout

\begin_layout Itemize
Repeat for 
\begin_inset Formula $m=1,\ldots,M$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
Base learner fits weighted training data and returns 
\begin_inset Formula $G_{m}(x)$
\end_inset


\end_layout

\begin_layout Itemize
Increase weight on the points 
\begin_inset Formula $G_{m}(x)$
\end_inset

 misclassifies 
\end_layout

\end_deeper
\begin_layout Itemize
Final prediction 
\begin_inset Formula $G(x)=\sign\left[\sum_{m=1}^{M}\alpha_{m}G_{m}(x)\right]$
\end_inset

.
 (recall 
\begin_inset Formula $G_{m}(x)\in\left\{ -1,1\right\} $
\end_inset

)
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $\alpha_{m}$
\end_inset

's are nonnegative,
\end_layout

\begin_deeper
\begin_layout Itemize
larger when 
\begin_inset Formula $G_{m}$
\end_inset

 fits its weighted 
\begin_inset Formula $\cd$
\end_inset

 well
\end_layout

\begin_layout Itemize
smaller when 
\begin_inset Formula $G_{m}$
\end_inset

 fits weighted 
\begin_inset Formula $\cd$
\end_inset

 less well
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Important: Each 
\begin_inset Formula $G_{m}(x)$
\end_inset

 produces a class 
\begin_inset Formula $\left\{ -1,1\right\} $
\end_inset

, not a score.
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Adaboost: Weighted Classification Error
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In round 
\begin_inset Formula $m$
\end_inset

, base learner gets a weighted training set.
\end_layout

\begin_deeper
\begin_layout Itemize
Returns a base classifier 
\begin_inset Formula $G_{m}(x)$
\end_inset

 that minimizes weighted 
\begin_inset Formula $0-1$
\end_inset

 error.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
The 
\series bold
weighted 0-1 error
\series default
 of 
\begin_inset Formula $G_{m}(x)$
\end_inset

 is
\begin_inset Formula 
\[
\mbox{err}_{m}=\frac{1}{W}\sum_{i=1}^{n}w_{i}\ind{y_{i}\neq G_{m}(x_{i})}\quad\text{where }W=\sum_{i=1}^{n}w_{i}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Notice: 
\begin_inset Formula $\text{err}_{m}\in[0,1]$
\end_inset

.
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
We treat the base learner as a black box.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
We can use any base hypothesis space and base learner (e.g.
 SVM, trees, etc.)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
BUT, for things to work, we need at least 
\begin_inset Formula $\err_{m}<0.5$
\end_inset

 (the 
\begin_inset Quotes eld
\end_inset

weak-learner
\begin_inset Quotes erd
\end_inset

 hypothesis) 
\end_layout

\end_deeper
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Classifier Weights
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Note Note
status open

\begin_layout Plain Layout
Next year, consider multiplying these classifier weights by 
\begin_inset Formula $\frac{1}{2}$
\end_inset

 so that they are the same thing you get when you try to minimize exponential
 loss.
 Now it's slightly confusing.
 You got two different AdaBoost 
\begin_inset Quotes eld
\end_inset

score
\begin_inset Quotes erd
\end_inset

 functions...
 doesn't matter if you're just taking the sign of it, but matters if you're
 trying to produce conditional probabilities.
 But multiplying the weights by 
\begin_inset Formula $\frac{1}{2}$
\end_inset

 puts a square in the weight update.
 Really, I should just switch to Schapire's notation and be done with HTF's
 notation.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The weight of classifier 
\begin_inset Formula $G_{m}(x)$
\end_inset

 is 
\begin_inset Formula $\alpha_{m}=\ln\left(\frac{1-\text{err}_{m}}{\text{err}_{m}}\right).$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/adaboostAlphaVsError.png
	lyxscale 40
	height 60theight%
	clip

\end_inset


\end_layout

\begin_layout Itemize
Higher weighted error 
\begin_inset Formula $\implies$
\end_inset

 lower weight
\begin_inset Note Note
status open

\begin_layout Plain Layout
Note that weight 
\begin_inset Formula $\alpha_{m}\to0$
\end_inset

 as weighted error 
\begin_inset Formula $\err_{m}\to0.5$
\end_inset

 (random guessing).
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
If we continued the x-axis to 1 instead of stopping at 
\begin_inset Formula $0$
\end_inset

, we'd see the alpha weights going negative.
 So adaboost will flip the sign of our classifier for us if we're doing
 worse than random.
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Example Reweighting
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
We train 
\begin_inset Formula $G_{m}$
\end_inset

 to minimize weighted error, and it achieves 
\begin_inset Formula $\err_{m}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then 
\begin_inset Formula $\alpha_{m}=\ln\left(\frac{1-\text{err}_{m}}{\text{err}_{m}}\right)$
\end_inset

 is the weight of 
\begin_inset Formula $G_{m}$
\end_inset

 in final ensemble.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $w_{i}$
\end_inset

 is weight of example 
\begin_inset Formula $i$
\end_inset

 before training:
\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $G_{m}$
\end_inset

 classfies 
\begin_inset Formula $x_{i}$
\end_inset

 correctly, then 
\begin_inset Formula $w_{i}$
\end_inset

 is unchanged.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Otherwise, 
\begin_inset Formula $w_{i}$
\end_inset

 is increased as 
\begin_inset Formula 
\begin{eqnarray*}
w_{i} & \gets & w_{i}e^{\alpha_{m}}\\
\pause & = & w_{i}\left(\frac{1-\text{err}_{m}}{\text{err}_{m}}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For 
\begin_inset Formula $\mbox{err}_{m}<0.5$
\end_inset

, this always increases the weight.
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
Weight increase much larger for classifiers that are generally doing well
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Adaboost: Example Reweighting
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Any misclassified point has weight adjusted as 
\begin_inset Formula $w_{i}\gets w_{i}\left(\frac{1-\text{err}_{m}}{\text{err}_{m}}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/adaboostWeightUpdate.png
	lyxscale 40
	height 60theight%
	clip

\end_inset


\end_layout

\begin_layout Itemize
The smaller 
\begin_inset Formula $\err_{m}$
\end_inset

, the more we increase weight of misclassified points.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Algorithm
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Given training set 
\begin_inset Formula $\cd=\left\{ \left(x_{1},y_{1}\right),\ldots,\left(x_{n},y_{n}\right)\right\} $
\end_inset

.
\end_layout

\begin_layout Enumerate
Initialize observation weights 
\begin_inset Formula $w_{i}=1$
\end_inset

, 
\begin_inset Formula $i=1,2,\ldots,n$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
For 
\begin_inset Formula $m=1$
\end_inset

 to 
\begin_inset Formula $M$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Enumerate
Base learner fits weighted training data and returns 
\begin_inset Formula $G_{m}(x)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Compute 
\series bold
weighted empirical 0-1 risk
\series default
:
\begin_inset Formula 
\[
\mbox{err}_{m}=\frac{1}{W}\sum_{i=1}^{n}w_{i}\ind{y_{i}\neq G_{m}(x_{i})}\quad\text{where }W=\sum_{i=1}^{n}w_{i}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Compute 
\begin_inset Formula $\alpha_{m}=\ln\left(\frac{1-\text{err}_{m}}{\text{err}_{m}}\right)$
\end_inset

 [
\series bold
classifier weight
\series default
]
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Set 
\begin_inset Formula $w_{i}\gets w_{i}\cdot\exp\left[\alpha_{m}\ind{y_{i}\neq G_{m}(x_{i})}\right],\quad i=1,2,\ldots,n$
\end_inset

 [
\series bold
example weight adjustment
\series default
]
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
Ouptut 
\begin_inset Formula $G(x)=\sign\left[\sum_{m=1}^{M}\alpha_{m}G_{m}(x)\right]$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost with Decision Stumps
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
After 1 round:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Figures/boosting/fig16.10a.pdf
	lyxscale 70
	height 50theight%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\size footnotesize
Plus size represents weight.
 Blackness represents score for red class.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{KPM Figure 16.10}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost with Decision Stumps
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
After 3 rounds:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Figures/boosting/fig16.10b.pdf
	lyxscale 70
	height 50theight%
	clip

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\size footnotesize
Plus size represents weight.
 Blackness represents score for red class.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{KPM Figure 16.10}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost with Decision Stumps
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
After 120 rounds:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Figures/boosting/fig16.10c.pdf
	lyxscale 70
	height 50theight%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\size footnotesize
Plus size represents weight.
 Blackness represents score for red class.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{KPM Figure 16.10}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Section
Does AdaBoost Minimize Training Error?
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Does it actually minimize training error?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Methods we've seen so far come in two categories:
\end_layout

\begin_deeper
\begin_layout Itemize
Regularized empirical risk minimization (L1/L2 regression, SVM, kernelized
 versions)
\end_layout

\begin_layout Itemize
Trees
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
GD and SGD converge to minimizers of convex objective function on training
 data
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Trees achieve 0 training error unless same input occurs with different outputs
 
\end_layout

\begin_deeper
\begin_layout Itemize
if we don't limit tree complexity
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
So far, AdaBoost is just an algorithm.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Does an AdaBoost classifier 
\begin_inset Formula $G(x)$
\end_inset

 even minimize training error?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Yes, if our base classifiers have an 
\begin_inset Quotes eld
\end_inset


\series bold
edge
\series default

\begin_inset Quotes erd
\end_inset

 over random.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Does it actually minimize training error?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Assume base classifier, 
\begin_inset Formula $G_{m}(x)$
\end_inset

 has 
\begin_inset Formula $\err_{m}\le\frac{1}{2}$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize
(Otherwise, let 
\begin_inset Formula $G_{m}(x)\gets-G_{m}(x)$
\end_inset

.)
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Define the 
\series bold
edge 
\series default
of classifier 
\begin_inset Formula $G_{m}(x)$
\end_inset

 at round 
\begin_inset Formula $m$
\end_inset

 to be 
\begin_inset Formula 
\[
\gamma_{m}=\frac{1}{2}-\err_{m}.
\]

\end_inset


\end_layout

\begin_layout Itemize
Measures how much better than random 
\begin_inset Formula $G_{m}$
\end_inset

 performs.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Does it actually minimize training error?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
The empirical 0-1 risk of the AdaBoost classifier 
\begin_inset Formula $G(x)$
\end_inset

 is bounded as
\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}\ind{y_{i}\neq G(x)}\le\prod_{m=1}^{M}\sqrt{1-4\gamma_{m}^{2}}.
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
What's the range of possible values for 
\begin_inset Formula $\sqrt{1-4\gamma_{m}^{2}}$
\end_inset

?.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Proof may be an optional homework problem.
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Formula $\le\exp\left(-2\sum_{m=1}^{M}\gamma_{m}^{2}\right)$
\end_inset

 since
\begin_inset Formula 
\begin{eqnarray*}
\sqrt{1-4\gamma_{m}^{2}} & = & \left(1-4\gamma_{m}^{2}\right)^{\frac{1}{2}}\\
 & \le & \left(e^{-4\gamma_{m}^{2}}\right)^{\frac{1}{2}}=e^{-2\gamma_{m}^{2}}
\end{eqnarray*}

\end_inset

upposedly based on 
\begin_inset Formula $1+x\le e^{x}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{For more details, see the book 
\backslash
emph{Boosting: Foundations and Algorithms} by Schapire and Freund, Section
 3.1}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost: Does it actually minimize training error?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Suppose 
\begin_inset Formula $\err_{m}\le0.4$
\end_inset

 for all 
\begin_inset Formula $m$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Then the 
\begin_inset Quotes eld
\end_inset

edge
\begin_inset Quotes erd
\end_inset

 is 
\begin_inset Formula $\gamma_{m}=.5-.4=.1$
\end_inset

, and training error is bounded as follows:
\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}\ind{y_{i}\neq G(x)}\le\prod_{m=1}^{M}\sqrt{1-4(.1)^{2}}\approx\left(.98\right)^{M}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Bound decreases exponentially:
\begin_inset Formula 
\begin{eqnarray*}
.98^{100} & \approx & .133\\
.98^{200} & \approx & .018\\
.98^{300} & \approx & .002
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
With a consistent edge, training error decreases very quickly to 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{For more details, see the book 
\backslash
emph{Boosting: Foundations and Algorithms} by Schapire and Freund.}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Training Error Rate Curves
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/schapireFig3.1-training-error-curves.png
	lyxscale 25
	height 50theight%

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Base learner
\begin_inset Quotes erd
\end_inset

 plots error rates 
\begin_inset Formula $\text{err}_{M}$
\end_inset

 on weighted training sets after 
\begin_inset Formula $M$
\end_inset

 rounds of boosting
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Train error
\begin_inset Quotes erd
\end_inset

 is the training error of the combined classifier
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Theory bound
\begin_inset Quotes erd
\end_inset

 plots the training error bound given by the theorem
\begin_inset Note Note
status open

\begin_layout Plain Layout
: 
\begin_inset Formula $\prod_{m=1}^{M}\sqrt{1-4\left(\frac{1}{2}-\err_{m}\right)^{2}}.$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Figure 3.1 from 
\backslash
emph{Boosting: Foundations and Algorithms} by Schapire and Freund.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Section
Test Performance of Boosting
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Next year: leave the out unless I give more reasons or caveats.
 I guess the point of this is to say that AdaBoost isn't 
\begin_inset Quotes eld
\end_inset

just
\begin_inset Quotes erd
\end_inset

 minimizing an objective function.
 The way it does it is good too...?
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Typical Train / Test Learning Curves
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Might expect too many rounds of boosting to overfit:
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/typicalTrainTestCurve.png
	lyxscale 40
	height 65theight%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Rob Schapire's NIPS 2007 Boosting tutorial.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Learning Curves for AdaBoost
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In typical performance, AdaBoost is surprisingly resistant to overfitting.
\end_layout

\begin_layout Itemize
Test continues to improve even after training error is zero!
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/actualTrainTestCurves.png
	lyxscale 40
	height 65theight%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Rob Schapire's NIPS 2007 Boosting tutorial.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Possible point of confusion: once we hit 0 training error, does the algorithm
 stop, or does anything blow up? No â€” things blow up if we get a base classifier
 with 0 error on the weighted training set.
 But that's not what we're talking about here.
 We're talking about the combined classifier having 0 training error.
 At each round, we'll still have weights on all our training examples.
 We can view the points being reweighed either by the weighed error, or
 by the exponential loss of the combined classifier, neither of which is
 likely to blow up.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Adaboost Fits an Additive Model 
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Adaptive Basis Function Model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
AdaBoost produces a classification score function of the form 
\begin_inset Formula 
\[
\sum_{m=1}^{M}\alpha_{m}G_{m}(x)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
each 
\begin_inset Formula $G_{m}$
\end_inset

 is a 
\series bold
base classifier
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
The 
\begin_inset Formula $G_{m}$
\end_inset

's are like basis functions, but they are learned from the data.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let's move beyond classification models...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Adaptive Basis Function Model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Base hypothesis space
\series default
 
\begin_inset Formula $\ch$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
An 
\series bold
adaptive basis function expansion 
\series default
over 
\begin_inset Formula $\ch$
\end_inset

 is
\begin_inset Formula 
\[
f(x)=\sum_{m=1}^{M}\nu_{m}h_{m}(x),
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h_{m}\in\ch$
\end_inset

 chosen in a learning process (
\begin_inset Quotes eld
\end_inset

adaptive
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Itemize
\begin_inset Formula $\nu_{m}\in\reals$
\end_inset

 are 
\series bold
expansion coefficients.

\series default
 
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Note: 
\series default
We are taking linear combination of outputs of 
\begin_inset Formula $h_{m}(x)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Functions in 
\begin_inset Formula $h_{m}\in\ch$
\end_inset

 must produce values in 
\begin_inset Formula $\reals$
\end_inset

 (or a vector space) 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
How to fit an adaptive basis function model?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Loss function
\series default
: 
\begin_inset Formula $\ell(y,\hat{y})$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Base hypothesis space
\series default
: 
\begin_inset Formula $\ch$
\end_inset

 of 
\series bold
real-valued
\series default
 functions
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Want to find
\begin_inset Formula 
\[
f(x)=\sum_{m=1}^{M}\nu_{m}h_{m}(x)
\]

\end_inset

that 
\series bold
minimizes empirical risk
\series default

\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}\ell\left(y_{i},f(x_{i})\right).
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We'll proceed in stages, adding a new 
\begin_inset Formula $h_{m}$
\end_inset

 in every stage.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Forward Stagewise Additive Modeling (FSAM)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Start with 
\begin_inset Formula $f_{0}\equiv0$
\end_inset

.
\end_layout

\begin_layout Itemize
After 
\begin_inset Formula $m-1$
\end_inset

 stages, we have
\begin_inset Formula 
\[
f_{m-1}=\sum_{i=1}^{m-1}\nu_{i}h_{i},
\]

\end_inset

where 
\begin_inset Formula $h_{1},\ldots,h_{m-1}\in\ch$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Want to find 
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
step direction
\series default
 
\begin_inset Formula $h_{m}\in\ch$
\end_inset

 and
\end_layout

\begin_layout Itemize

\series bold
step size 
\series default

\begin_inset Formula $\nu_{i}>0$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
So that 
\begin_inset Formula 
\[
f_{m}=f_{m-1}+\nu_{i}h_{m}
\]

\end_inset

reduces empirical risk as much as possible.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Forward Stagewise Additive Modeling
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Initialize 
\begin_inset Formula $f_{0}(x)=0$
\end_inset

.
\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $m=1$
\end_inset

 to 
\begin_inset Formula $M$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Enumerate
Compute:
\begin_inset Formula 
\[
\left(\nu_{m},h_{m}\right)=\argmin_{\nu\in\reals,h\in\ch}\sum_{i=1}^{n}\ell\left(y_{i},f_{m-1}(x_{i})\underbrace{+\nu h(x_{i})}_{\text{new piece}}\right).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Set 
\begin_inset Formula $f_{m}=f_{m-1}+\nu_{m}h$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
Return: 
\begin_inset Formula $f_{M}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Exponential Loss and AdaBoost
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Take loss function to be 
\begin_inset Formula 
\[
\ell(y,f(x))=\exp\left(-yf(x)\right).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\ch$
\end_inset

 be our base hypothesis space of classifiers 
\begin_inset Formula $h:\cx\to\left\{ -1,1\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then Forward Stagewise Additive Modeling (FSAM) reduces to AdaBoost!
\end_layout

\begin_deeper
\begin_layout Itemize
Possible homework problem (and see HTF Section 10.4).
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Only difference:
\end_layout

\begin_deeper
\begin_layout Itemize
AdaBoost gets whichever 
\begin_inset Formula $G_{m}$
\end_inset

 the base learner returns from 
\begin_inset Formula $\ch$
\end_inset

 â€“ no guarantees it's best in 
\begin_inset Formula $\ch$
\end_inset

.
\end_layout

\begin_layout Itemize
FSAM explicitly requires getting the best in 
\begin_inset Formula $\ch$
\end_inset


\begin_inset Formula 
\[
G_{m}=\argmin_{G\in\ch}\sum_{i=1}^{N}w_{i}^{(m)}\ind{y_{i}\neq G(x_{i})}
\]

\end_inset

 
\end_layout

\end_deeper
\end_deeper
\begin_layout Section
Robustness and AdaBoost
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Exponential Loss
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Note that exponential loss puts a very large weight on bad misclassifications.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/loss.Zero_One.Hinge.Logistic_Rescaled.Exponential.png
	lyxscale 25
	height 65theight%

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost / Exponential Loss: Robustness Issues
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
When Bayes error rate is high (e.g.
 
\begin_inset Formula $\pr\left(f^{*}(X)\neq Y\right)=0.25$
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
e.g.
 there's some intrinsic randomness in the label
\end_layout

\begin_layout Itemize
e.g.
 training examples with same input, but different classifications.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Best we can do is predict the most likely class for each 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Some training predictions 
\series bold
should be wrong
\series default
 (because example doesn't have majority class)
\end_layout

\begin_deeper
\begin_layout Itemize
AdaBoost / exponential loss puts a lot of focus on geting those right
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Empirically, AdaBoost has degraded performance in situations with 
\end_layout

\begin_deeper
\begin_layout Itemize
high Bayes error rate, or when there's 
\end_layout

\begin_layout Itemize
high 
\begin_inset Quotes eld
\end_inset


\series bold
label noise
\series default

\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Logistic loss performs better in settings with high Bayes error
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Quick Sketch of Theoretical Understanding of Boosting
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost Generalization Bound
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Freund and Schapire [1997] showed (roughly) that for 
\begin_inset Formula $f$
\end_inset

 chosen by AdaBoost,
\begin_inset Formula 
\[
R(f)\le\hat{R}_{n}(f)+\tilde{O}\left(\sqrt{\frac{dT}{m}}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $d$
\end_inset

 measures the complexity of the base hypothesis space (VC-dimension)
\end_layout

\begin_layout Itemize
\begin_inset Formula $T$
\end_inset

 is the number of rounds of boosting
\end_layout

\begin_layout Itemize
\begin_inset Formula $m$
\end_inset

 is the size of our training set
\end_layout

\begin_layout Plain Layout
Bound suggests that we may overfit as number of rounds 
\begin_inset Formula $T$
\end_inset

 increases, but this not observed.
\end_layout

\end_deeper
\begin_layout Plain Layout
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost Generalization Bound, with Margins
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Freund and Schapire [1997] showed (roughly) that for 
\begin_inset Formula $f$
\end_inset

 chosen by AdaBoost,
\begin_inset Formula 
\[
R(f)\le\hat{R}_{n}(f)+\tilde{O}\left(\sqrt{\frac{dT}{m}}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $d$
\end_inset

 measures the complexity of the base hypothesis space (VC-dimension)
\end_layout

\begin_layout Itemize
\begin_inset Formula $T$
\end_inset

 is the number of rounds of boosting
\end_layout

\begin_layout Itemize
\begin_inset Formula $m$
\end_inset

 is the size of our training set
\end_layout

\begin_layout Plain Layout
Bound suggests that we may overfit as number of rounds 
\begin_inset Formula $T$
\end_inset

 increases, but this not observed.
\end_layout

\end_deeper
\end_inset


\begin_inset Note Note
status open

\begin_layout Section
Population Minimizer
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Population Minimizers
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In traditional statistics, the 
\series bold
population 
\series default
refers to
\end_layout

\begin_deeper
\begin_layout Itemize
the full population of a group, rather than a sample.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
In machine learning, the 
\series bold
population case 
\series default
is the hypothetical case of
\end_layout

\begin_deeper
\begin_layout Itemize
an infinite training sample from 
\begin_inset Formula $P_{\cx\times\cy}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
A 
\series bold
population minimizer 
\series default
for a loss function is another name for the risk minimizer.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For the exponential loss 
\begin_inset Formula $\ell(m)=e^{-m}$
\end_inset

, the population minimizer is given by
\begin_inset Formula 
\[
f^{*}(x)=\frac{1}{2}\ln\frac{\pr(Y=1\mid X=x)}{\pr(Y=-1\mid X=x)}
\]

\end_inset


\end_layout

\begin_layout Itemize
(Short proof in KPM 16.4.1)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
By solving for 
\begin_inset Formula $\pr(Y=1\mid X=x)$
\end_inset

, we can give probabilistic predictions from AdaBoost as well.
 
\end_layout

\end_deeper
\begin_layout Plain Layout
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Population Minimizers
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
AdaBoost has the robustness issue because of the exponential loss.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Logistic loss 
\begin_inset Formula $\ell(m)=\ln\left(1+e^{-m}\right)$
\end_inset

 has the same population minimizer.
\end_layout

\begin_deeper
\begin_layout Itemize
But works better with high label noise or high Bayes error rate
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
In practice we use regularized logistic regressoin â€“ how do we adjust to
 map 
\begin_inset Formula $\hat{f}(x)$
\end_inset

 to a probability in this case?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Population minimizer of SVM hinge loss is
\begin_inset Formula 
\[
f^{*}(x)=\sign\left[\pr\left(Y=1\mid X=x\right)-\frac{1}{2}\right].
\]

\end_inset


\end_layout

\begin_layout Itemize
Because of the 
\begin_inset Formula $\sign$
\end_inset

, we cannot solve for the probabilities.
 
\end_layout

\end_deeper
\end_inset


\end_layout

\end_body
\end_document
