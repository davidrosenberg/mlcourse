#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin /Users/drosen/Dropbox/repos/mlcourse/Lectures/
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty


\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\end_preamble
\options handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\dom}{\textrm{\textbf{dom} }}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\rank}{\text{\textbf{rank }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\conv}{\textrm{\textbf{conv} }}
\end_inset


\begin_inset FormulaMacro
\newcommand{\relint}{\text{\textbf{relint }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\aff}{\text{\textbf{aff }}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Title
Kernel Methods
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003 
\begin_inset Note Note
status open

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David Rosenberg 
\end_layout

\begin_layout Date
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
today
\end_layout

\end_inset


\end_layout

\begin_layout Institute
New York University
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Argyriou et al, "When is there a Representer Theorem? Vector Versus
 Matrix Regularizers"}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
This development is based on Shai Shalev-Shwartz and Shai Ben-David's 
\emph on
Understanding Machine Learning: From Theory to Algorithms
\emph default
.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Setup and Motivation
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Models
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
So far we've discussed
\end_layout

\begin_deeper
\begin_layout Itemize
Linear regression
\end_layout

\begin_layout Itemize
Ridge regression
\end_layout

\begin_layout Itemize
Lasso regression
\end_layout

\begin_layout Itemize
Support Vector Machines
\end_layout

\begin_layout Itemize
Perceptrons
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Each of these methods assumes
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx$
\end_inset

.
\end_layout

\begin_layout Itemize
Feature map 
\begin_inset Formula $\psi:\cx\to\reals^{d}$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Linear (or affine) hypothesis space:
\begin_inset Formula 
\[
\ch=\left\{ x\mapsto w^{T}\psi(x)\mid w\in\reals^{d}\right\} .
\]

\end_inset

 applicable when we use 
\begin_inset Formula $\ell_{2}$
\end_inset

 regularization.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Models Need Big Feature Space
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
To get 
\series bold
expressive
\series default
 hypothesis spaces using linear models, 
\end_layout

\begin_deeper
\begin_layout Itemize
need high-dimensional feature spaces
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
(What do we mean by expressive?)
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Very large feature spaces have two problems:
\end_layout

\begin_deeper
\begin_layout Enumerate
Overfitting
\end_layout

\begin_layout Enumerate
Memory and computational costs 
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Overfitting we handle with regularization.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Kernel methods can help with memory and computational costs.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
In practice, most applicable when we use 
\begin_inset Formula $\ell_{2}$
\end_inset

 regularization.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Some Methods Can Be 
\begin_inset Quotes eld
\end_inset

Kernelized
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A method is 
\series bold
kernelized 
\series default
if inputs only appear inside inner products: 
\begin_inset Formula $\left\langle \psi(x),\psi(y)\right\rangle $
\end_inset

 for 
\begin_inset Formula $x,y\in\cx$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The function 
\series bold
kernel function
\series default
 corresponding to 
\begin_inset Formula $\psi$
\end_inset

 is 
\begin_inset Formula 
\[
k(x,y)=\left\langle \psi(x),\psi(y)\right\rangle .
\]

\end_inset

 
\end_layout

\begin_layout Itemize
Can think of the kernel function as a 
\series bold
similarity score
\series default
.
\end_layout

\begin_deeper
\begin_layout Itemize
But this is not precise.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
There are many ways to design a similarity score.
 
\end_layout

\begin_deeper
\begin_layout Itemize
A kernel function is special because it's an inner product.
\end_layout

\begin_layout Itemize
Has many mathematical benefits.
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
What's the Benefit of Kernelization?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Computational.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Access to infinite-dimensional feature spaces.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Allows thinking in terms of 
\begin_inset Quotes eld
\end_inset

similarity
\begin_inset Quotes erd
\end_inset

 rather than features.
 (debatable)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Generalizing from SVM
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Soft-Margin SVM (no intercept)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The SVM objective function is
\begin_inset Formula 
\[
\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\left(1-y_{i}\left[w^{T}x_{i}\right]\right)_{+}.
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We found that the minimizer 
\begin_inset Formula $w^{*}\in\reals^{d}$
\end_inset

 has the form 
\begin_inset Formula 
\[
w^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}x_{i}.
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Representer Theorem
\series default
 
\begin_inset Formula $\implies$
\end_inset

 same result in a much broader context.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Introduce a Feature Map
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space: 
\begin_inset Formula $\cx$
\end_inset

 (no assumptions).
 
\end_layout

\begin_layout Itemize
Feature space: 
\begin_inset Formula $\ch$
\end_inset

 (a Hilbert space, usually 
\begin_inset Formula $\reals^{d}$
\end_inset

) .
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Feature map 
\begin_inset Formula $\psi:\cx\to\ch$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Featurized SVM objective: 
\begin_inset Formula 
\[
\min_{w\in\ch}\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\left(1-y_{i}\left[\left\langle w,\psi\left(x_{i}\right)\right\rangle \right]\right)_{+}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Now 
\begin_inset Formula $\|w\|^{2}=\left\langle w,w\right\rangle $
\end_inset

, where 
\begin_inset Formula $\left\langle \cdot,\cdot\right\rangle $
\end_inset

 is inner product for 
\begin_inset Formula $\ch$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Note that minimizer 
\begin_inset Formula $w^{*}\in\ch$
\end_inset

.
 What are predictions 
\begin_inset Formula $x\mapsto?$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generalize
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Featurized SVM objective: 
\begin_inset Formula 
\[
\min_{w\in\ch}\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\left(1-y_{i}\left[\left\langle w,\psi\left(x_{i}\right)\right\rangle \right]\right)_{+}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Generalized objective
\series default
: 
\begin_inset Formula 
\[
\min_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,\psi(x_{1})\right\rangle ,\ldots,\left\langle w,\psi(x_{n})\right\rangle \right),\pause
\]

\end_inset

where
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $R:\reals^{\ge0}\to\reals$
\end_inset

 is nondecreasing (
\series bold
Regularization term
\series default
)
\end_layout

\begin_layout Itemize
and 
\begin_inset Formula $L:\reals^{n}\to\reals$
\end_inset

 is arbitrary.
 (
\series bold
Loss term
\series default
)
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generalized Objective Function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Generalized objective
\series default
: 
\begin_inset Formula 
\[
\min_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,\psi(x_{1})\right\rangle ,\ldots,\left\langle w,\psi(x_{n})\right\rangle \right),
\]

\end_inset

where
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $R:\reals^{\ge0}\to\reals$
\end_inset

 is nondecreasing (
\series bold
Regularization term
\series default
), and
\end_layout

\begin_layout Itemize
\begin_inset Formula $L:\reals^{n}\to\reals$
\end_inset

 is arbitrary (
\series bold
Loss term
\series default
).
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Is ridge regression of this form? What is 
\begin_inset Formula $R(\cdot)$
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What if we penalize with 
\begin_inset Formula $\lambda\|w\|_{2}$
\end_inset

 instead of 
\begin_inset Formula $\lambda\|w\|_{2}^{2}$
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What if we use lasso regression? 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
The Representer Theorem
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status collapsed

\begin_layout Plain Layout
The Representer Theorem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Representer Theorem]
\end_layout

\end_inset

 Let 
\begin_inset Formula 
\[
J(w)=R\left(\|w\|\right)+L\left(\left\langle w,\psi(x_{1})\right\rangle ,\ldots,\left\langle w,\psi(x_{n})\right\rangle \right),
\]

\end_inset

where
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $R:\reals^{\ge0}\to\reals$
\end_inset

 is nondecreasing (
\series bold
Regularization term
\series default
), and
\end_layout

\begin_layout Itemize
\begin_inset Formula $L:\reals^{n}\to\reals$
\end_inset

 is arbitrary (
\series bold
Loss term
\series default
).
\end_layout

\end_deeper
\begin_layout Theorem
If 
\begin_inset Formula $J(w)$
\end_inset

 has a minimizer, then it has a minimizer of the form
\begin_inset Formula 
\[
w^{*}=\sum_{i=1}^{n}\alpha_{i}\psi(x_{i}).
\]

\end_inset


\end_layout

\begin_layout Standard
[If 
\begin_inset Formula $R$
\end_inset

 is strictly increasing, then all minimizers have this form.
 (homework)]
\end_layout

\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Frame

\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status collapsed

\begin_layout Plain Layout
The Representer Theorem (Proof)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Let 
\begin_inset Formula $w^{*}$
\end_inset

 be a minimizer.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Let 
\begin_inset Formula $M=\linspan\left(\psi(x_{1}),\ldots,\psi(x_{n})\right)$
\end_inset

.
 [the 
\series bold

\begin_inset Quotes eld
\end_inset

span of the data
\series default

\begin_inset Quotes erd
\end_inset

]
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Let 
\begin_inset Formula $w=\proj_{M}w^{*}$
\end_inset

.
 So 
\begin_inset Formula $\exists\alpha$
\end_inset

 s.t.
 
\begin_inset Formula $w=\sum_{i=1}^{n}\alpha_{i}\psi(x_{i})$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "enu:residual-is-orthogonal"

\end_inset

Then 
\begin_inset Formula $w^{\perp}:=w^{*}-w$
\end_inset

 is orthogonal to 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Projections decrease norms: 
\begin_inset Formula $\|w\|\le\|w^{*}\|$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Since 
\begin_inset Formula $R$
\end_inset

 is nondecreasing, 
\begin_inset Formula $R(\|w\|)\le R(\|w^{*}\|)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
By 
\begin_inset CommandInset ref
LatexCommand eqref
reference "enu:residual-is-orthogonal"

\end_inset

, 
\begin_inset Formula $\left\langle w^{*},\psi(x_{i})\right\rangle =\pause\left\langle w+w^{\perp},\psi(x_{i})\right\rangle =\pause\left\langle w,\psi(x_{i})\right\rangle $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset Formula $L\left(\left\langle w^{*},\psi(x_{1})\right\rangle ,\ldots,\left\langle w^{*},\psi(x_{n})\right\rangle \right)=L\left(\left\langle w,\psi(x_{1})\right\rangle ,\ldots,\left\langle w,\psi(x_{n})\right\rangle \right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset Formula $J(w)\le J(w^{*})$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Therefore 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $w=\sum_{i=1}^{n}\alpha_{i}\psi(x_{i})$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 is also a minimizer.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
Q.E.D.
\end_layout

\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Frame

\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Representer Theorem for Kernelization
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized Predictions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider 
\begin_inset Formula $w=\sum_{i=1}^{n}\alpha_{i}\psi\left(x_{i}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
How do we make predictions for a given 
\begin_inset Formula $x\in\cx$
\end_inset

?
\begin_inset Formula 
\begin{eqnarray*}
\pause f(x)=\left\langle w^{*},\psi(x)\right\rangle \pause & = & \left\langle \sum_{i=1}^{n}\alpha_{i}\psi\left(x_{i}\right),\psi(x)\right\rangle \\
\pause & = & \sum_{i=1}^{n}\alpha_{i}\left\langle \psi(x_{i}),\psi(x)\right\rangle \\
\pause & = & \sum_{i=1}^{n}\alpha_{i}k(x_{i},x)
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized Regularization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider 
\begin_inset Formula $w=\sum_{i=1}^{n}\alpha_{i}\psi\left(x_{i}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
What does 
\begin_inset Formula $R(\|w\|)$
\end_inset

 look like?
\begin_inset Formula 
\begin{eqnarray*}
\pause\|w\|^{2} & = & \left\langle w,w\right\rangle \\
 & = & \left\langle \sum_{i=1}^{n}\alpha_{i}\psi\left(x_{i}\right),\sum_{j=1}^{n}\alpha_{j}\psi\left(x_{j}\right)\right\rangle \\
\pause & = & \sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}\left\langle \psi(x_{i}),\psi(x_{j})\right\rangle \\
\pause & = & \sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}k(x_{i},x_{j})\pause
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
(You should recognize the last expression as a quadratic form.)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Matrix (a.k.a.
 Gram Matrix)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
The 
\series bold
kernel matrix
\series default
 for a kernel 
\begin_inset Formula $k$
\end_inset

 on a set 
\begin_inset Formula $\left\{ x_{1},\ldots,x_{n}\right\} $
\end_inset

 is
\begin_inset Formula 
\[
K=\begin{pmatrix}k(x_{i},x_{j})\end{pmatrix}_{i,j}=\begin{pmatrix}k(x_{1},x_{1}) & \cdots & k(x_{1},x_{n})\\
\vdots & \ddots & \cdots\\
k(x_{n},x_{1}) & \cdots & k(x_{n},x_{n})
\end{pmatrix}\in\reals^{n\times n}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Definition
This matrix is also known as the 
\series bold
Gram matrix
\series default
.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized Regularization: Matrix Form
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider 
\begin_inset Formula $w=\sum_{i=1}^{n}\alpha_{i}\psi\left(x_{i}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
What does 
\begin_inset Formula $R(\|w\|)$
\end_inset

 look like?
\begin_inset Formula 
\begin{eqnarray*}
\|w\|^{2} & = & \sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}k(x_{i},x_{j})\\
\pause & = & \alpha^{T}K\alpha
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
So 
\begin_inset Formula $R(\|w\|)=R\left(\sqrt{\alpha^{T}K\alpha}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized Predictions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Write 
\begin_inset Formula $f_{\alpha}(x)=\sum_{i=1}^{n}\alpha_{i}k(x_{i},x)$
\end_inset

.
\end_layout

\begin_layout Itemize
Predictions on the training points have a particulalry simple form: 
\begin_inset Formula 
\begin{eqnarray*}
\pause\begin{pmatrix}f_{\alpha}(x_{1})\\
\vdots\\
f_{\alpha}(x_{n})
\end{pmatrix} & = & \begin{pmatrix}\alpha_{1}k(x_{1},x_{1})+\cdots+\alpha_{n}k(x_{1},x_{n})\\
\vdots\\
\alpha_{1}k(x_{n},x_{1})+\cdots+\alpha_{n}k(x_{1,}x_{n})
\end{pmatrix}\\
\pause & = & \begin{pmatrix}k(x_{1},x_{1}) & \cdots & k(x_{1},x_{n})\\
\vdots & \ddots & \cdots\\
k(x_{n},x_{1}) & \cdots & k(x_{n},x_{n})
\end{pmatrix}\begin{pmatrix}\alpha_{1}\\
\vdots\\
\alpha_{n}
\end{pmatrix}\\
\pause & = & K\alpha
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized Objective
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Substituting 
\series bold

\begin_inset Formula 
\begin{eqnarray*}
w & = & \sum_{i=1}^{n}\alpha_{i}\psi\left(x_{i}\right)
\end{eqnarray*}

\end_inset


\series default
into generalized objective, we get 
\begin_inset Formula 
\[
\min_{\alpha\in\reals^{n}}R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
No direct access to 
\begin_inset Formula $\psi(x_{i})$
\end_inset

.
 
\end_layout

\begin_layout Itemize
All references are via kernel matrix 
\begin_inset Formula $K$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
(Assumes 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $L$
\end_inset

 do not hide any references to 
\begin_inset Formula $\psi(x_{i})$
\end_inset

.) 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
This is the 
\series bold
kernelized objective function.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized SVM 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The SVM objective:
\begin_inset Formula 
\[
\min_{w\in\ch}\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\left(1-y_{i}\left[\left\langle w,\psi\left(x_{i}\right)\right\rangle \right]\right)_{+}.
\]

\end_inset


\end_layout

\begin_layout Itemize
Kernelizing yields
\begin_inset Formula 
\[
\min_{\alpha\in\reals^{n}}\frac{1}{2}\alpha^{T}K\alpha+\frac{c}{n}\sum_{i=1}^{n}\left(1-y_{i}\left(K\alpha\right)_{i}\right)_{+}
\]

\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized Ridge Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Ridge Regression:
\begin_inset Formula 
\[
\min_{w\in\reals^{d}}\frac{1}{n}\sum_{i=1}^{n}\left(w^{T}x_{i}-y_{i}\right)^{2}+\lambda\|w\|^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
Featurized Ridge Regression
\begin_inset Formula 
\[
\min_{w\in\ch}\frac{1}{n}\sum_{i=1}^{n}\left(\left\langle w,\psi(x_{i})\right\rangle -y_{i}\right)^{2}+\lambda\|w\|^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
Kernelized Ridge Regression
\begin_inset Formula 
\[
\min_{\alpha\in\reals^{n}}\frac{1}{n}\|K\alpha-y\|^{2}+\lambda\alpha^{T}K\alpha,
\]

\end_inset

where 
\begin_inset Formula $y=\left(y_{1},\ldots,y_{n}\right)^{T}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Kernel Examples
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
SVM Dual
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Recall the SVM dual optimization problem
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\sup_{\alpha} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i}\\
\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
 & \quad & \alpha_{i}\in\left[0,\frac{c}{n}\right]\;i=1,\ldots,n.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Notice: 
\begin_inset Formula $x$
\end_inset

's only show up as inner products with other 
\begin_inset Formula $x$
\end_inset

's.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Can replace 
\begin_inset Formula $x_{j}^{T}x_{i}$
\end_inset

 by an arbitrary kernel 
\begin_inset Formula $k(x_{j},x_{i})$
\end_inset

.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
What kernel are we currently using?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space: 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Feature space: 
\begin_inset Formula $\ch=\reals^{d}$
\end_inset

, with standard inner product
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Feature map
\begin_inset Formula 
\[
\psi(x)=x.
\]

\end_inset


\end_layout

\begin_layout Itemize
Kernel: 
\begin_inset Formula 
\[
k(w,x)=w^{T}x
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Quadratic Kernel in 
\begin_inset Formula $\reals^{2}$
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space: 
\begin_inset Formula $\cx=\reals^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Feature space: 
\begin_inset Formula $\ch=\reals^{5}$
\end_inset


\end_layout

\begin_layout Itemize
Feature map:
\begin_inset Formula 
\[
\psi:(x_{1},x_{2})\mapsto\left(x_{1},x_{2},x_{1}^{2},x_{2}^{2},\sqrt{2}x_{1}x_{2}\right)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Gives us ability to represent conic section boundaries.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Define kernel as inner product in feature space:
\begin_inset Formula 
\begin{eqnarray*}
k(w,x) & = & \langle\psi(w),\psi(x)\rangle\\
\pause & = & w_{1}x_{1}+w_{2}x_{2}+w_{1}^{2}x_{1}^{2}+w_{2}^{2}x_{2}^{2}+2w_{1}w_{2}x_{1}x_{2}\\
\pause & = & w_{1}x_{1}+w_{2}x_{2}+(w_{1}x_{1})^{2}+(w_{2}x_{2})^{2}+2(w_{1}x_{1})(w_{2}x_{2})\\
\pause & = & \langle w,x\rangle+\langle w,x\rangle^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Based on Guillaume Obozinski's Statistical Machine Learning course
 at Louvain, Feb 2014.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Quadratic Kernel in 
\begin_inset Formula $\reals^{d}$
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Feature space: 
\begin_inset Formula $\ch=\reals^{D}$
\end_inset

, where 
\begin_inset Formula $D=d+{d \choose 2}\approx d^{2}/2$
\end_inset

.
\end_layout

\begin_layout Itemize
Feature map:
\begin_inset Formula 
\[
\ensuremath{\phi(x)=(x_{1},\ldots,x_{d},x_{1}^{2},\ldots,x_{d}^{2},\sqrt{2}x_{1}x_{2},\ldots,\sqrt{2}x_{i}x_{j},\ldots\sqrt{2}x_{d-1}x_{d})^{T}}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Still have
\begin_inset Formula 
\begin{eqnarray*}
k(w,x) & = & \left\langle \phi(w),\phi(x)\right\rangle \\
\pause & = & \left\langle x,y\right\rangle +\left\langle x,y\right\rangle ^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Computation for inner product with explicit mapping: 
\begin_inset Formula $O(d^{2})$
\end_inset


\end_layout

\begin_layout Itemize
Computation for implicit kernel calculation: 
\begin_inset Formula $O(d)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Based on Guillaume Obozinski's Statistical Machine Learning course
 at Louvain, Feb 2014.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
String Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Computer virus finding: want to find a sequence of characters 
\begin_inset Formula $v$
\end_inset

 in a file that indicates file contains a virus.
\end_layout

\begin_layout Itemize
Alphabet set 
\begin_inset Formula $\Sigma$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\cx$
\end_inset

 is set of all finite-length strings over 
\begin_inset Formula $\Sigma$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\cx_{d}$
\end_inset

 is set of strings over 
\begin_inset Formula $\Sigma$
\end_inset

 of length at most 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Itemize
Feature map - indiciator variable for every substring of length at most
 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Polynomial Kernel in 
\begin_inset Formula $\reals^{d}$
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Kernel function:
\begin_inset Formula 
\[
k(w,x)=\left(1+\left\langle w,x\right\rangle \right)^{M}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Corresponds to a feature map with all terms up to degree 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For any 
\begin_inset Formula $M$
\end_inset

, computing the kernel has same computational cost
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Cost of explicit inner product computation grows rapidly in 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Radial Basis Function (RBF) / Gaussian Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\begin_inset Formula 
\[
k(w,x)=\exp\left(-\frac{\|w-x\|^{2}}{2\sigma^{2}}\right),
\]

\end_inset

where 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is known as the bandwidth parameter.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Does it act like a similarity score?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Why 
\begin_inset Quotes eld
\end_inset

radial
\begin_inset Quotes erd
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Have we departed from our 
\begin_inset Quotes eld
\end_inset

inner product of feature vector
\begin_inset Quotes erd
\end_inset

 recipe?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Yes and no: corresponds to an infinite dimensional feature vector
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Probably the most common nonlinear kernel.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Prediction Functions with RBF Kernel
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
RBF Basis
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals$
\end_inset


\end_layout

\begin_layout Itemize
Output space: 
\begin_inset Formula $\cy=\reals$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
RBF kernel 
\begin_inset Formula $k(w,x)=\exp\left(-\left(w-x\right)^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Suppose we have 6 training examples: 
\begin_inset Formula $x_{i}\in\left\{ -6,-4,-3,0,2,4\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If representer theorem applies, then
\begin_inset Formula 
\[
f(x)=\sum_{i=1}^{6}\alpha_{i}k(x_{i},x).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $f$
\end_inset

 is a linear combination of 6 basis functions of form 
\begin_inset Formula $k(x_{i},\cdot)$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/kernels/kernel-fns-1.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset

 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
RBF Predictions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Basis functions
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/kernels/kernel-fns-1.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Predictions of the form
\begin_inset Formula 
\[
f(x)=\sum_{i=1}^{6}\alpha_{i}k(x_{i},x)
\]

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/kernels/kernel-fns-2.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If we have a kernelized algorithm with RBF kernel, prediction functions
 
\begin_inset Formula $x\mapsto\left\langle w,\psi(x)\right\rangle $
\end_inset

 will look this way.
\end_layout

\begin_deeper
\begin_layout Itemize
whether we got 
\begin_inset Formula $w$
\end_inset

 from SVM, ridge regression, etc...
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
When is 
\begin_inset Formula $k(x,w)$
\end_inset

 a kernel function? (Mercer's Theorem)
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
How to Get Kernels?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Explicitly construct 
\begin_inset Formula $\psi(x):\cx\to\reals^{d}$
\end_inset

 and define 
\begin_inset Formula $k(x,w)=\psi(x)^{T}\psi(w)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Directly define the kernel function 
\begin_inset Formula $k(x,w)$
\end_inset

, and verify it corresponds to 
\begin_inset Formula $\left\langle \psi(x),\psi(w)\right\rangle $
\end_inset

 for some 
\begin_inset Formula $\psi$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
There are many theorems to help us with the second approach
\end_layout

\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Positive Semidefinite Matrices
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A real, symmetric matrix 
\begin_inset Formula $M\in\reals^{n\times n}$
\end_inset

 is 
\series bold
positive semidefinite (psd)
\series default
 if for any 
\begin_inset Formula $x\in\reals^{n}$
\end_inset

, 
\begin_inset Formula 
\[
x^{T}Mx\ge0.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Theorem
The following conditions are each necessary and sufficient for 
\begin_inset Formula $M$
\end_inset

 to be positive semidefinite:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $M$
\end_inset

 has a 
\begin_inset Quotes eld
\end_inset

square root
\begin_inset Quotes erd
\end_inset

, i.e.
 there exists 
\begin_inset Formula $R$
\end_inset

 s.t.
 
\begin_inset Formula $M=R^{T}R$
\end_inset

.
\end_layout

\begin_layout Itemize
All eigenvalues of 
\begin_inset Formula $M$
\end_inset

 are greater than or equal to 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Positive Semidefinite Function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A symmetric kernel function 
\begin_inset Formula $k:\cx\times\cx\to\reals$
\end_inset

 is 
\series bold
positive semidefinite (psd)
\series default
 if for any finite set 
\begin_inset Formula $\left\{ x_{1},\ldots,x_{n}\right\} \in\cx$
\end_inset

, the kernel matrix on this set 
\begin_inset Formula 
\[
K=\begin{pmatrix}k(x_{i},x_{j})\end{pmatrix}_{i,j}=\begin{pmatrix}k(x_{1},x_{1}) & \cdots & k(x_{1},x_{n})\\
\vdots & \ddots & \cdots\\
k(x_{n},x_{1}) & \cdots & k(x_{n},x_{n})
\end{pmatrix}
\]

\end_inset

is a positive semidefinite matrix.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Mercer's Theorem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
A symmetric function 
\begin_inset Formula $k(w,x)$
\end_inset

 can be expressed as an inner product
\begin_inset Formula 
\[
k(w,x)=\left\langle \psi(w),\psi(x)\right\rangle 
\]

\end_inset

for some 
\begin_inset Formula $\psi$
\end_inset

 if and only if 
\begin_inset Formula $k(w,x)$
\end_inset

 is 
\series bold
positive semidefinite.
\begin_inset Note Note
status open

\begin_layout Theorem

\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Proof
[Sketch] Suppose 
\begin_inset Formula $k(w,x)$
\end_inset

 is psd.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
Let 
\end_layout

\end_deeper
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generating New Kernels from Old
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Suppose 
\begin_inset Formula $k,k_{1},k_{2}:\cx\times\cx\to\reals$
\end_inset

 are psd kernels.
 Then so are the following:
\begin_inset Formula 
\begin{eqnarray*}
k_{\mbox{new}}(w,x) & = & k_{1}(w,x)+k_{2}(w,x)\\
k_{\mbox{new}}(w,x) & = & \alpha k(w,x)\\
k_{\mbox{new}}(w,x) & = & f(w)f(x)\mbox{ for any function \ensuremath{f(x)}}\\
k_{\mbox{new}}(w,x) & = & k_{1}(w,x)k_{2}(w,x)
\end{eqnarray*}

\end_inset

 are also A symmetric function 
\begin_inset Formula $k(w,x)$
\end_inset

 can be expressed as an inner product
\begin_inset Formula 
\[
k(w,x)=\left\langle \phi(w),\phi(x)\right\rangle 
\]

\end_inset

for some 
\begin_inset Formula $\phi$
\end_inset

 if and only if 
\begin_inset Formula $k(w,x)$
\end_inset

 is 
\series bold
positive semidefinite.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
If we start with a psd kernel, can we generate more?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Additive Closure
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k_{1}$
\end_inset

 and 
\begin_inset Formula $k_{2}$
\end_inset

 are psd kernels with feature maps 
\begin_inset Formula $\phi_{1}$
\end_inset

 and 
\begin_inset Formula $\phi_{2}$
\end_inset

, respectively.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula 
\[
k_{1}(w,x)+k_{2}(w,x)
\]

\end_inset

is a psd kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Concatenate the feature vectors to get 
\begin_inset Formula 
\[
\phi(x)=\left(\phi_{1}(x),\phi_{2}(x)\right).
\]

\end_inset

Then 
\begin_inset Formula $\phi$
\end_inset

 is a feature map for 
\begin_inset Formula $k_{1}+k_{2}$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closure under Positive Scaling
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k$
\end_inset

 is a psd kernel with feature maps 
\begin_inset Formula $\phi$
\end_inset

.
\end_layout

\begin_layout Itemize
Then for any 
\begin_inset Formula $\alpha>0$
\end_inset

, 
\begin_inset Formula 
\[
\alpha k
\]

\end_inset

is a psd kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Note that 
\begin_inset Formula 
\[
\phi(x)=\sqrt{\alpha}\phi(x)
\]

\end_inset

 is a feature map for 
\begin_inset Formula $\alpha k$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Scalar Function Gives a Kernel
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For any function 
\begin_inset Formula $f(x)$
\end_inset

, 
\begin_inset Formula 
\[
k(w,x)=f(w)f(x)
\]

\end_inset

is a kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Let 
\begin_inset Formula $f(x)$
\end_inset

 be the feature mapping.
 (It maps into a 1-dimensional feature space.)
\begin_inset Formula 
\[
\left\langle f(x),f(w)\right\rangle =f(x)f(w)=k(w,x).
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closure under Hadamard Products
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k_{1}$
\end_inset

 and 
\begin_inset Formula $k_{2}$
\end_inset

 are psd kernels with feature maps 
\begin_inset Formula $\phi_{1}$
\end_inset

 and 
\begin_inset Formula $\phi_{2}$
\end_inset

, respectively.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula 
\[
k_{1}(w,x)k_{2}(w,x)
\]

\end_inset

is a psd kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Take the outer product of the feature vectors: 
\begin_inset Formula 
\[
\phi(x)=\phi_{1}(x)\left[\phi_{2}(x)\right]^{T}.
\]

\end_inset

Note that 
\begin_inset Formula $\phi(x)$
\end_inset

 is a matrix.
\end_layout

\begin_layout Itemize
Continued...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closure under Hadamard Products
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Then
\begin_inset Formula 
\begin{eqnarray*}
\left\langle \phi(x),\phi(w)\right\rangle  & = & \sum_{i,j}\phi(x)\phi(w)\\
 & = & \sum_{i,j}\left[\phi_{1}(x)\left[\phi_{2}(x)\right]^{T}\right]_{ij}\left[\phi_{1}(w)\left[\phi_{2}(w)\right]^{T}\right]_{ij}\\
 & = & \sum_{i,j}\left[\phi_{1}(x)\right]_{i}\left[\phi_{2}(x)\right]_{j}\left[\phi_{1}(w)\right]_{i}\left[\phi_{2}(w)\right]_{j}\\
 & = & \left(\sum_{i}\left[\phi_{1}(x)\right]_{i}\left[\phi_{1}(w)\right]_{i}\right)\left(\sum_{j}\left[\phi_{2}(x)\right]_{j}\left[\phi_{2}(w)\right]_{j}\right)\\
 & = & k_{1}(w,x)k_{2}(w,x)
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\end_body
\end_document
