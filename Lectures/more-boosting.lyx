#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin /Users/drosen/Dropbox/repos/mlcourse/Lectures/
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty


\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\end_preamble
\options handout,aspectratio=169
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\err}{\text{err}}
\end_inset


\end_layout

\begin_layout Title
Gradient Boosting, Continued
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003 
\begin_inset Note Note
status open

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David Rosenberg 
\end_layout

\begin_layout Institute
New York University
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Just in article version
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Plots courtesy of Ningshan Zhang.}}
\end_layout

\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Todo:
\end_layout

\begin_layout Plain Layout
What's the deal with stepsize? Do people actually do line search?
\end_layout

\begin_layout Plain Layout
It's mostly implemented just for trees anyway?
\end_layout

\begin_layout Plain Layout
1) expressivity / complexity of boosting decision stumps
\end_layout

\begin_layout Plain Layout
2) generalization bounds in terms of complexity of base hypothesis space
\end_layout

\begin_layout Plain Layout
3) Picture of fitting sinc with decision trees and GBM
\end_layout

\begin_layout Plain Layout
4) put in L2 boosting before general GBM boosting?
\end_layout

\begin_layout Plain Layout
5) Population minimizer of exponential loss
\end_layout

\begin_layout Plain Layout
6) adaboost is special case of stagewise
\end_layout

\begin_layout Plain Layout
7) stagewise is special case of gbm for exponential loss
\end_layout

\begin_layout Plain Layout
8) regularization on step size
\end_layout

\begin_layout Plain Layout
9) regularization by subsampling (of rows) 
\begin_inset Quotes eld
\end_inset

bag fraction
\begin_inset Quotes erd
\end_inset

 ; frequent default is 50%...
 but what if we have tons of data?[what happens in the limit of a single
 or very few data points?] – possible issue is that we mess up predictions
 at other points...
 [homework problem???]
\end_layout

\begin_layout Plain Layout
10) do we take step proportional to the gradient size as is usual in gradient
 descent style algorithms?
\end_layout

\begin_layout Plain Layout
11) regularization by subsampling (of columns) 
\begin_inset Quotes eld
\end_inset

According to use feedback, using column sub-sampling prevents overfitting
 even more so than the traditional row sub-sampling.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout
12) What's the hypothesis space for boosting methods? [universal?]
\end_layout

\end_inset


\end_layout

\begin_layout Section
Review: Gradient Boosting
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Gradient Boosting Machine
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Initialize 
\begin_inset Formula $f_{0}(x)=0$
\end_inset

.
\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $m=1,2,\ldots$
\end_inset

 (until stopping condition met)
\end_layout

\begin_deeper
\begin_layout Enumerate
Compute 
\series bold
unconstrained gradient
\series default
:
\begin_inset Formula 
\[
{\bf g}_{m}=\left(\left.\frac{\partial}{\partial f(x_{i})}\left(\sum_{i=1}^{n}\ell\left(y_{i},f(x_{i})\right)\right)\right|_{f(x_{i})=f_{m-1}(x_{i})}\right)_{i=1}^{n}
\]

\end_inset


\end_layout

\begin_layout Enumerate
Fit regression model to 
\begin_inset Formula $-{\bf g}_{m}$
\end_inset

:
\begin_inset Formula 
\[
h_{m}=\argmin_{h\in\ch}\sum_{i=1}^{n}\left(\left(-{\bf g}_{m}\right)_{i}-h(x_{i})\right)^{2}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
For fixed step size 
\begin_inset Formula $\nu_{m}=\nu\in(0,1]$
\end_inset

 [
\begin_inset Formula $v=0.1$
\end_inset

 is typical], take the step
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f_{m}(x)=f_{m-1}(x)+\nu_{m}h_{m}(x).
\]

\end_inset

Could also choose 
\begin_inset Formula $v_{m}$
\end_inset

 with line search.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Unconstrained Functional Gradient Stepping
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/unconstrained-fnl-gradient-steps-SeniElderFigB.1.png
	lyxscale 50
	height 60theight%

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula $R(\mathbf{f})$
\end_inset

 is the empirical risk, where 
\begin_inset Formula $\mathbf{f}=\left(f(x_{1}),f(x_{2})\right)$
\end_inset

 are predictions on training set.
\begin_inset Newline newline
\end_inset

Issue: 
\begin_inset Formula $\hat{{\bf f}}_{M}$
\end_inset

 only defined at training points.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Seni and Elder's 
\backslash
emph{Ensemble Methods in Data Mining}, Fig B.1.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

Projected
\begin_inset Quotes erd
\end_inset

 Functional Gradient Stepping
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename /Users/drosen/Dropbox/repos/mlcourse/Figures/boosting/projected-fnl-grad-SeniElderFigB.2.png
	lyxscale 50
	height 55theight%

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula $T(x;p)\in\ch$
\end_inset

 is our actual step direction – like the projection of 
\begin_inset Formula $\mbox{-{\bf g}=-}\del R(\mathbf{f})$
\end_inset

 onto 
\begin_inset Formula $\ch$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Seni and Elder's 
\backslash
emph{Ensemble Methods in Data Mining}, Fig B.2.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Gradient Boosting Machine Ingredients (Recap)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Take any [sub]differentiable loss function.
\end_layout

\begin_layout Itemize
Choose a base hypothesis space for regression.
\end_layout

\begin_layout Itemize
Choose number of steps (or a stopping criterion).
\end_layout

\begin_layout Itemize
Choose step size methodology.
\end_layout

\begin_layout Itemize
Then you're good to go!
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Gradient Tree Boosting
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Tree Boosting
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Common form of gradient boosting machine takes
\begin_inset Formula 
\[
\ch=\left\{ \mbox{regression trees of size \ensuremath{J}}\right\} ,
\]

\end_inset

where 
\begin_inset Formula $J$
\end_inset

 is the number of terminal nodes.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $J=2$
\end_inset

 gives decision stumps
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
HTF recommends 
\begin_inset Formula $4\le J\le8$
\end_inset

 (but some recent results use much larger trees)
\end_layout

\begin_layout Itemize
Software packages:
\end_layout

\begin_deeper
\begin_layout Itemize
Gradient tree boosting is implemented by the 
\series bold
gbm package
\series default
 for R
\end_layout

\begin_layout Itemize
as 
\family typewriter
\size footnotesize
GradientBoostingClassifier
\family default
\size default
 and 
\family typewriter
\size footnotesize
GradientBoostingRegressor
\family default
\size default
 in 
\series bold
sklearn
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
For trees, there are other tweaks on the algorithm one can do
\end_layout

\begin_deeper
\begin_layout Itemize
See HTF 10.9-10.12 
\end_layout

\end_deeper
\end_deeper
\begin_layout Section
GBM Regression with Stumps
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Sinc Function: Our Dataset
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/sinc-fn-data.png
	lyxscale 70
	height 60theight%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Natekin and Knoll's "Gradient boosting machines, a tutorial"}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Minimizing Square Loss with Ensemble of Decision Stumps
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/sinc-fit-1step10steps.png
	height 30theight%

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/boosting/sinc-fit-50steps100steps.png
	height 30theight%

\end_inset

 
\end_layout

\begin_layout Standard
Decision stumps with 
\begin_inset Formula $1,10,50$
\end_inset

, and 
\begin_inset Formula $100$
\end_inset

 steps, step size 
\begin_inset Formula $\lambda=1$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Figure 3 from Natekin and Knoll's "Gradient boosting machines, a tutorial"}
}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Step Size as Regularization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename /Users/drosen/Dropbox/repos/mlcourse/Figures/boosting/sinc-regression-train-validation.png
	lyxscale 35
	width 90text%

\end_inset

 
\end_layout

\begin_layout Standard
Performance vs rounds of boosting and step size.
 (Left is training set, right is validation set)
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Figure 5 from Natekin and Knoll's "Gradient boosting machines, a tutorial"}
}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Rule of Thumb
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The smaller the step size, the more steps you'll need.
\end_layout

\begin_layout Itemize
But never seems to make results worse, and often better.
\end_layout

\begin_layout Itemize
So make your step size as small as you have patience for.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Variations on Gradient Boosting
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Stochastic Gradient Boosting
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For each stage, 
\end_layout

\begin_deeper
\begin_layout Itemize
choose random subset of data for computing projected gradient step.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Typically, about 50% of the dataset size.
\begin_inset Quotes erd
\end_inset

 (but more on this)
\end_layout

\begin_layout Itemize
Fraction is often called the 
\series bold
bag fraction
\series default
.
\end_layout

\end_deeper
\begin_layout Itemize
Why do this?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Subsample percentage is additional regularization parameter.
\end_layout

\begin_layout Itemize
Faster.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
So....
 this is a minibatch method.
 
\end_layout

\begin_deeper
\begin_layout Itemize
we're estimating the 
\begin_inset Quotes eld
\end_inset

true
\begin_inset Quotes erd
\end_inset

 step direction (the projected gradient) using a subset of data
\end_layout

\begin_layout Itemize
\begin_inset CommandInset href
LatexCommand href
name "Stochastic Gradient Boosting"
target "http://statweb.stanford.edu/~jhf/ftp/stobst.pdf"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Introduced by Friedman (1999) in 
\backslash
href{http://statweb.stanford.edu/~jhf/ftp/stobst.pdf}{Stochastic Gradient Boosting}.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Some Comments on Bag Size
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Justification for 50% of dataset size:
\end_layout

\begin_deeper
\begin_layout Itemize
In bagging, sampling 50% without replacement gives very similar results
 to full bootstrap sample
\end_layout

\begin_layout Itemize
See Buja and Stuetzle's 
\begin_inset CommandInset href
LatexCommand href
name "Observations on Bagging"
target "http://stat.wharton.upenn.edu/~buja/PAPERS/sinica-bagging-buja-stuetzle.pdf"

\end_inset

.
\end_layout

\begin_layout Itemize
So if we're subsampling because we're inspired by bagging, this makes sense.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
But if we think of stochastic gradient boosting as a minibatch method,
\end_layout

\begin_deeper
\begin_layout Itemize
then makes little sense to choose batch size as a fixed percent of dataset
 size,
\end_layout

\begin_layout Itemize
especially for large datasets.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Just as we argued for minibatch SGD, 
\end_layout

\begin_deeper
\begin_layout Itemize
sample size needed for a good estimate of step direction is independent
 of training set size
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Minibatch size should depend on 
\end_layout

\begin_deeper
\begin_layout Itemize
the complexity of base hypothesis space
\end_layout

\begin_layout Itemize
the complexity of the target function (Bayes decision function)
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Seems like an interesting area for both practical and theoretical pursuit.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Newton Step Direction
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Second order Taylor expansion of 
\begin_inset Formula $J({\bf f})$
\end_inset

 at 
\begin_inset Formula ${\bf f}_{0}$
\end_inset

:
\begin_inset Formula 
\[
J({\bf f}_{0}+{\bf f})=J({\bf f}_{0})+\left[\del_{{\bf f}}J({\bf f}_{0})\right]^{T}{\bf f}+\frac{1}{2}{\bf f}^{T}\left[\del_{{\bf f}}^{2}J({\bf f}_{0})\right]{\bf f}
\]

\end_inset


\end_layout

\begin_layout Itemize
The loss for predicting 
\begin_inset Formula ${\bf f}_{i}$
\end_inset

 on the 
\begin_inset Formula $i$
\end_inset

th data point is 
\begin_inset Formula $\ell\left(y_{i},{\bf f}_{i}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Denote the derivative of this loss w.r.t.
 
\begin_inset Formula ${\bf f}_{i}$
\end_inset

 is 
\begin_inset Formula $g_{i}=\partial_{{\bf f}_{i}}\ell\left(y_{i},{\bf f}_{i}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
The second derivative w.r.t.
 
\begin_inset Formula ${\bf f}_{i}$
\end_inset

 is 
\begin_inset Formula $h_{i}=\partial_{{\bf f}_{i}}^{2}\ell\left(y_{i},{\bf f}_{i}\right)$
\end_inset

.
 
\end_layout

\begin_layout Itemize
So
\begin_inset Formula 
\begin{eqnarray*}
J({\bf f}) & = & \sum_{i=1}^{n}\ell\left(y_{i,}{\bf f}_{i}\right)\\
\del_{{\bf f}}J({\bf f}) & = & {\bf g}\\
\del_{{\bf f}}^{2}J({\bf f}_{0}) & = & \diag\left(\partial_{{\bf f}_{1}}^{2}\ell\left(y_{1},{\bf f}_{1}\right),\ldots,\partial_{{\bf f}_{n}}^{2}\ell\left(y_{n},{\bf f}_{n}\right)\right)
\end{eqnarray*}

\end_inset

of For GBM, we find the closest 
\begin_inset Formula $h\in\cf$
\end_inset

 to the negative gradient
\begin_inset Formula 
\[
-{\bf g}=-\del_{\vf}J({\bf f}).
\]

\end_inset


\end_layout

\begin_layout Itemize
This is a 
\begin_inset Quotes eld
\end_inset

first order
\begin_inset Quotes erd
\end_inset

 method.
 
\end_layout

\begin_layout Itemize
Newton method is a 
\begin_inset Quotes eld
\end_inset

second order method
\begin_inset Quotes erd
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
Approximate 
\begin_inset Formula $J({\bf f})$
\end_inset

 using 2nd order Taylor expansion (i.e 
\end_layout

\begin_layout Itemize
Step direction is towards the 
\end_layout

\end_deeper
\end_deeper
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Column / Feature Subsampling for Regularization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Similar to random forest, randomly choose a subset of features for each
 round.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
XGBoost paper says: 
\begin_inset Quotes eld
\end_inset

According to user feedback, using column sub-sampling prevents overfitting
 even more so than the traditional row sub-sampling.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Newton Step Direction
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For GBM, we find the closest 
\begin_inset Formula $h\in\cf$
\end_inset

 to the negative gradient
\begin_inset Formula 
\[
-{\bf g}=-\del_{\vf}J({\bf f}).
\]

\end_inset


\end_layout

\begin_layout Itemize
This is a 
\begin_inset Quotes eld
\end_inset

first order
\begin_inset Quotes erd
\end_inset

 method.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Newton's method is a 
\begin_inset Quotes eld
\end_inset

second order method
\begin_inset Quotes erd
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
Find 2nd order (quadratic) approximation to 
\begin_inset Formula $J$
\end_inset

 at 
\begin_inset Formula $\vf$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Requires computing gradient and Hessian of 
\begin_inset Formula $J$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Newton step direction points towards minimizer of the quadratic.
\end_layout

\begin_layout Itemize
Minimizer of quadratic is easy to find in closed form
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Boosting methods with projected Newton step direction:
\end_layout

\begin_deeper
\begin_layout Itemize
LogitBoost (logistic loss function)
\end_layout

\begin_layout Itemize
XGBoost (any loss – uses regression trees for base classifier)
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Section
Questions
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Hypothesis Space for AdaBoost
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Base hypothesis space: 
\begin_inset Formula $\ch$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consists of hard-classification functions
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $h\in\ch$
\end_inset

, we have 
\begin_inset Formula $h:\cx\to\left\{ -1,1\right\} $
\end_inset

\SpecialChar endofsentence

\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The hypothesis for AdaBoost after 
\begin_inset Formula $T$
\end_inset

 rounds is
\begin_inset Formula 
\[
\cc_{T}=\left\{ h(x)=\sign\left(\sum_{i=1}^{T}\alpha_{i}h_{i}(x)\right)\mid\alpha_{i}\in\reals,h_{i}\in\ch\,\forall i=1,\ldots,T\right\} .
\]

\end_inset

 
\end_layout

\end_deeper
\end_inset


\begin_inset Note Note
status open

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Questions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Plain Layout
1) expressivity / complexity of boosting decision stumps
\end_layout

\begin_layout Plain Layout
2) generalization bounds in terms of complexity of base hypothesis space
 
\end_layout

\begin_layout Plain Layout
5) Population minimizer of exponential loss 
\end_layout

\begin_layout Plain Layout
9) regularization by subsampling (of rows) 
\begin_inset Quotes eld
\end_inset

bag fraction
\begin_inset Quotes erd
\end_inset

 ; frequent default is 50%...
 but what if we have tons of data?[what happens in the limit of a single
 or very few data points?] – possible issue is that we mess up predictions
 at other points...
 [homework problem???]
\end_layout

\begin_layout Plain Layout
12) What's the hypothesis space for boosting methods? [universal?]
\end_layout

\end_deeper
\end_inset


\end_layout

\end_body
\end_document
