{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#Author: Yao Zhu\n",
    "#Licensed under the MIT License\n",
    "#https://opensource.org/licenses/MIT\n",
    "#Based on the code from Steven Rabanser at https://github.com/steverab/tensor-gmm/tree/master/python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d = 10              # dimension of each data point.\n",
    "k = 6               # number of components. We require k<=d\n",
    "n = 1000            # same number of data points from each component.\n",
    "\n",
    "s_true = 10          # true variance parameter, i.e., \\sigma^2.\n",
    "\n",
    "# For reproducible experiments, fix the random seed.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(d, k, n):\n",
    "    # Randomly generate the k mu_i's. They will be linearly independent a.s.\n",
    "    # NOTE: without scaling (i.e., scale=1), the results are very bad. It might\n",
    "    # require more data points, and more stable numerical implementation, more\n",
    "    # power iterations. And it might be that uniform w_i is a difficult case?\n",
    "    # well.\n",
    "    scale = 20\n",
    "    MU_true = scale*np.random.rand(d, k)\n",
    "    \n",
    "    tot = k * n         # total number of data points.\n",
    "    \n",
    "    # Generate data from k mixture of spherical Gaussians. n data points from\n",
    "    # each component. Thus the true mixing weight of each component is 1/k.\n",
    "    X = np.zeros((tot, d))\n",
    "    for i in range(k):\n",
    "        mean = np.transpose(MU_true[:, i])\n",
    "        covariance = s_true * np.identity(d)\n",
    "        mvn = np.random.multivariate_normal(mean, covariance, n)\n",
    "        X[i*n:(i+1)*n, :] = mvn\n",
    "\n",
    "    # Each row of X is a data point.\n",
    "    return (X, MU_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_moment(X):\n",
    "    tot = len(X[:,1])\n",
    "    M1 = np.matmul(X.T, np.ones((tot,1)))\n",
    "    M1 /= tot\n",
    "    return M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def second_moment(X):\n",
    "    tot = len(X[:,1])\n",
    "    E2 = np.matmul(X.T, X)\n",
    "    E2 /= tot\n",
    "    return E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_variance(M1, E2):\n",
    "    # Once we have M1 and E2, we can estimate the variance parameter by\n",
    "    # computing the smallest eigenvalue of the empirical covariance matrix.\n",
    "    covX = E2 - np.matmul(M1, M1.T)\n",
    "    eigen_pair = np.linalg.eig(covX)\n",
    "    # Extract the smallest eigenvalue.\n",
    "    s_est = min(eigen_pair[0].tolist())\n",
    "\n",
    "    return s_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_whitening(M2, X, k):\n",
    "    # Thin eigendecomposition of M2 by thin-SVD on it. However, because M2 is\n",
    "    # just and estimate, you may find that the result is not thin at all, i.e.,\n",
    "    # Lambda could be of length d.\n",
    "    U, Lambda, _ = np.linalg.svd(M2)\n",
    "    # Use only the first k-columns of U and first k elements of Lambda to define\n",
    "    # the whitening matrix.\n",
    "    B = np.matmul(U[:,0:k], np.diag(np.sqrt((1/Lambda[0:k]))))\n",
    "    # The right pseudo inverse of B.T.\n",
    "    BT_pinv = np.matmul(U[:,0:k], np.diag(np.sqrt((Lambda[0:k]))))\n",
    "    # Whiten the data matrix.\n",
    "    X_whit = np.matmul(X, B)\n",
    "    \n",
    "    return (B, X_whit, BT_pinv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tensor_power_method(X_whit, B, s_est, M1):\n",
    "    # Convergence tolerance of the power iteration.\n",
    "    TOL = 1e-8\n",
    "    # Max number of power iteration.\n",
    "    maxiter = 100\n",
    "    # The estimated eigenvectors of the whitened 3rd order purified moment.\n",
    "    V_est = np.zeros((k, k))\n",
    "    # The estimated eigenvalues of the whitened 3rd order purified moment.\n",
    "    lamb_est = np.zeros((k, 1))\n",
    "\n",
    "    tot = len(X_whit[:,1])\n",
    "    for i in range(k):\n",
    "        # Initial as a random vec on the unit sphere.\n",
    "        v_old = np.random.rand(k, 1)\n",
    "        v_old = np.divide(v_old, np.linalg.norm(v_old))\n",
    "        # Run at most maxiter number of power iterations.\n",
    "        for iter in range(maxiter):\n",
    "            alpha = np.matmul(X_whit, v_old)\n",
    "            v_new = (np.matmul(X_whit.T, alpha*alpha)) / tot\n",
    "            # We need this temporary matvec.\n",
    "            BT_M1 = np.matmul(B.T, M1)\n",
    "            beta = np.matmul(B, v_old)\n",
    "            # Note although in essence np.dot(beta.T, beta) is a scalar, it's\n",
    "            # still represented as an array, thus we need another np.matmul.\n",
    "            v_new -= s_est * (np.matmul(BT_M1, np.dot(beta.T, beta)))\n",
    "            v_new -= s_est * (2 * np.matmul(B.T, np.matmul(np.matmul(B, v_old), np.dot(BT_M1.T, v_old) )))\n",
    "            # Apply the deflation.\n",
    "            if i > 0:\n",
    "                for j in range(i):\n",
    "                    v_new -= np.reshape(V_est[:, j] * np.power(np.dot(v_old.T, V_est[:, j]), 2) * lamb_est[j], (k,1))\n",
    "            l = np.linalg.norm(v_new)\n",
    "            v_new = np.divide(v_new, l)\n",
    "            if np.linalg.norm(v_old - v_new) < TOL:\n",
    "                V_est[:, i] = np.reshape(v_new, k)\n",
    "                lamb_est[i] = l\n",
    "                break\n",
    "            v_old = v_new\n",
    "\n",
    "    return (V_est, lamb_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_transform(V_est, lamb_est, BT_pinv):\n",
    "    # w_i = 1/(lamb[i]*lamb[i])\n",
    "    w_est = 1 / (lamb_est*lamb_est)\n",
    "    # You may find that sum(w_est) is not close to 1. Thus, a renormalization\n",
    "    # might be needed.\n",
    "    w_est = w_est / sum(w_est)\n",
    "    MU_est = np.matmul(np.matmul(BT_pinv, V_est), np.diag(np.reshape(lamb_est, k)))\n",
    "    \n",
    "    return (MU_est, w_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_results(s_true, s_est, MU_true, MU_est, w_est):\n",
    "    print (\"The true variance parameter is: \" + str(s_true))\n",
    "    print (\"The estimated variance parameter is: \" + str(s_est))\n",
    "    k = len(MU_true[1,:])\n",
    "    # In order to detect the permutation to align MU_true and MU_test, compute\n",
    "    # the normalized Gram matirx.\n",
    "    MM_Gram = np.zeros((k,k))\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            MM_Gram[i,j] = np.inner(MU_true[:,i], MU_est[:,j]) / (np.linalg.norm(MU_true[:,i]) * np.linalg.norm(MU_est[:,j]))\n",
    "    for i in range(k):\n",
    "        j = np.argmax(MM_Gram[i,:])\n",
    "        print(\"Parameters specific to the \" + str(i+1) + \"-th component:\")\n",
    "        print(\"\\t\" + \"The true      mixing weight: \" + str(1.0/k))\n",
    "        print(\"\\t\" + \"The estimated mixing weight: \" + str(w_est[j][0]))\n",
    "        print(\"\\t\" + \"The true      mean: \" + str(MU_true[:,i].T))\n",
    "        print(\"\\t\" + \"The estimated mean: \" + str(MU_est[:,j].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true variance parameter is: 10\n",
      "The estimated variance parameter is: 9.525512197706476\n",
      "Parameters specific to the 1-th component:\n",
      "\tThe true      mixing weight: 0.16666666666666666\n",
      "\tThe estimated mixing weight: 0.162392586472\n",
      "\tThe true      mean: [ 10.97627008   8.75174423  11.36089122  15.56313502   2.36548852\n",
      "   5.29111224  12.24191445  13.95262392   6.30856702   4.17753512]\n",
      "\tThe estimated mean: [ 10.93920512   8.91967678  11.12889194  15.29780592   2.48027203\n",
      "   5.25682771  12.17614775  13.7304478    6.47640475   4.45684425]\n",
      "Parameters specific to the 2-th component:\n",
      "\tThe true      mixing weight: 0.16666666666666666\n",
      "\tThe estimated mixing weight: 0.173429700447\n",
      "\tThe true      mean: [ 14.30378733  17.83546002  18.51193277  17.40024296  12.79842043\n",
      "  15.48467379  12.33867994   1.20450943   7.27421542   3.22619036]\n",
      "\tThe estimated mean: [ 14.03919635  17.69596299  17.60796485  17.12310153  12.95907186\n",
      "  15.18983525  12.54766186   1.76641073   7.3837034    3.46600575]\n",
      "Parameters specific to the 3-th component:\n",
      "\tThe true      mixing weight: 0.16666666666666666\n",
      "\tThe estimated mixing weight: 0.157399820451\n",
      "\tThe true      mean: [ 12.05526752  19.27325521   1.42072116  19.57236684   2.86706575\n",
      "   9.12300664  18.87496157  13.33533431  11.40393541  13.06216651]\n",
      "\tThe estimated mean: [ 12.11323024  19.07668825   1.50348689  19.51854522   3.00824464\n",
      "   8.94444128  18.92924198  13.31231596  11.15731764  12.76335769]\n",
      "Parameters specific to the 4-th component:\n",
      "\tThe true      mixing weight: 0.16666666666666666\n",
      "\tThe estimated mixing weight: 0.155291892631\n",
      "\tThe true      mean: [ 10.89766366   7.66883038   1.74258599  15.98317128  18.89337834\n",
      "  11.36867898  13.63640598  13.41275739   8.77203027   5.06583205]\n",
      "\tThe estimated mean: [ 10.82118404   7.63779151   1.83359399  15.85115763  18.66998     11.52015604\n",
      "  13.62538501  13.35243145   8.84764987   5.22673714]\n",
      "Parameters specific to the 5-th component:\n",
      "\tThe true      mixing weight: 0.16666666666666666\n",
      "\tThe estimated mixing weight: 0.158591826193\n",
      "\tThe true      mean: [  8.47309599  15.83450076   0.40436795   9.22958725  10.43696644\n",
      "   0.37579601   7.19015801   4.20765122  19.76747676   9.32621546]\n",
      "\tThe estimated mean: [  8.42898599  15.9637536    0.36114195   9.44385724  10.51611765\n",
      "   0.6340536    7.12307794   4.03461342  19.62117237   9.25756993]\n",
      "Parameters specific to the 6-th component:\n",
      "\tThe true      mixing weight: 0.16666666666666666\n",
      "\tThe estimated mixing weight: 0.192894173805\n",
      "\tThe true      mean: [ 12.91788226  10.5778984   16.65239691  15.61058353   8.2932388\n",
      "  12.35270994   8.74063908   2.57852595   2.04089621   4.88851184]\n",
      "\tThe estimated mean: [ 12.64544145  10.55605766  15.2736856   15.67218425   8.94716934\n",
      "  11.93047186   8.71718025   3.1757811    3.31118149   5.38842026]\n"
     ]
    }
   ],
   "source": [
    "(X, MU_true) = generate_data(d, k, n)\n",
    "\n",
    "# Explicitly form the first and second raw moments.\n",
    "M1 = first_moment(X)\n",
    "E2 = second_moment(X)\n",
    "\n",
    "# Estimate the variance parameter.\n",
    "s_est = estimate_variance(M1, E2)\n",
    "\n",
    "# Form the second order purified moment.\n",
    "M2 = E2 - s_est * np.identity(d)\n",
    "\n",
    "# Perform whitening.\n",
    "B, X_whit, BT_pinv = perform_whitening(M2, X, k)\n",
    "\n",
    "# Tensor power method.\n",
    "(V_est, lamb_est) = tensor_power_method(X_whit, B, s_est, M1)\n",
    "\n",
    "# Backward transform.\n",
    "MU_est, w_est = backward_transform(V_est, lamb_est, BT_pinv)\n",
    "\n",
    "# Report the results.\n",
    "report_results(s_true, s_est, MU_true, MU_est, w_est)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (sandboxed)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
